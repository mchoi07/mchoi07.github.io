{"pages":[{"title":"","text":"","link":"/404.html"},{"title":"About Minkyu Choi","text":"IntroMinkyu Choi is a Artificial Intelligence &amp; Machine Learning Engineer working in the various industries around 3 years as a consultant. He had worked in a commerical sectors for a couple of years and moved to government sectors. He has been exposed to diverse machine learning and deep learning projects, experienced in various big data engineering projects including Hadoop, Spark, Kafka, etc. Minkyu’s Expertises ML/AI Infrastructure Cloud Computing &amp; Architect (Amazon Web Service) Data Pipelining — Batch &amp; Streaming Machine Learning &amp; Deep Learning Github LinkedIn Email #contact-buttons { width: 100%; display: flex; } #contact-butons a.button { flex-grow: 1; } @media screen and (max-width: 720px) { #contact-buttons a.button { width: 100%; margin-right: 0; } } EducactionGeorgia Institute of TechnologyMS in Data Science Knowledge-Based Artificial Intelligence: Cognitive Systems Big Data for Healthcare Computational Data Analytics (Machine Learning) Machine Learning For Trading High-Dimensional Data Analysis Baruch CollegeBBA in Computer Information System Data Warehousing Database Management System Quantiative Decision Making Regression and Forecasting Modeling ProfessionalUnited States Department of Defense (The Joint Artifical Intelligence Center)Artificial Intelligence Engineer Project Overview: The Joint Artificial Intelligence Center (JAIC) is the Department of Defense’s (DoD) Artificial Intelligence (AI) Center of Excellence that provides a critical mass of expertise to help the Department harness the game-changing power of AI. Architecting an infrastructure of DoD AI/ML platform Developing a data strategy and capability of the JAIC Common Foundation (JCF) Participating a ML/DL model development within CI/CD ML DevOpsSec life cycle Techfield LLCSubject Matter Expert in Big Data and Data Science Managed +10 consultants in the big data and data science team, supporting the projects such as a cloud data pipeline solution and an automated application platform with a machine learning system Worked closely with one of the fortune 500 client in order to accomplish a goal of the datapipeline project for the entire batch and streaming processing and achieved a cost reductionand efficient processing time Endpoint ClinicalData Engineer Worked as a back-end database developer of IRT system – providing integrated solution of clinical trials (e.g., patient record, drug supply management and site management) in order to provide technical solution for large size of pharmaceutical clients Argus Information and Advisory ServicesData Analyst Developed strategies, best practices, and technical requirements to improve data manipulation and analysis by querying millions of rows of data across multiple databases, using MS SQL Server to identify and resolve data issues, and leveraged large sets of data to define benchmark models to enable client’s objectives to better manage data and drive insights SkillPrograming &amp; Scripting Language: Scala | Java | Python | R |Bash |Big Data Application: Hadoop | Spark | Kafka | Flume | Nifi | Sqoop | ELK |Database: MySql | Cassandra | Hbase | Elasticsearch | DynamoDB |","link":"/about/index.html"}],"posts":[{"title":"Simulation 01 - Random Number","text":"A simulation is not real but it can represent the real. It’s because a simulation is an imitation of real situation - it can’t be exact but it can be approximate. Most of simulation models are strated from generating random number because randomness creates value on a simulation modeling. It is really important to give an algorithm that produces a sequence of pseudo-random number (PRNs) $R_1, R_2,…$ that “appear” to be iid Unif(0,1). There are many different Uniform(0,1) Generators. Output of random device Nice randomness properties. However, Unif(0,1) sequence storage difficult, sot it’s tough to repeat experiment Examples: flip a coin particle count by Geiger coutner least significant digits of atomic clock Table of random numbers List of digits supplied in tables - A Million random Digits with 100,00 Normal Deviates. Cumbersome, slow, table too small - not very useful Mid-Square Idea - Take the middle part of square of the previous random number. John von Neumann was a brilliant and fun-loving guy, but method is terrible Example: Take $R_i = X_i/10000$, ∀i, where the Xi’s are positiveintegers &lt; 10000. Set seed $X_0 = 6642$; then $6632^2$ = 43983424 so $X_1 = 9834$; then $9834^2$ - 96707556 so $X_2$ = 7075, etc,… Unfortunately, positive serial correlation in $R_i$’s. Also, occasionally degenerates; eg., consider $X_i$ = 0003 Fibonacci These methods are also no good!! Take $X_i = (X_{i-1} + X_{i-2})mod(m), i = 1,2,…,$ where $R_i = X_i/m$ ,$m$ is the modulus, $X_01,X_0$ are seeds, and $a = b mod m$ if $a$ is the remainer of $b/m$ Problem: small numbers follow small numbers Also, it’s not possible to get $X_{i-1} &lt; X_{i+1} &lt; X_i$ or $X_i &lt; X_{i+1} &lt; X_{i-1} $ (which should occur w.p 1/3) $X_{i+1}$ Linear congruential (most commonly used in practice LCGs are the most widely used generators. These are pretty good when implemented properly. $X_i = (aX_{i-1} + c) mod(m)$, where $X_0$ is the seed. $R_i = X_i/m, i = 1,2,…$ Choose a,c,m carefully to get good stastistical quality and long period or cycle length, i.e., time until LCG starts to repeat itself. If $c = 0$, LCG is called a multiplicative generator Tausworthe (linear recursion mod 2) Tausworthe Generator is a kind of multicative recursive generator. $X_{i+1} = (aX_{i-1} + c) mod(2)$, where $X_0$ is the seed. Reference Georgia Tech’s ISYE6644 class content","link":"/2019/07/18/random-number/"},{"title":"Simulation 03 - Input Analysis","text":"How can we tell our random variables are well made? In simulation terminology, we have something called input analysis. In my previous two postings, random number, random variable, I’ve talked about how to generate a random number and how we could make a radom variable by using those random numbers. Then how can we tell our random variables are well made? In simulation terminology, we have something called input analysis. The random variables are our input, and we need to analyze those inputs to verify it’s relevance. If you use the wrong input random variables in the simulation model, it will result in wrong output. A proper input analysis can save you from Garbage-in-garbage-out How can we conduct a proper input analysis? We have to collect data Data Sampling - we could shuffle the data and take some samples from there We have to figure out a distribution of data Plot the data to histogram Discrete vs contunuous Univariate / Multivariate If data is not enough — we can guess a good distribution We have to do a statistical test to verify the distribution 1. Point EstimationA statistic can not explicitly depend on any unknown parameters because statics are based on the actural observations. Statistics are random variable - we could expect to have two different values of a static when we take two different samples. But we need to find the unknown parameters. How can we do it? we could estimate unknown parameters from the existing probability of distribution. Let $X_1, . . . . , X_n$ be i.i.d. Random Variables and let $M(X) ≡ M(X_1, . . . . , X_n)$ be a statistic based on the $X_i$’s. Suppose we use $M(X)$ to estimate some unknown parameter $θ$ Then $M(X)$ is called a point estimator for $θ$ . $\\bar{x}$ is a point estimator for the mean $μ = E[Xi]$ $S^2$ is a point estimator for the variance $σ2 = Var(Xi) $ *It would be nice if $M(X)$ had certain properties: * Its expected value should equal the parameter it’s trying to estimate It should have low variance We all know that good estimator should be unbiased because if we use the biased estimator we won’t figure out whether our model is good or not. We will be fooled by a biased estimator. $T(X)$ is unbiased for $θ$ if $E[M(X)] = θ$ EX1 - Suppose $X_1, . . . , X_n$ are i.i.d. anything with mean μ. Then So $\\bar{X}$ is alwys unbiased for $\\mu$. That’s why ${\\bar{X}}$ is called the sample mean EX2 - Suppose $X_1, . . . , X_n$ are i.i.d. anything with mean μ and variance $\\sigma^2$. Then Thus, $S^2$ is always unbiased for $\\sigma^2$. This is why $S^2$ is called the sample variance. 2. Mean Square ErrorIn a perfect sceanario, our estimator will be exactly same as $\\theta$. Then we will see no error. However, it’s not the real case. Our goal is to reduce the error between an estimator and $\\theta$. The mean squared error of an estimator $M(X)$ of $\\theta$ is $$\\begin{aligned} MSE(M(X)) ≡ E[(M(X)-\\theta)^2]\\end{aligned}$$ $$\\begin{aligned} Baia(M(X)) ≡ E[T(X) - \\theta]\\end{aligned}$$ We could interpret MSE like this: Lower MSE mean we are avoiding the bias and variance. Our goal should be finding a good estimator which can lower our error. If $M_1(X)$ and $M_2(X)$ are two estimators of $\\theta$, we’d usually prefer the one with the lower MSE — even if it happens to have higer bias. 3. Maximum Linelihood EstimatorsWhat if we don’t have a set of data, but we have a pdf/pmf $f(x)$ of the distribution. How can we find the $\\theta$? Consider an i.i.d. random sample $X_1, . . . , X_n$, where each $X_i$ has pdf/pmf $f(x)$. Further, suppose that $\\theta$ is some unknown parameter from $X_i$. The likelihood function is $L(\\theta) ≡ \\prod f(x_i)$ The maximum likelihood estimator (MLE) of $\\theta$ is the value of $\\theta$ that maximizes $L(\\theta)$. The MLE is a function of the $X_i$’s and is a RV. EX1 - Suppose $X_1, . . . , X_n$ ~ Exp$(\\gamma)$. Find the MLE for $\\gamma$ Since the natural log function is one-to-one, it’s easy to see that the $\\gamma$ that maximizes $L(\\gamma)$ also maximize $ln(L(\\gamma))$ This implies that the MLE is $\\hat{\\gamma} = 1 / \\bar{X}$ Invariance Property If $\\hat{\\theta}$ is the MLE of some parameter $\\theta$ and $h(.)$ is a one-to-one function, then $h(\\hat{\\theta})$ is the MLE of $h(\\theta)$ For Bern(p) distribution the MLE of $p$ is $\\hat{p}=\\bar{X}$ (which also happens to be unbiased). If we consider the 1:1 function $h(\\theta) = \\theta^2$ for ($\\theta &gt; 0)$, then the Invariance property says that the MLE of $p^2$ is $\\bar{X}^2$ But such a property does not hold for unbiasedness $$\\begin{aligned}E[S^2] = σ^2\\end{aligned}but\\begin{aligned}E[\\sqrt{S^2}]= σ\\end{aligned}$$ Really that MLE for $\\sigma^2$ is $\\hat{\\sigma^2} = 1/n\\sum(X_i - \\bar{X})^2$. The good news is that we can still get the MLE for $\\sigma$. If we consider the 1:1 function $h(\\theta) = +\\sqrt{\\theta}$, then the invariance property says that the MLE of $\\sigma$ is $$\\begin{aligned}\\hat{\\sigma} = \\sqrt{\\hat{\\sigma^2}} = \\sqrt{\\sum(X_i-\\bar{X})^2/n}\\end{aligned}$$ 4. The Method of MomentsRecall: the $k$th moment of a random variable X is Suppose $X_1, . . . , X_n$ are i.i.d. from p.m.f. / p.d.f. $f(x)$. Then the method of moments(MOM) estimator for $E[X^k]$ is $$m_k ≡ 1/n\\sum X^k_i$$ 5. Goodness-of-Fit TestsWe finanlly guessed a reasonable distribution and then estimated the relevant parameters. Now let’s conduct a formal test to see just how sucessful our toils have been. In particular, test $$H_0 : X_1, X_2, . . . , X_n - p.m.f / p.d.f. f(x)$$ At level of significance $$\\alpha ≡ P(Reject H_0 | H_0 true) =P(\\text{Type I Error})$$ Chi-Square-TestGoodness-of-fit test procedure: Divide the domain of $f(x)$ into $k$ sets, say, $A_1, A_2, A_3, . . . , A_k$ (distinct points if X is discret r intervals if X is continuous) Tally the actual number of observations that fall in ach set, say, $O_i i = 1, 2, . . . , k$. If $p_i ≡ P(X ∈ Ai)$, then $O_i \\text{~ Bin(}n,p_i)$ Determine the expected number of observations that would fall in each set if $H_0$ were true, say, $E_i = E[O-I] = np_i, i = 1,2, . . . , k$ Calculate a test statistic based on the differences between the $E_i$ and $O_i$. A large value of $X^2_0$ indicate a bad fit. We reject $H_0 \\text{ if } \\chi^2_0 &gt; \\chi^2_{\\alpha,k-1-s}$, where $s$ is the number of nuknown paramets from $f(x) that have to be estimated. Usual recommendation: For the $\\chi^2$ g-o-f test to work, pick $k,n$ such that $E_i &gt;= 5$ and $n$ at least 30 Kolmogorov_Smirnov Goodness-of-Fit Test We’ll test $H_0 : X_1, X_2, . . . , X_n$ some distribution with $c.d.f. F (x)$. Recall the difincation of the empirical c.d.f. (also called the sample c.d.f) of the data is $$\\hat{F_n}(x) ≡ (\\text{number of } X_i &lt;= x) / n$$ The Glivenko-Cantelli Lemma says that $\\hat{F_n}(x) -&gt; F(x)$ for all $x$ as $ n-&gt;\\infinite$. So if $H_0$ is ture then $\\hat{F_n}(x)$ should be a good approxmination to the true c.d.f. $F(x)$, for large $n$ The main question: Does the empirical distribution actually support the assumption that H0 is true? ReferenceThumnailGeorgia Tech’s ISYE6644 class content","link":"/2019/07/23/input-analysis/"},{"title":"Simulation 02 - Random variable","text":"Random variable can be generated from a good random number generator. If real variables has moved the reality, we could design a future with a good random variables. Author: Minkyu ChoiLast updated: 07/19/2019 Inverse Transform MethodInverse transform sampling is a method for generating random numbers from any probability distribution by using its inverse cumulative distribution F−1(x)F−1(x). Recall that the cumulative distribution for a random variable XX is FX(x)=P(X≤x)FX(x)=P(X≤x). In what follows, we assume that our computer can, on demand, generate independent realizations of a random variable UU uniformly distributed on [0,1] Cutpoint MethodThis inverse-transform method has the advantage of having an optimal O(n) setup time. However, the average number of steps required to sample X is not optimal, and if several samples of X are needed, then the cutpoint method offers an average number of two comparison steps needed to sample an observation, yet still has an O(n) initial setup time Without loss of generality, we can assume that X = [1, n]. Also, let qi = P(X ≤ i). Then the idea behind the cutpoint method is to choose m ≥ n, and define sets Q1, . . . , Qm for which for all i = 1, . . . , m. In words, the unit interval [0, 1] is partitioned into m equal sub-intervals of the form $[\\frac{(i−1)} m, \\frac{i}m)$, i = 1, . . . , m. And when U falls into the i th sub-interval, then Qi contains all the possible qj values for which F −1 (U) = j. That way, instead of searching through all of the q values, we save time by only examining the qj values in Qi , since these are the only possible values for which $F^{-1} (U) = j$. Convolution Method Sum of n variables: $x = y_1 + y_2 + … y_n$ Generate n random variate yi’s and sum For sums of two variables, pdf of x = convolution of pdfs of y1 and y2. Hence the name Although no convolution in generation If pdf or CDF = Sum ⇒ Composition Variable x = Sum ⇒ Convolution Acceptance-Rejection MethodFinding an explicit formula for F −1 (y) for the cdf of a rv X we wish to generate, F(x) = P(X ≤ x), is not always possible. Moreover, even if it is, there may be alternative methods for generating a rv distributed as F that is more efficient than the inverse transform method or other methods we have come across. Here we present a very clever method known as the acceptance-rejection method. Composition MethodCan be used when m can be expressed as a convex combination of other distributions Fi , where we hope to be able to sample from $F_i$ more easily than from F directly. ReferencesLink-1Link-2Link-3Link-4Link-5","link":"/2019/07/19/random-variable/"},{"title":"Realtime Virus Scanning","text":"To protect our system and computer we should make sure that data which we download is clean. Everytime we bring data to our system or user upload data such as file attachments, we must make sure that data is free from viruses and trojans. If our system has sensitive data and critical for operation you have to be more cautious about bringing data to your system - cyber attack, nowadays, is being serious and cunning. In a normal usecase, we set up Anti Virus (AV) scanner on a file system. AV scanner monitor our file system and RAM in real-time or batch. However, it cannot make sure that each file doesn’t have any malicious content in real-time. In this project, we will use two open source products to detect virus/trojan in realtime. We are going to use Apache Nifi and ClamAV Apache Nifi is a very powerful, easy to use and stable system to process and distribute data between disparate system. Apache Nifi is a real time data ingestion platform, which can transfer and manage data transfer between different sources and destination systems. ClamAV is an open source antivirus engine for detecting trojans, viruses, malware &amp; other malicious threats. ReferenceThumnail Images","link":"/2019/11/20/nifi-virus-scanning/"},{"title":"Simulation 04 - Output Analysis","text":"Ananyzing the ouput of a simulation model is important. How can we be sure that our output is proper and will not hurt an experiment result using those outputs. Keep this in mind - out is rearely i.i.d. Why do we worry about output? In put processes driving a simulation are random variables. It means our output from the simulation must be random. If we runs the simulation it only yields estimates of measure of system performace, and these estimators are themselves random variables, and are therfore subject to sampling error. Sampling error must be taken into account to make valid inferences concerning system performance. Measures of Interest Means - what is the mean customer waiting time? Variances - how much is the waiting time liable to vary? Quantiles - what’s the 99% quantile of the line length in a certain queue? Sucess probabilities - will my job be completed on time? Would like point estimators and confidence intervals for the above. There are two general types of simulations with respect to output analysis. To facilitate the presentation, we identify two types of simulations with respect to output analysis: Finite-Horizon (Terminating) Simulations - Interested in short-term performance The termincation of finite-horizon simulation takes place at a specific time or is caused by the occurrence of a specific event. EX1 - Mass transit system during rush hour EX2 - Distribution system over one month Steady-State simulations - Interested in long-term performance The purpose of steady-state simulation is to study the long-run behavior of a system. A performance measure is a steady-state parameter if it is a characteristic of the equilibrium distribution of an output process. EX1 - Continuously operating communication system where the objective is the computation of the mean delay of a packet in the long run EX2 - Distribution system over a long period of time EX3 - Markov chains Finite-Horizon Simulation First thing we have to do to conduct this simulation is getting expected values from replications. So basically, we need to decide a number of independetn replications (IR). IR estimates Var($\\bar{Y}_m$) by conducting $r$ independent simulation runs (replications) of the system under study, where each replication consists of $m$ observations. It is easy to make the replications independent - just re-initialize each replication with a different pseudo-random number seed Sample means from replication If each run is started under the same operating conditions (e.g., all queues empty and idle), then the replication sample means $Z_1, Z_2, . . . , Z_r$ are $i.i.d.$ random variables. Suppose we want to estimate the expected average waiting time for the first m = 5000 customers at the bank. We make r = 5 independent replications of the system, each initialized empty andidle and consisting of 5000 waiting times. The resulting replicate means are: Steady-state simulation How about we need to simulate the entire time line? We should consider to use a steady-state simulation. Estimate some parameter of interest, e.g., the mean customer waiting time or the expected profit produced by a certain factory configuration. In particular, suppose the mean of this output is the unknown quantity $\\mu$. We’ll use the sample mean $\\bar{Y}_n$ to estimate $\\mu$ We must accompany the value of any point extimator with a measure of its variance. In stead of Var($\\bar{Y}_n)$ we canestimate the variance parameter, Thus, $\\sigma^2$ is imply the sume of all covariances! $\\sigma^2$ pops up all over the place: simulation output analysis, Brownian motions, fnancial engineering application, etc. Many methods for estimating $\\sigma^2$ and for conducting steady-state output analysis in general: Batch means The method of batch means (BM) is often used to estimate $\\sigma^2$ and to calculate confidence intervals for $\\mu$ Idea: Divide one long simulation run into a number of contiguous batches, and then appeal to a central limit theorem to assume that the resulting batch sample means are approximately i.i.d. normal. In particular, suppose that we partition $Y_1, Y_2, . . . , Y_n$ into $b$ nonoverlapping, contiguous batches, each consisting of $m$ observations (assume that $n = bm$) The $i$th batch mean is the sample mean of the $m$ observations from batch $i = 1, 2, . . . , b$ $E[H]$ decreases in b, though it smooths out around b = 30. A common recommendation is to take b =. 30 and concentrate on increasing the batch size m as much as possible. The technique of BM is intuitively appealing and easy to understand. But problems can come up if the Yj ’s are not stationary (e.g., if significant initialization bias is present), if the batch means are not normal, or if the batch means are not independent. If any of these assumption violations exist, poor confidence interval coverage may result — unbeknownst to the analyst. To ameliorate the initialization bias problem, the user can truncate some of the data or make a long run In addition, the lack of independence or normality of the batch means can be countered by increasing the batch size m. ReferenceThumnailGeorgia Tech’s ISYE6644 class content","link":"/2019/07/30/output-analysis/"},{"title":"암호화복호화","text":"여러분들의 데이터는 안전합니까? 오늘은 암호화 (Encryption) 복호화(Decryption)에 대한 이야기를 나눠보고자 합니다. 암호와/복호와암호화는 데이터를 암호화 하여서 누군가가 읽을 수 없도록 정보를 전달화는 과정입니다. 암호와에는 여러가지 알고리즘이 쓰임니다. 복호와는 암호화된 정보를 다시 읽을 수 있게하는 과정으로써 데이터가 누출되더라도 복호화를 하지못하면 암호화된 데이터를 읽을 수 없습니다. 암호와 종류 단방향 암호: 암호화 후 복호화 할 수 없습니다. 예를 들면 사용자 비밀번호 사용자가 입력한 비밀번호를 암호화 하고 모든 접근자는 암호화 된 코드를 다시 평문으로 볼 수 없습니다. 해킹이 되어도 복호화가 굉장히 어렵습니다. 예외적인 경우로는 RainbowTable 이 있습니다. 더 자세한 정보: 참고영상 양방향 암호: 암호와와 복호화 모두 가능합니다. 사용자 주소, 이메일, 전자서명 등과 같이 정보를 재사용해야 되는 경우에 사용합니다. 양방향 암호에는 크게 두 가지 종류가 있습니다. 대칭형 암호 (비밀키 암호) 대칭형 암호는 암호화 할 때 사용하는 키와 복호화 할 때 사용하는 키가 동일한 암호화 기법입니다. 예를 들면 “APPLE”를 “ABCDE”로 암호화 했다면 복호화도 반드시 “ABCDE”로 해야됩니다. 예를 들면 AES Algorithm 하지만 대칭형 암호에는 키 배송에 관한 문제가 발생됩니다. 송신 측에서는 데이터를 암호화한 후에 수신 측에 암호키를 전달해야되고 전달하는 과정에서 이 함호 키가 털리면 데이터가 유출됩니다. 그리고 키 관리가 어렵습니다. 비대칭형 암호 (공개키 암호) 비대칭현 암호는 암호와 키와 복호화 키가 다릅니다. 클라이언트와 서버가 각각의 공개키와 비밀키를 갖고, 서로 공개키를 공개합니다. 클라이언트는 서버의 공개키로 데이터를 암호화한 후에 서버로 보내면 서버는 자신의 비밀키를 가지고 클라이언트가 보낸 데이터를 복호화 합니다. 예를 들면 RSA, Diffe-Hellman, ECC, etc 공개키 는 공유되지만 암호키 는 공개되지 않기에 공개키가 중간에 탈취되어도 데이터를 안전하게 지킬 수 있습니다. 하지만 문제는 비대칭형 암호는 대칭형 암호에 비해 느리고 많은 자료를 암호와 복호화 하는데 불편합니다 단점이 있습니다. 암호 알고리즘단방향 SHA : 가장 대표적인 해시함수 PBKDF2 : 해시함수의 컨테이너인 PBKDF2는 솔트를 적용한 후 해시 함수의 반복 횟수를 임의로 선택할 수 있다. PBKDF2는 구현하기 쉬운 알고리즘이며 SHA와 같이 검증된 해시 함수만 사용합니다. bcrypt : 패스워드 저장을 목적으로 설계되었으며 가장 많이 쓰이는 알고리즘입니다. 입력값을 72 byte로 해야하기 때문에 조금 사용에 불편함이 있을 수 있습니다. scrypt : scrypt는 상대적으로 최신 알고리즘이며 위에 알고리즘들 보다 더 성능적으로 뛰어난다고 평가되지만 잘 알려져 있지 않습니다. scrypt는 다이제스트를 생성할 때 메모리 오버헤드를 갖도록 설계되어, 억지 기법 공격 (brute-force attack)을 시도할 때 병렬화 처리가 매우 어렵습니다. 따라서 PBKDF2보다 안전하고 bcrypt에 비해 더 경쟁력 있다고 여겨집니다. 양방향 AES: 현재 가장 보편적으로 쓰이는 암호와 방식이며 미국 표준 방식인 AES. 128 ~ 256 byte 키를 적용 할 수 있어서 보안성이 뛰어난 공개된 알고리즘입니다. RSA : 공개키 암호 시스템의 하나로 암호와 뿐만 아니라 전자서명까지 가증한 알고리즘입니다. Referencehttps://sieunlim.tistory.com/16 https://record22.tistory.com/44","link":"/2019/12/07/%EC%95%94%ED%98%B8%ED%99%94%EB%B3%B5%ED%98%B8%ED%99%94/"}],"tags":[{"name":"simulation","slug":"simulation","link":"/tags/simulation/"},{"name":"Apache Nifi","slug":"apache-nifi","link":"/tags/apache-nifi/"},{"name":"암호화복호화","slug":"암호화복호화","link":"/tags/%EC%95%94%ED%98%B8%ED%99%94%EB%B3%B5%ED%98%B8%ED%99%94/"}],"categories":[{"name":"ENG-Statistics","slug":"eng-statistics","link":"/categories/eng-statistics/"},{"name":"ENG-Data Engineering","slug":"eng-data-engineering","link":"/categories/eng-data-engineering/"},{"name":"KOR-컴퓨터과학","slug":"kor-컴퓨터과학","link":"/categories/kor-%EC%BB%B4%ED%93%A8%ED%84%B0%EA%B3%BC%ED%95%99/"}]}