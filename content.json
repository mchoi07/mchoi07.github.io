{"pages":[{"title":"","text":"","link":"/404.html"},{"title":"About Minkyu Choi","text":"IntroMinkyu Choi is a Artificial Intelligence &amp; Machine Learning Engineer working in the various industries around 3 years as a consultant. He had worked in a commerical sectors for a couple of years and moved to government sectors. He has been exposed to diverse machine learning and deep learning projects, experienced in various big data engineering projects including Hadoop, Spark, Kafka, etc. Minkyu’s Expertises ML/AI Infrastructure Cloud Computing &amp; Architect (Amazon Web Service) Data Pipelining — Batch &amp; Streaming Machine Learning &amp; Deep Learning Github LinkedIn Email #contact-buttons { width: 100%; display: flex; } #contact-butons a.button { flex-grow: 1; } @media screen and (max-width: 720px) { #contact-buttons a.button { width: 100%; margin-right: 0; } } EducactionGeorgia Institute of TechnologyMS in Data Science Knowledge-Based Artificial Intelligence: Cognitive Systems Big Data for Healthcare Computational Data Analytics (Machine Learning) Machine Learning For Trading High-Dimensional Data Analysis Baruch CollegeBBA in Computer Information System Data Warehousing Database Management System Quantiative Decision Making Regression and Forecasting Modeling ProfessionalUnited States Department of Defense (The Joint Artifical Intelligence Center)Artificial Intelligence Engineer Project Overview: The Joint Artificial Intelligence Center (JAIC) is the Department of Defense’s (DoD) Artificial Intelligence (AI) Center of Excellence that provides a critical mass of expertise to help the Department harness the game-changing power of AI. Architecting an infrastructure of DoD AI/ML platform Developing a data strategy and capability of the JAIC Common Foundation (JCF) Participating a ML/DL model development within CI/CD ML DevOpsSec life cycle Techfield LLCSubject Matter Expert in Big Data and Data Science Managed +10 consultants in the big data and data science team, supporting the projects such as a cloud data pipeline solution and an automated application platform with a machine learning system Worked closely with one of the fortune 500 client in order to accomplish a goal of the datapipeline project for the entire batch and streaming processing and achieved a cost reductionand efficient processing time Endpoint ClinicalData Engineer Worked as a back-end database developer of IRT system – providing integrated solution of clinical trials (e.g., patient record, drug supply management and site management) in order to provide technical solution for large size of pharmaceutical clients Argus Information and Advisory ServicesData Analyst Developed strategies, best practices, and technical requirements to improve data manipulation and analysis by querying millions of rows of data across multiple databases, using MS SQL Server to identify and resolve data issues, and leveraged large sets of data to define benchmark models to enable client’s objectives to better manage data and drive insights SkillPrograming &amp; Scripting Language: Scala | Java | Python | R |Bash |Big Data Application: Hadoop | Spark | Kafka | Flume | Nifi | Sqoop | ELK |Database: MySql | Cassandra | Hbase | Elasticsearch | DynamoDB |","link":"/about/index.html"}],"posts":[{"title":"(ENG) Simulation 01 - Random Number","text":"A simulation is not real but it can represent the real. It’s because a simulation is an imitation of real situation - it can’t be exact but it can be approximate. Most of simulation models are strated from generating random number because randomness creates value on a simulation modeling. It is really important to give an algorithm that produces a sequence of pseudo-random number (PRNs) $R_1, R_2,…$ that “appear” to be iid Unif(0,1). There are many different Uniform(0,1) Generators. Output of random device Nice randomness properties. However, Unif(0,1) sequence storage difficult, sot it’s tough to repeat experiment Examples: flip a coin particle count by Geiger coutner least significant digits of atomic clock Table of random numbers List of digits supplied in tables - A Million random Digits with 100,00 Normal Deviates. Cumbersome, slow, table too small - not very useful Mid-Square Idea - Take the middle part of square of the previous random number. John von Neumann was a brilliant and fun-loving guy, but method is terrible Example: Take $R_i = X_i/10000$, ∀i, where the Xi’s are positiveintegers &lt; 10000. Set seed $X_0 = 6642$; then $6632^2$ = 43983424 so $X_1 = 9834$; then $9834^2$ - 96707556 so $X_2$ = 7075, etc,… Unfortunately, positive serial correlation in $R_i$’s. Also, occasionally degenerates; eg., consider $X_i$ = 0003 Fibonacci These methods are also no good!! Take $X_i = (X_{i-1} + X_{i-2})mod(m), i = 1,2,…,$ where $R_i = X_i/m$ ,$m$ is the modulus, $X_01,X_0$ are seeds, and $a = b mod m$ if $a$ is the remainer of $b/m$ Problem: small numbers follow small numbers Also, it’s not possible to get $X_{i-1} &lt; X_{i+1} &lt; X_i$ or $X_i &lt; X_{i+1} &lt; X_{i-1} $ (which should occur w.p 1/3) $X_{i+1}$ Linear congruential (most commonly used in practice LCGs are the most widely used generators. These are pretty good when implemented properly. $X_i = (aX_{i-1} + c) mod(m)$, where $X_0$ is the seed. $R_i = X_i/m, i = 1,2,…$ Choose a,c,m carefully to get good stastistical quality and long period or cycle length, i.e., time until LCG starts to repeat itself. If $c = 0$, LCG is called a multiplicative generator Tausworthe (linear recursion mod 2) Tausworthe Generator is a kind of multicative recursive generator. $X_{i+1} = (aX_{i-1} + c) mod(2)$, where $X_0$ is the seed. Reference Georgia Tech’s ISYE6644 class content","link":"/2019/07/18/random-number/"},{"title":"(ENG) Simulation 02 - Random variable","text":"Random variable can be generated from a good random number generator. If real variables has moved the reality, we could design a future with a good random variables. Author: Minkyu ChoiLast updated: 07/19/2019 Inverse Transform MethodInverse transform sampling is a method for generating random numbers from any probability distribution by using its inverse cumulative distribution F−1(x)F−1(x). Recall that the cumulative distribution for a random variable XX is FX(x)=P(X≤x)FX(x)=P(X≤x). In what follows, we assume that our computer can, on demand, generate independent realizations of a random variable UU uniformly distributed on [0,1] Cutpoint MethodThis inverse-transform method has the advantage of having an optimal O(n) setup time. However, the average number of steps required to sample X is not optimal, and if several samples of X are needed, then the cutpoint method offers an average number of two comparison steps needed to sample an observation, yet still has an O(n) initial setup time Without loss of generality, we can assume that X = [1, n]. Also, let qi = P(X ≤ i). Then the idea behind the cutpoint method is to choose m ≥ n, and define sets Q1, . . . , Qm for which for all i = 1, . . . , m. In words, the unit interval [0, 1] is partitioned into m equal sub-intervals of the form $[\\frac{(i−1)} m, \\frac{i}m)$, i = 1, . . . , m. And when U falls into the i th sub-interval, then Qi contains all the possible qj values for which F −1 (U) = j. That way, instead of searching through all of the q values, we save time by only examining the qj values in Qi , since these are the only possible values for which $F^{-1} (U) = j$. Convolution Method Sum of n variables: $x = y_1 + y_2 + … y_n$ Generate n random variate yi’s and sum For sums of two variables, pdf of x = convolution of pdfs of y1 and y2. Hence the name Although no convolution in generation If pdf or CDF = Sum ⇒ Composition Variable x = Sum ⇒ Convolution Acceptance-Rejection MethodFinding an explicit formula for F −1 (y) for the cdf of a rv X we wish to generate, F(x) = P(X ≤ x), is not always possible. Moreover, even if it is, there may be alternative methods for generating a rv distributed as F that is more efficient than the inverse transform method or other methods we have come across. Here we present a very clever method known as the acceptance-rejection method. Composition MethodCan be used when m can be expressed as a convex combination of other distributions Fi , where we hope to be able to sample from $F_i$ more easily than from F directly. ReferencesLink-1Link-2Link-3Link-4Link-5","link":"/2019/07/19/random-variable/"},{"title":"(ENG) Simulation 04 - Output Analysis","text":"Ananyzing the ouput of a simulation model is important. How can we be sure that our output is proper and will not hurt an experiment result using those outputs. Keep this in mind - out is rearely i.i.d. Why do we worry about output? In put processes driving a simulation are random variables. It means our output from the simulation must be random. If we runs the simulation it only yields estimates of measure of system performace, and these estimators are themselves random variables, and are therfore subject to sampling error. Sampling error must be taken into account to make valid inferences concerning system performance. Measures of Interest Means - what is the mean customer waiting time? Variances - how much is the waiting time liable to vary? Quantiles - what’s the 99% quantile of the line length in a certain queue? Sucess probabilities - will my job be completed on time? Would like point estimators and confidence intervals for the above. There are two general types of simulations with respect to output analysis. To facilitate the presentation, we identify two types of simulations with respect to output analysis: Finite-Horizon (Terminating) Simulations - Interested in short-term performance The termincation of finite-horizon simulation takes place at a specific time or is caused by the occurrence of a specific event. EX1 - Mass transit system during rush hour EX2 - Distribution system over one month Steady-State simulations - Interested in long-term performance The purpose of steady-state simulation is to study the long-run behavior of a system. A performance measure is a steady-state parameter if it is a characteristic of the equilibrium distribution of an output process. EX1 - Continuously operating communication system where the objective is the computation of the mean delay of a packet in the long run EX2 - Distribution system over a long period of time EX3 - Markov chains Finite-Horizon Simulation First thing we have to do to conduct this simulation is getting expected values from replications. So basically, we need to decide a number of independetn replications (IR). IR estimates Var($\\bar{Y}_m$) by conducting $r$ independent simulation runs (replications) of the system under study, where each replication consists of $m$ observations. It is easy to make the replications independent - just re-initialize each replication with a different pseudo-random number seed Sample means from replication If each run is started under the same operating conditions (e.g., all queues empty and idle), then the replication sample means $Z_1, Z_2, . . . , Z_r$ are $i.i.d.$ random variables. Suppose we want to estimate the expected average waiting time for the first m = 5000 customers at the bank. We make r = 5 independent replications of the system, each initialized empty andidle and consisting of 5000 waiting times. The resulting replicate means are: Steady-state simulation How about we need to simulate the entire time line? We should consider to use a steady-state simulation. Estimate some parameter of interest, e.g., the mean customer waiting time or the expected profit produced by a certain factory configuration. In particular, suppose the mean of this output is the unknown quantity $\\mu$. We’ll use the sample mean $\\bar{Y}_n$ to estimate $\\mu$ We must accompany the value of any point extimator with a measure of its variance. In stead of Var($\\bar{Y}_n)$ we canestimate the variance parameter, Thus, $\\sigma^2$ is imply the sume of all covariances! $\\sigma^2$ pops up all over the place: simulation output analysis, Brownian motions, fnancial engineering application, etc. Many methods for estimating $\\sigma^2$ and for conducting steady-state output analysis in general: Batch means The method of batch means (BM) is often used to estimate $\\sigma^2$ and to calculate confidence intervals for $\\mu$ Idea: Divide one long simulation run into a number of contiguous batches, and then appeal to a central limit theorem to assume that the resulting batch sample means are approximately i.i.d. normal. In particular, suppose that we partition $Y_1, Y_2, . . . , Y_n$ into $b$ nonoverlapping, contiguous batches, each consisting of $m$ observations (assume that $n = bm$) The $i$th batch mean is the sample mean of the $m$ observations from batch $i = 1, 2, . . . , b$ $E[H]$ decreases in b, though it smooths out around b = 30. A common recommendation is to take b =. 30 and concentrate on increasing the batch size m as much as possible. The technique of BM is intuitively appealing and easy to understand. But problems can come up if the Yj ’s are not stationary (e.g., if significant initialization bias is present), if the batch means are not normal, or if the batch means are not independent. If any of these assumption violations exist, poor confidence interval coverage may result — unbeknownst to the analyst. To ameliorate the initialization bias problem, the user can truncate some of the data or make a long run In addition, the lack of independence or normality of the batch means can be countered by increasing the batch size m. ReferenceThumnailGeorgia Tech’s ISYE6644 class content","link":"/2019/07/30/output-analysis/"},{"title":"(ENG) Simulation 03 - Input Analysis","text":"How can we tell our random variables are well made? In simulation terminology, we have something called input analysis. In my previous two postings, random number, random variable, I’ve talked about how to generate a random number and how we could make a radom variable by using those random numbers. Then how can we tell our random variables are well made? In simulation terminology, we have something called input analysis. The random variables are our input, and we need to analyze those inputs to verify it’s relevance. If you use the wrong input random variables in the simulation model, it will result in wrong output. A proper input analysis can save you from Garbage-in-garbage-out How can we conduct a proper input analysis? We have to collect data Data Sampling - we could shuffle the data and take some samples from there We have to figure out a distribution of data Plot the data to histogram Discrete vs contunuous Univariate / Multivariate If data is not enough — we can guess a good distribution We have to do a statistical test to verify the distribution 1. Point EstimationA statistic can not explicitly depend on any unknown parameters because statics are based on the actural observations. Statistics are random variable - we could expect to have two different values of a static when we take two different samples. But we need to find the unknown parameters. How can we do it? we could estimate unknown parameters from the existing probability of distribution. Let $X_1, . . . . , X_n$ be i.i.d. Random Variables and let $M(X) ≡ M(X_1, . . . . , X_n)$ be a statistic based on the $X_i$’s. Suppose we use $M(X)$ to estimate some unknown parameter $θ$ Then $M(X)$ is called a point estimator for $θ$ . $\\bar{x}$ is a point estimator for the mean $μ = E[Xi]$ $S^2$ is a point estimator for the variance $σ2 = Var(Xi) $ *It would be nice if $M(X)$ had certain properties: * Its expected value should equal the parameter it’s trying to estimate It should have low variance We all know that good estimator should be unbiased because if we use the biased estimator we won’t figure out whether our model is good or not. We will be fooled by a biased estimator. $T(X)$ is unbiased for $θ$ if $E[M(X)] = θ$ EX1 - Suppose $X_1, . . . , X_n$ are i.i.d. anything with mean μ. Then So $\\bar{X}$ is alwys unbiased for $\\mu$. That’s why ${\\bar{X}}$ is called the sample mean EX2 - Suppose $X_1, . . . , X_n$ are i.i.d. anything with mean μ and variance $\\sigma^2$. Then Thus, $S^2$ is always unbiased for $\\sigma^2$. This is why $S^2$ is called the sample variance. 2. Mean Square ErrorIn a perfect sceanario, our estimator will be exactly same as $\\theta$. Then we will see no error. However, it’s not the real case. Our goal is to reduce the error between an estimator and $\\theta$. The mean squared error of an estimator $M(X)$ of $\\theta$ is $$\\begin{aligned} MSE(M(X)) ≡ E[(M(X)-\\theta)^2]\\end{aligned}$$ $$\\begin{aligned} Baia(M(X)) ≡ E[T(X) - \\theta]\\end{aligned}$$ We could interpret MSE like this: Lower MSE mean we are avoiding the bias and variance. Our goal should be finding a good estimator which can lower our error. If $M_1(X)$ and $M_2(X)$ are two estimators of $\\theta$, we’d usually prefer the one with the lower MSE — even if it happens to have higer bias. 3. Maximum Linelihood EstimatorsWhat if we don’t have a set of data, but we have a pdf/pmf $f(x)$ of the distribution. How can we find the $\\theta$? Consider an i.i.d. random sample $X_1, . . . , X_n$, where each $X_i$ has pdf/pmf $f(x)$. Further, suppose that $\\theta$ is some unknown parameter from $X_i$. The likelihood function is $L(\\theta) ≡ \\prod f(x_i)$ The maximum likelihood estimator (MLE) of $\\theta$ is the value of $\\theta$ that maximizes $L(\\theta)$. The MLE is a function of the $X_i$’s and is a RV. EX1 - Suppose $X_1, . . . , X_n$ ~ Exp$(\\gamma)$. Find the MLE for $\\gamma$ Since the natural log function is one-to-one, it’s easy to see that the $\\gamma$ that maximizes $L(\\gamma)$ also maximize $ln(L(\\gamma))$ This implies that the MLE is $\\hat{\\gamma} = 1 / \\bar{X}$ Invariance Property If $\\hat{\\theta}$ is the MLE of some parameter $\\theta$ and $h(.)$ is a one-to-one function, then $h(\\hat{\\theta})$ is the MLE of $h(\\theta)$ For Bern(p) distribution the MLE of $p$ is $\\hat{p}=\\bar{X}$ (which also happens to be unbiased). If we consider the 1:1 function $h(\\theta) = \\theta^2$ for ($\\theta &gt; 0)$, then the Invariance property says that the MLE of $p^2$ is $\\bar{X}^2$ But such a property does not hold for unbiasedness $$\\begin{aligned}E[S^2] = σ^2\\end{aligned}but\\begin{aligned}E[\\sqrt{S^2}]= σ\\end{aligned}$$ Really that MLE for $\\sigma^2$ is $\\hat{\\sigma^2} = 1/n\\sum(X_i - \\bar{X})^2$. The good news is that we can still get the MLE for $\\sigma$. If we consider the 1:1 function $h(\\theta) = +\\sqrt{\\theta}$, then the invariance property says that the MLE of $\\sigma$ is $$\\begin{aligned}\\hat{\\sigma} = \\sqrt{\\hat{\\sigma^2}} = \\sqrt{\\sum(X_i-\\bar{X})^2/n}\\end{aligned}$$ 4. The Method of MomentsRecall: the $k$th moment of a random variable X is Suppose $X_1, . . . , X_n$ are i.i.d. from p.m.f. / p.d.f. $f(x)$. Then the method of moments(MOM) estimator for $E[X^k]$ is $$m_k ≡ 1/n\\sum X^k_i$$ 5. Goodness-of-Fit TestsWe finanlly guessed a reasonable distribution and then estimated the relevant parameters. Now let’s conduct a formal test to see just how sucessful our toils have been. In particular, test $$H_0 : X_1, X_2, . . . , X_n - p.m.f / p.d.f. f(x)$$ At level of significance $$\\alpha ≡ P(Reject H_0 | H_0 true) =P(\\text{Type I Error})$$ Chi-Square-TestGoodness-of-fit test procedure: Divide the domain of $f(x)$ into $k$ sets, say, $A_1, A_2, A_3, . . . , A_k$ (distinct points if X is discret r intervals if X is continuous) Tally the actual number of observations that fall in ach set, say, $O_i i = 1, 2, . . . , k$. If $p_i ≡ P(X ∈ Ai)$, then $O_i \\text{~ Bin(}n,p_i)$ Determine the expected number of observations that would fall in each set if $H_0$ were true, say, $E_i = E[O-I] = np_i, i = 1,2, . . . , k$ Calculate a test statistic based on the differences between the $E_i$ and $O_i$. A large value of $X^2_0$ indicate a bad fit. We reject $H_0 \\text{ if } \\chi^2_0 &gt; \\chi^2_{\\alpha,k-1-s}$, where $s$ is the number of nuknown paramets from $f(x) that have to be estimated. Usual recommendation: For the $\\chi^2$ g-o-f test to work, pick $k,n$ such that $E_i &gt;= 5$ and $n$ at least 30 Kolmogorov_Smirnov Goodness-of-Fit Test We’ll test $H_0 : X_1, X_2, . . . , X_n$ some distribution with $c.d.f. F (x)$. Recall the difincation of the empirical c.d.f. (also called the sample c.d.f) of the data is $$\\hat{F_n}(x) ≡ (\\text{number of } X_i &lt;= x) / n$$ The Glivenko-Cantelli Lemma says that $\\hat{F_n}(x) -&gt; F(x)$ for all $x$ as $ n-&gt;\\infinite$. So if $H_0$ is ture then $\\hat{F_n}(x)$ should be a good approxmination to the true c.d.f. $F(x)$, for large $n$ The main question: Does the empirical distribution actually support the assumption that H0 is true? ReferenceThumnailGeorgia Tech’s ISYE6644 class content","link":"/2019/07/23/input-analysis/"},{"title":"(ENG) Secure File Transfer Protocol","text":"SFTP (SSH File Transfer Protocol) is a secure file transfer protocol. It runs over the SSH protocol. It supports the full security and authentication functionality of SSH. System Requirement Ubuntu 16.04 Step 1 - OpenSSHFirst, we need to check the SSH connection. By default OpenSSH comes with the most of the Lunux system. Please confirm this with this command. 1ssh -v localhost If everything is good, you should be able to see this. 12&gt; debug1: Connecting to localhost [127.0.0.1] port 22.&gt; debug1: Connection established. If you don’t have OpenSSH set up. You should install it on your system. 123456sudo apt updatesudo apt install openssh-serversudo systemctl stop ssh.servicesudo systemctl start ssh.servicesudo systemctl enable ssh.service Step 2 - Create SFTP GROUP and USERCreate a New User Switch to the root user: 1sudo -s Add a new user 1adduser &lt;UbuntuUsername&gt; You will be prompted to add a password. Put a simple password and change it later. Create a Group We have to create the sftp_group first. You could name it whatever you want. 1sudo groupadd sftp_group Now, we could add user into this group 1sudo usermod -aG sftp_group &lt;UbuntuUsername&gt; Step 3 - Configure SFTP / ChrootA chroot enable system to isolate application form the rest of your computer by limiting them. If you turn on chroot on user account, the account will be isolated and can only access its own directory and files. There are two different ways which you could do access control. Locking down per user We might need to provide limited access to our user because if we give full access to our user it would be a huge security flaw. If you want to lock down user to only specific directory to add and remove files, please follow steps below. Create desired path and directory. 12#For example/home/sftp_root/sftp_home /home/sftp_root is owned by root while ../sftp_home can be ownd by our user or user group Change a permission 1chmod 755 /home/sftp_root This changes our permissions to only allow writing by the user who owns the directory while read and execute to everyone else. 12345#it changes a directory to be owned by the user root and group root.chown root:root /home/sftp_root#it gives ownership to the user and usergroup only to sftp_home.chown &lt;User&gt;:&lt;Usergroup&gt; /home/sftp_root/sftp_home Locking down user 1vi /etc.ssh/sshd_config Find this and comment it out 123Subsystem sftp /var/lib/openssh/sftp-server#to#Subsystem sftp /var/lib/openssh/sftp-server And add this: 12345678Subsystem sftp internal-sftp Match User [Your New Username] ChrootDirectory /home/sftp_rootX11Forwarding noAllowTcpForwarding noAllowAgentForwarding noForceCommand internal-sftpPasswordAuthentication yes Match User: Tells the SSH server to only apply the following settings to the one user ChrootDirectory: This tells the server what directory our user is allowed to ONLY work within this directory X11Forwading, AllowTCPForwarding, AllowAgentForwarding: Prohibits the user from port forwarding, tunneling and X11 forwarding fot the user. These are all security things. ForceCommand internal-sftp: Forces the SSH server to the run the SFTP program upon access which disables shell access. PasswordAuthentication: Allows for the user to login with a typed password. You can remove this is you would rather use a security key which is by far safer. 123sudo systemctl restart ssh.service#or/etc/init.d/ssh restart Locking down User GroupOnly step 4 is different from locking down per user. Add this: 1234567Subsystem sftp internal-sftpMatch Group sftp_groupX11Forwarding noAllowTcpForwarding noChrootDirectory /home/sftp_rootForceCommand internal-sftp ReferenceThumnaillink1link2","link":"/2019/11/18/sftp/"},{"title":"(KOR) 암호화복호화","text":"여러분들의 데이터는 안전합니까? 오늘은 암호화 (Encryption) 복호화(Decryption)에 대한 이야기를 나눠보고자 합니다. 암호와/복호와암호화는 데이터를 암호화 하여서 누군가가 읽을 수 없도록 정보를 전달화는 과정입니다. 암호와에는 여러가지 알고리즘이 쓰입니다. 복호와는 암호화된 정보를 다시 읽을 수 있게하는 과정으로써 데이터가 누출되더라도 복호화를 하지못하면 암호화된 데이터를 읽을 수 없습니다. 암호와 종류 단방향 암호: 암호화 후 복호화 할 수 없습니다. 예를 들면 사용자 비밀번호 사용자가 입력한 비밀번호를 암호화 하고 모든 접근자는 암호화 된 코드를 다시 평문으로 볼 수 없습니다. 해킹이 되어도 복호화가 굉장히 어렵습니다. 예외적인 경우로는 RainbowTable 이 있습니다. 더 자세한 정보: 참고영상 양방향 암호: 암호와와 복호화 모두 가능합니다. 사용자 주소, 이메일, 전자서명 등과 같이 정보를 재사용해야 되는 경우에 사용합니다. 양방향 암호에는 크게 두 가지 종류가 있습니다. 대칭형 암호 (비밀키 암호) 대칭형 암호는 암호화 할 때 사용하는 키와 복호화 할 때 사용하는 키가 동일한 암호화 기법입니다. 예를 들면 “APPLE”를 “ABCDE”로 암호화 했다면 복호화도 반드시 “ABCDE”로 해야됩니다. 예를 들면 AES Algorithm 하지만 대칭형 암호에는 키 배송에 관한 문제가 발생됩니다. 송신 측에서는 데이터를 암호화한 후에 수신 측에 암호키를 전달해야되고 전달하는 과정에서 이 함호 키가 털리면 데이터가 유출됩니다. 그리고 키 관리가 어렵습니다. 비대칭형 암호 (공개키 암호) 비대칭현 암호는 암호와 키와 복호화 키가 다릅니다. 클라이언트와 서버가 각각의 공개키와 비밀키를 갖고, 서로 공개키를 공개합니다. 클라이언트는 서버의 공개키로 데이터를 암호화한 후에 서버로 보내면 서버는 자신의 비밀키를 가지고 클라이언트가 보낸 데이터를 복호화 합니다. 예를 들면 RSA, Diffe-Hellman, ECC, etc 공개키 는 공유되지만 암호키 는 공개되지 않기에 공개키가 중간에 탈취되어도 데이터를 안전하게 지킬 수 있습니다. 하지만 문제는 비대칭형 암호는 대칭형 암호에 비해 느리고 많은 자료를 암호와 복호화 하는데 불편합니다 단점이 있습니다. 암호 알고리즘단방향 SHA : 가장 대표적인 해시함수 PBKDF2 : 해시함수의 컨테이너인 PBKDF2는 솔트를 적용한 후 해시 함수의 반복 횟수를 임의로 선택할 수 있다. PBKDF2는 구현하기 쉬운 알고리즘이며 SHA와 같이 검증된 해시 함수만 사용합니다. bcrypt : 패스워드 저장을 목적으로 설계되었으며 가장 많이 쓰이는 알고리즘입니다. 입력값을 72 byte로 해야하기 때문에 조금 사용에 불편함이 있을 수 있습니다. scrypt : scrypt는 상대적으로 최신 알고리즘이며 위에 알고리즘들 보다 더 성능적으로 뛰어난다고 평가되지만 잘 알려져 있지 않습니다. scrypt는 다이제스트를 생성할 때 메모리 오버헤드를 갖도록 설계되어, 억지 기법 공격 (brute-force attack)을 시도할 때 병렬화 처리가 매우 어렵습니다. 따라서 PBKDF2보다 안전하고 bcrypt에 비해 더 경쟁력 있다고 여겨집니다. 양방향 AES: 현재 가장 보편적으로 쓰이는 암호와 방식이며 미국 표준 방식인 AES. 128 ~ 256 byte 키를 적용 할 수 있어서 보안성이 뛰어난 공개된 알고리즘입니다. RSA : 공개키 암호 시스템의 하나로 암호와 뿐만 아니라 전자서명까지 가증한 알고리즘입니다. Referencehttps://sieunlim.tistory.com/16 https://record22.tistory.com/44","link":"/2019/12/07/%EC%95%94%ED%98%B8%ED%99%94%EB%B3%B5%ED%98%B8%ED%99%94/"},{"title":"(KOR) 머신러닝 속성코스 01 - Overview","text":"구글의 Tensorflow로 배워보는 머신러닝 속성학습 지금부터 시작합니다. 안녕하세요 AI Nomad 최민규입니다. 앞으로 진행하게 될 AI에 관련된 다양한 프로젝트들과 콘텐츠에 앞서 어떤 프로젝트로 시작하면 좋을까 생각하다가 구글에서 있는 Machine Learning Crash 코스를 go over 하면서 기본적인 머신러닝에 대한 개념을 정리하면 어떨까 생각해 봤습니다. 구글에서는 이 코스를 이미 한국어로 번역해 두었지만. 번역이 조금 서툴더군요. 그래서 제가 구글의 자료를 쭉 정리하며 조금 더 자세한 설명을 붙여가면서 이 컨탠츠를 진행해 보도록 하겠습니다. ReferenceThumnailGoogle Machine Learning Crash Course Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.","link":"/2019/12/13/mlcrash01-overview/"},{"title":"(ENG) Realtime Virus Scanning","text":"To protect our system and computer we should make sure that data which we download is clean. Everytime we bring data to our system or user upload data such as file attachments, we must make sure that data is free from viruses and trojans. If our system has sensitive data and critical for operation you have to be more cautious about bringing data to your system - cyber attack, nowadays, is being serious and cunning. In a normal usecase, we set up Anti Virus (AV) scanner on a file system. AV scanner monitor our file system and RAM in real-time or batch. However, it cannot make sure that each file doesn’t have any malicious content in real-time. In this project, we will use two open source products to detect virus/trojan in realtime. We are going to use Apache Nifi and ClamAV Apache Nifi is a very powerful, easy to use and stable system to process and distribute data between disparate system. Apache Nifi is a real time data ingestion platform, which can transfer and manage data transfer between different sources and destination systems. ClamAV is an open source antivirus engine for detecting trojans, viruses, malware &amp; other malicious threats. 1. UsecaseA usecase is that user need to transfer some files to the applicaion, and we have to make sure that the files don’t contain any malicious codes or contents. Since this is not bulk transformation, we want to transfer a file to endpoint in realtime after scanning. A diagram below is a high level work flow of this usecase. 2. Setting Nifi ServerThere are many different ways that you could set up Nifi server depending on the operating system. In this project, I am using Ubuntu 16.04. Updating and Upgrading apt-get1234apt-get autocleanapt-get clean allapt-get -y updateapt-get -y upgrade Installing Java (JRE)Apache Nifi is built on Java. We have to have java installed in the system 1apt install oracle-java8-installer -y Installing Nifi12345wget \"https://www-us.apache.org/dist/nifi/1.10.0/nifi-1.10.0-bin.tar.gz\"mkdir /opt/nifitar -xvzf nifi-1.10.0-bin.tar.gz --directory /opt/nifi --strip-components 1 Set JAVA_HOME123vim ~/.bash_profileexport JAVA_HOME=/usr/lib/jvm/java-8-oraclesource ~./bashrc Start/Stop Apache Nifi123/opt/nifi/bin/nifi.sh start#or/opt/nifi/bin/nifi.sh stop Get StartedYou should open a browser to access NiFI GUI. 1234#default#http://localhost:8080/nifi#or#http://IP-Address:8080/nifi If you need to change port: 12vi /opt/nifi/conf/nifi.properties# change the defalt port to what you desire If everything is good you should be able to see this screen. 2. Setting ClamAV Server at restWe are going to deply a virus scanner and make it usable in a server at REST. Even though we have multiple applications like one for email attachment, SFTP, etc., we just need to deploy a AV scanner for many applicaions. Simple Clam AV REST Proxy. This will be built on top of clamav-java. Pleas fine more detail here. We need two containers. One is ClamAV daemon as a Docker images. It builds with a current virus database and runs freshclam in the background constantly updating the virus signature database. clamd itself is listening on exposed port 3310. Another one is the server implementation. This is a precompiled and packaged docker container running the server. You also need the ClamAV virus scanner for the REST endpoint. To run use something like this. Start ClamAV server, using https://hub.docker.com/r/mkodockx/docker-clamav/ here docker run -d --name clamav-server -p 3310:3310 mkodockx/docker-clamav Test that it’s running ok: curl localhost:3310 UNKNOWN COMMAND Start the REST API image, clamd-server docker container linked to this container. docker run -d -e 'CLAMD_HOST=clamav-server' -p 8080:8080 --link clamav-server:clamav-server -t -i lokori/clamav-rest Test the REST api: curl localhost:8080 Clamd responding: true Testing the REST service You can use curl as it’s REST. Here’s an example test session: 12345curl localhost:8080Clamd responding: truecurl -F &quot;name=blabla&quot; -F &quot;file=@./eicar.txt&quot; localhost:8080/scanEverything ok : false EICAR is a test file which is recognized as a virus by scanners even though it’s not really a virus. Read more EICAR information here. 3. Design Dataflow in NifiIn our previous discussion, we’ve setup nifi server. We’ll use three processor to make it working. GetFile, ExecuteStreamCommand, RouteOnAttribute and PutFile. GetFile and PutFile can be chnaged to any endpoint of your application. For example, we could get a file from SFTP and put file to HDFS. I would like to focus on ExecuteStreamCommand becase rest of processors are straight forward. Please find more information about those processors from an offical Apache Nifi Website. ExecuteStreamCommand will executes an external command on the contents of a flow file, and creates a new flow file with the results of the command. We will use Python. Therefore, when files are come from GetFile Nifi will execute a python script to check the virus via API from ClamAV server. You should install python properly in your Nifi server. ![ExecuteStreamCommand Configuration](/2019/11/20/nifi-virus-scanning/ExecuteStreamCommand Configuration.png) This is a configuration of Command Arguments Command Path is where your python command located. Working Directory is where your python script is located. Command Arguments is your python script OutPut Destination Attribute Make sure that you define this value because we have to keep our content of file. By doing so we will write the result of scanning as an attribute. And then we will sort out files throught RouteOnAttribute processor. Let’s take a look our python script. 12345678910111213import sysimport requestsif __name__ =='__main__': url = 'http://localhost:9090/scan' payload = {'name': 'value1'} systemin = sys.stdin files = {'file': systemin} r = requests.post(url, files=files, data=payload) if 'false' in r.text: sys.stdout.write(\"False\") sys.stdout.write(\"True\") If a file is clean, it will have an Attribute value True. If it’s not an attribute value will look like this FalseTrue . We will route our files based one this value. Let’s check the configuration of RouteOnAttribute. It will let your nifi to send your files to next processor only if the files are clean. 4. ConclusionIt might not be only way to do this process. However, both Nifi and Clam are open source, so we don’t need to purchase any other license like Mcafee. Also with these simple tools we could process the files in realtime, and it works fairly well! Hopefully you enjoy this article. If you have a question or comment, you are very welcome to email me at any time. Referenceslink1link2link3link4","link":"/2019/11/20/nifi-virus-scanning/"},{"title":"(KOR) 머신러닝 속성코스 02 - 기본용어","text":"supervised unsupervised feature label … 머신러닝을 시작하게 되면 새로 배워야 하는 용어들이 많죠? 하지만 이러한 용어들을 자신의 개념으로 잘 정리하는 것이 참 중요합니다. 왜냐하면, 우리가 앞으로 배우게 될 머신러닝의 기초가 되기 때문이죠… 안녕하세요 AI Nomad 최민규입니다. 머신러닝 속성코스 두 번째 세션에서는 머신러닝에서 사용되는 기본적인 용어들을 정리해보려 합니다. 1. Supervised and Unsupervised Learning머신러닝을 처음 시작하게 되면 가장 먼저 알게 되는 용어는 Supervised Learning 과 Unsupervised Learning 입니다. 영어로 supervised라고 하면 감독이 돼다, 관리되다 정도로 해석됩니다. 따라서 supervised learning 을 직역하면 감독의 지시 아래 배워지다 정도로 해석됩니다. 우리가 무언가를 배울 때 좋은 감독이 있으면 배움이 편합니다. 왜냐하면, 그들이 우리가 무엇을 배워야 할지 감독해주고 지시해주기 때문입니다. 머신러닝에서 supervised learning 도 비슷한 의미가 있다고 할 수 있습니다. 컴퓨터가 사람의 감독 아래 지시되고 학습되게 되는 거죠. 감독들은 선수들을 지도하고 지시합니다. A와 B를하라고 왜냐하면 감독들은 A와 B를 했을때 C라는 결과가 나올 거를 경험을 통해 알기 때문이죠. 머신러닝에서 A 와 B 를 feature 이라고 부릅니다. 그리고 C 를 label 이라고 부르죠. supervised learning 을 학습시키기 위해서는 feature 와 label 이 필요합니다. 감독이 있기 때문이죠. 그렇다면 감독이 존재하지 않는 unsupervised learning 은 어떨까요? 감독은 없고 선수들만 있는 팀이 있다고 가정합시다. 선수들은 매주 경기에 나가야 하죠. 구단주는 선수들에게 리그 3위 안에 들지 않으면 팀을 해체하겠다고 합니다. 감독이 없다는 것은 label 이 없다는 것이죠. 선수들은 직관적으로 어떠한 훈련을 해야 할지는 알 수 있을 것입니다. 하지만 감독의 경험이 없는 것이죠. 그러므로 unsupervised learning 에는 feature 만이 존재합니다. 이 상황에서 선수들은 어떻게 감독 없이 팀을 빌딩 할 수 있을까요? 스스로 답을 찾아야죠. 훈련과 경기를 통한 trial and error 즉 시행착오를 거쳐서 선수들 스스로 배워가는 수밖에 없겠죠. 이것이 머신러닝에서 unsupervised learning 이라고 불리는 것입니다. 데이터에는 feature 들만 존재하고 label 이 없습니다. 따라서 여러 알고리즘들이 feature 을 가지고 시행착오를 거쳐 가장 이상적인 label 을 찾아가는 것을 우리는 unsupervised learning 이라고 합니다 2. Labels (결과값)앞에서 나온 label은 어떠한 x들에 대한 y 즉 결과값이라고 정리 할 수 있겠습니다. 예를 들면, 미래의 주식가격, 사진 속에 등장한 동물의 종류, 어떤 소리에 의미 등이 있겠군요. 3. Feature (원인)Feature은 간단하게 어떤 결과에 대한 원인입니다. y라는 결과에 대한 x라는 원인이죠. 대학교 성적을 결과라고 한다면 이 결과에 대한 feature 즉 원인으로는 하루 공부 시간, 연애 여부, 아르바이트 여부, 등등이 있겠네요. 4. Model (모델)모델은 feature와 feature 사이 또 feature와 label의 관계라고 보시면 됩니다. 모델은 A와 B라는 feature들의 관계를 설명하고 A,B 라는 feature들과 C라는 label의 관계를 설명합니다. 이들의 관계를 잘 설명하는 모델을 우리는 좋은 모델이라고 부르며 좋은 결과를 내게 됩니다. 5. Train (학습)우리가 모델을 이야기할 때 train 시킨다 학습 시킨다는 용어를 많이 사용합니다. 모델을 학습시킨다 즉 train 한다는 말은 쉽게 말하면 모델을 만든다 모델을 학습시킨다고 이해하시면 되겠습니다. 그렇다면 모델을 만들고 학습시킨다는데 어떻게 무엇으로 학습시킨다는 거죠? 바로 데이터입니다. 데이터에 존재하는 많으면 많은 적으면 적은 feature들과 lable을 통해서 모델을 학습시키는 것입니다. 우리가 강아지에게 손을 달라고 했을 때 손을 주는 훈련을 한다고 가정을 해봅시다. 강아지가 이 훈련을 습득하는 방법은 보통 다음과 같겠죠 손 -&gt; 간식 (o)발 -&gt; 간식 (x)누움 -&gt; 간식 (x)손 -&gt; 간식 (o)손 -&gt; 간식 (o)손 -&gt; 간식 (o) 강아지는 손을 줬을 때 간식을 먹었다는 데이터를 기반으로 학습했고 사람이 손이라고 이야기했을 때 손을 주는 행동을 하게 되죠. 머신러닝도 마찮가지로 데이터를 기반으로 어떤 모델을 학습하고, 우리가 모델에 input을 주었을 때 모델은 학습된 데이터를 바탕으로 어떤 output 즉 inference 를 만들어 내게 됩니다. 6. Inference (암시)Inference는 unlabeled example을 학습된 모델에 적용할 때 사용합니다. 즉 모델을 학습시키고 새로운 input을 모델에 넣었을 때 모델이 만드는 output을 inference라고 부르죠. 결국 이 output도 예측값이기 때문에 결과를 암시한다 라는 의미로 inference라고 부릅니다. 7. 요약데이터를 빼놓고는 머신러닝을 이야기 할 수 없습니다. 데이터가 quantitative or qualitative 한지를 떠나서 머신러닝에서 데이터를 보는 관점은 딱 두 가지 입니다. feature 와 label. 이러한 feature와 label의 관계를 설명하는 것이 모델이고, 모델을 만들기 위해서는 데이터를 가지고 모델을 train (학습) 시키는 과정이 필요합니다. 모델을 통해 생성된 결과값 즉 prediction이 바로 inference 가 되게 되는 것이죠. 이렇게 이번 세션에서는 기본적인 머신러닝 용어들에 대해 배워봤습니다. 다음 세션은 본격적으로 모델에 사용되는 알고리즘과 알고리즘이 학습되는 로직에 대한 세션을 준비하겠습니다. &gt;&gt; Previous: Overview ReferenceThumnailGoogle Machine Learning Crash Course Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.","link":"/2019/12/16/mlcrash02-keyterm/"},{"title":"(ENG) Feature-Engineering-101","text":"If you ask yourself what’s the most important thing in machine learning, what’s your answer? All data scientist would have different answers. Among the other many answers, I believe feature engineering is one of the most important in machine learning. Sometimes, it’s a more critical step than a model selection and training a model because a model cannot improve a model itself even though we put a lot of effort on a hyper parameter turning. However, well selected/extracted features could be applied to many different models and improve a performance. A feature engineering includes feature selection and feature extraction. A feature selection is trial-error process to select relevant features from existing features. Since all features are simply selected from original features it’s easy to interpret what those features means. However, it is difficult to consider a relationship in selected features. On the other hands, a feature extraction is more like functional process to extract relevant features from existing features. It requires a form of function which enable an algorithm to create/extract a new set of features. A relationship between features will be considered and number of features could be significantly reduced. Yet, an interpretation of extracted features is not easy. We should use different methods of feature engineering depending on a machine learning algorithm we want to use. In supervised learning, we could select features form Information gain, Stepwise regression, LASSO, Genetic algorithms, etc. If we want to extract features, Partial Least Squares (PLS) is an option. In unsupervised learning, we could do a feature selection with PCA loading; a feature extraction uses Principal component analysis (PCA), Wavelets transforms, Autoencoder, etc. ReferenceThumnail","link":"/2020/01/23/feature-engineering-101/"},{"title":"(ENG) Naive Bayes from scratch","text":"Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predcitors) in a learning problem. Maxumum-likelihood training can be done by evaluting a closed-form exporession, which takes linear time, rather tahn by expensive iterative approximation as used for many other typs of classifier. Wikipedia Usecase - Spam filterWe will use the Naive Bayes algorithm to fit a spam filter. Spam filters are used in alal email services to classify received emails as “Spam” or “Not Spam”. A simple apporach involves maintaining a vocabulary of words that commonly occur in “Spam” emails and classifying an email as “Spam” if the number of words from the dictionary that are present in the email is over a certain threshold. Assume we are given the vocabulary consists of 15 words $V$ = {secret, offer, low, price, valued, customer, today, dollar, million, sports, is, for, play, healthy, pizza} We will use $V_i$ to represent the ith word in $V$. As our training dataset, we are also given 3 example spam messages: • million dollar offer • secret offer today • secret is secret and 4 example non-spam messages• low price for valued customer • play secret sports today• sports is healthy• low price pizza 1234567import numpy as npV = ['secret', 'offer', 'low', 'price', 'valued', 'customer', 'today', 'dollar', 'million', 'sports', 'is', 'for', 'play', 'healthy', 'pizza']message = ['million dollar offer','secret offer today','secret is secret', 'low price for valued customer', 'play secret sports today', 'sports is healthy', 'low price pizza'] train is our input vector x corresponding to each training message, and it has length n = 15 (length of V). Since we have 7 training example of message, We will have 7 by 15 training data - 7 data 15 features. 12train = np.zeros((7, 15))label = [0,0,0,1,1,1,1] 12345678'''Converting 7 training message data set to x vector which has length n = 15'''for i in range(len(train)): for j in range(len(V)): for k in range(len(message[i].split(\" \"))): if V[j] == message[i].split(\" \")[k]: train[i,j] += 1 123456'''This is the feture x matrix.row = n messagecol = i_th feature vector '''train array([[0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.], [1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], [0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.], [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])Let’s list them separately 1234567for i in range(len(label)): if label[i] == 0: print(\"this is {}th message - It's SCAM\".format(i)) print(train[i]) else: print(\"this is {}th message - It's NOT SCAM\".format(i)) print(train[i]) this is 0th message - It&apos;s SCAM [0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.] this is 1th message - It&apos;s SCAM [1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] this is 2th message - It&apos;s SCAM [2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] this is 3th message - It&apos;s NOT SCAM [0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.] this is 4th message - It&apos;s NOT SCAM [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0.] this is 5th message - It&apos;s NOT SCAM [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.] this is 6th message - It&apos;s NOT SCAM [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]123456789101112131415'''spam and ham array will be count of x_i when each message is spam or not spam.spam = (Y=0|x_i)ham = (Y=1|x_i)'''spam = np.zeros((1, 15))ham = np.zeros((1, 15))for c in range(len(label)): for i in range(train.shape[1]): if label[c] == 0: # Spam spam[0,i] += train[c,i] else: # Not Spam ham[0,i] += train[c,i] 1spam array([[3., 2., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0.]])If you see a result above, spam[0] = 3 ,spam[1] = 2 , spam [2] = 0, it means total count of x0 = secret is three among training data classified as spam. We have two offer and zeor low in our training data. 1234567891011121314151617'''In this section, we will calculate probabiliy of each word xi when a message is spam or not.prob_spam = P(x_i|y=0)prob_ham = P(x_i|y=1)'''spam_counter = 0ham_counter = 0for c in range(len(label)): if label[c] == 0: spam_counter += 1 else: ham_counter += 1 prob_spam_x_i = spam/spam_counterprob_ham_x_i = ham/ham_counter 1prob_spam_x_i array([[1. , 0.66666667, 0. , 0. , 0. , 0. , 0.33333333, 0.33333333, 0.33333333, 0. , 0.33333333, 0. , 0. , 0. , 0. ]])1prob_ham_x_i array([[0.25, 0. , 0.5 , 0.5 , 0.25, 0.25, 0.25, 0. , 0. , 0.5 , 0.25, 0.25, 0.25, 0.25, 0.25]])123456789101112131415161718192021'''Bayes TherormCalculating P(y=0|x_i)prob_spam_per_word = P(y=0|x_i)'''prob_spam_per_word = np.zeros((1, 15))prob_ham_per_word = np.zeros((1, 15))prob_spam = 3/7prob_ham = 4/7for i in range(prob_spam_per_word.shape[1]): spam_a = (prob_spam_x_i[0,i]*prob_spam) spam_b = spam_a + (prob_ham_x_i[0,i]*prob_ham) spam_c = spam_a/spam_b ham_a = (prob_ham_x_i[0,i]*prob_ham) ham_b = ham_a + (prob_spam_x_i[0,i]*prob_spam) ham_c = ham_a/ham_b prob_spam_per_word[0,i] = spam_c prob_ham_per_word[0,i] = ham_c 1prob_spam_per_word array([[0.75, 1. , 0. , 0. , 0. , 0. , 0.5 , 1. , 1. , 0. , 0.5 , 0. , 0. , 0. , 0. ]])The result above means that the probability of spam per i_th words. For example, the probability of spam when we have secret is 75% and a probability of spam of offer is 100%. 1prob_ham_per_word # This is the probability of ham per words. array([[0.25, 0. , 1. , 1. , 1. , 1. , 0.5 , 0. , 0. , 1. , 0.5 , 1. , 1. , 1. , 1. ]])Since we don’t have enough data, I will just use training dataset to evaluate the bayes classifier 1234567891011121314151617181920212223threshold = 0.50 # In what percentage do you want to classfy an eamil as spam.for t in range(train.shape[0]): conditional_prob_spam = prob_spam conditional_prob_ham = prob_ham for i in range(prob_spam_per_word.shape[1]): if train[t][i] == 1: conditional_prob_spam = conditional_prob_spam * prob_spam_per_word[0,i] conditional_prob_ham = conditional_prob_ham * prob_ham_per_word[0,i] if conditional_prob_spam != 0: prob = conditional_prob_spam / (conditional_prob_spam + conditional_prob_ham) * 100 else: prob = 0.0 if prob &gt; threshold*100: label = \"SPAM\" else: label = \"NOT SPAM\" print(\"{} th email is {} with a probability of being spam {}%\".format(t,label,prob)) #print(prob) 0 th email is SPAM with a probability of being spam 100.0% 1 th email is SPAM with a probability of being spam 100.0% 2 th email is NOT SPAM with a probability of being spam 42.857142857142854% 3 th email is NOT SPAM with a probability of being spam 0.0% 4 th email is NOT SPAM with a probability of being spam 0.0% 5 th email is NOT SPAM with a probability of being spam 0.0% 6 th email is NOT SPAM with a probability of being spam 0.0%Given a new message “today is secret”, decide whether it is spam or not spam, based on the Naive Bayes classifier, learned from the above data. 123456'''\"today is secret\"Vectorizing the email.. '''target = np.array([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.])target[0] 12345678910111213141516171819202122232425'''Please play around with different threshold. '''threshold = 0.50 # In what percentage do you want to classfy an eamil as spam.conditional_prob_spam = prob_spamconditional_prob_ham = prob_hamfor i in range(prob_spam_per_word.shape[1]): if target[i] == 1: conditional_prob_spam = conditional_prob_spam * prob_spam_per_word[0,i] conditional_prob_ham = conditional_prob_ham * prob_ham_per_word[0,i]if conditional_prob_spam != 0: prob = conditional_prob_spam / (conditional_prob_spam + conditional_prob_ham) * 100else: prob = 0.0if prob &gt; threshold*100: label = \"SPAM\" else: label = \"NOT SPAM\"print(\"{} th email is {} with a probability of being spam {}%\".format(t,label,prob))#print(prob) 6 th email is SPAM with a probability of being spam 69.23076923076923%","link":"/2020/03/23/Naive%20Bayes%20from%20scratch/"},{"title":"(ENG) Boosting (AdbBoost)","text":"In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. Wikipedia BoostingToday, I would like to introduce Boosting method in Machine Learning. Basically, Boosting is a set of algorithms (classifiers) which changes weak learner to strong learners. It is one of the ensemble methods for improving the model predictions of any given learning algorithm. However, unlike the regular ensemble method which we group several model and use them in parallel, Boosting is using a single model in sequential order with differnt weights. Boosting is also using ramdom sampling with a replacement. It will start training model from Sample 1 to Sample N. In each training, there would be well-classified data and wrong-classified data. Since it allows a replacement, some wrong classified data might be included in different samples or not.![Boosting Algorithm](/2020/04/01/boosting/Boosting Algorithm.png) Since each previous model affect a current model by assigning weights to data which it coudln’t classify correctly, this is not parallel - it’s sequential. At the end, it will use all trained models with different weights and generate the output. One of the disadvantages of Boosting is that it easily get corrupted by outliers because it will assign heavy weights to those outliers and it will mislead the models. AdaBoost (Adaptive Boosting)![Adaboost flow chart](/2020/04/01/boosting/Adaboost flow chart.png) AdaBoost works in a way putting more weights on difficult to classify data and less on those already handled well. In AdaBoost, we use something called Decision Stumps, the simplest model we could construct on data. It split the data into two subsets based on the feature. To find the best decision stump, we should lay out all features of data along with every possible threshold and look for one gives us best accuracy. In this example, I will consturct AdaBoost from the scratch with some mathematical explanation. ExampleX1 = (−1,0,+), X2 = (−0.5,0.5,+)X3 = (0,1,−), X4 = (0.5,1,−)X5 = (1,0,+), X6 = (1,−1,+)X7 = (0,−1,−),X8 = (0,0,−) How to? Constructing $D_t$. It’s basically weight of each $i^{th}$ data. $D_{t+1}(i)$ = $\\frac{D_t(i)}{Z_t} * e^{-\\alpha_t}$ if $y_i = h_t(x_i)$$D_{t+1}(i)$ = $\\frac{D_t(i)}{Z_t} * e^{\\alpha_t}$ if $y_i \\ne h_t(x_i)$, where $Z_t$ = Normalization Constant = Sum of $D_t$ and $\\alpha_t$ = $\\frac{1}{2}ln(\\frac{1-\\epsilon_t}{\\epsilon_t})$. $h_t: \\epsilon_t = \\sum\\limits_{i=1}^{m}D_t\\vert(y_i\\ne h_t(x_i))$ if $(y_i\\ne h_t(x_i)$ then 1 otherwise 0​ Step 1 Put an initial random decision stump aka classifier $h_1$. h1 is a randomly assinged decision stump.Let’s get $D_1$ = $\\frac{1}{m}$, where $m = 8$ t err alpha Z d1 d2 d3 d4 d5 d6 d7 d8 1 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 Now, in a second row, I will put 1 if $h_1$ classify $y_i$ incorrectly, else 0. t err alpha Z d1 d2 d3 d4 d5 d6 d7 d8 1 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0 1 0 0 0 0 1 1 $h_1$ has classified correctly $d_t(1),d_t(3),d_t(4),d_t(5),d_t(6)$ Once we have $D_1(i)$ and a result of classification by $h_1$ we could calculate $\\epsilon_1$, where $h_t: \\epsilon_t = \\sum\\limits_{i=1}^{m}D_t\\vert(y_i\\ne h_t(x_i))$ if $(y_i\\ne h_t(x_i)$ then 1 otherwise 0 In a thrid row, I will get all $D_1\\vert(y_i\\ne h_1(x_i)$, and if take sumation of all values, it’s our $\\epsilon_1$. t err alpha Z d1 d2 d3 d4 d5 d6 d7 d8 1 0.375 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0 1 0 0 0 0 1 1 0 0.13 0 0 0 0 0.13 0.13 Let’s get $\\alpha_1$ and $Z_1$. Remember $Z_t$ = Normalization Constant = Sum of $D_t$, and $\\alpha_t$ = $\\frac{1}{2}ln(\\frac{1-\\epsilon_t}{\\epsilon_t})$. t err alpha Z d1 d2 d3 d4 d5 d6 d7 d8 1 0.375 0.255 1.000 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0 1 0 0 0 0 1 1 0 0.13 0 0 0 0 0.13 0.13 If you see the table above, you could notice that d2, d7, d8 has more weight, 0.13 becase $h_1$ failed to classify them correctly. Based on this information, we could move onto second iteration. In order to calculate $D_2(i)$ we need additional information other than table above. $D_{t+1}(i)$ = $\\frac{D_t(i)}{Z_t} * e^{-\\alpha_t}$ if $y_i = h_t(x_i)$$D_{t+1}(i)$ = $\\frac{D_t(i)}{Z_t} * e^{\\alpha_t}$ if $y_i \\ne h_t(x_i)$ We need to what what’s $e^{-\\alpha_1}$ and $e^{\\alpha_1}$ $e^{-\\alpha_1}=0.775$ $e^{\\alpha_1}=1.291$ t err alpha Z d1 d2 d3 d4 d5 d6 d7 d8 1 0.375 0.255 1.000 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0 1 0 0 0 0 1 1 0 0.13 0 0 0 0 0.13 0.13 2 0.323 0.371 0.968 0.097 0.161 0.097 0.097 0.097 0.097 0.161 0.161 We’ve got new weights for data. d1 was 0.125; it has become 0.097. d2 has become 0.161 from 0.125 because $h_1$ failed to classfy this data. Let’s get the $h_2$ - I was focusing on having all +in the same group. If you do same calculation we’ve done above, You will get this table: t err alpha Z d1 d2 d3 d4 d5 d6 d7 d8 1 0.375 0.255 1.000 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0 1 0 0 0 0 1 1 0 0.13 0 0 0 0 0.13 0.13 2 0.323 0.371 0.968 0.097 0.161 0.097 0.097 0.097 0.097 0.161 0.161 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0.16 0.16 $e^{-\\alpha_2}=0.490$ $e^{\\alpha_2}=2.041$ Lastly, let’s do the final iteration. t err alpha Z d1 d2 d3 d4 d5 d6 d7 d8 1 0.375 0.255 1.000 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0 1 0 0 0 0 1 1 0 0.13 0 0 0 0 0.13 0.13 2 0.194 0.713 0.968 0.097 0.161 0.097 0.097 0.097 0.097 0.161 0.161 0 0 1 1 0 0 0 0 0 0 0.1 0.1 0 0 0 0 3 0.049 0.082 0.204 0.204 0.049 0.049 0.082 0.082 A reasoning of $h_3$ is to have all o in a same group. Also, d5 and d6 have never been misclassified, so I want to have a information of d5 and d6 in my model. And the result will be: t err alpha Z d1 d2 d3 d4 d5 d6 d7 d8 1 0.375 0.255 1.000 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0 1 0 0 0 0 1 1 0 0.13 0 0 0 0 0.13 0.13 2 0.323 0.371 0.968 0.097 0.161 0.097 0.097 0.097 0.097 0.161 0.161 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0.16 0.16 3 0.138 0.916 0.943 0.069 0.115 0.069 0.069 0.069 0.069 0.241 0.241 0 0 0 0 1 1 0 0 0 0 0 0 0.07 0.07 0 0 Graphically, it will look like: Lastly, the final classifier is $H_{final}(x)=sign(0.255h_1+0.371h_2+0.943*h_3)$","link":"/2020/04/01/boosting/"}],"tags":[{"name":"Simulation","slug":"simulation","link":"/tags/simulation/"},{"name":"Linux","slug":"linux","link":"/tags/linux/"},{"name":"암호화복호화","slug":"암호화복호화","link":"/tags/%EC%95%94%ED%98%B8%ED%99%94%EB%B3%B5%ED%98%B8%ED%99%94/"},{"name":"Apache Nifi","slug":"apache-nifi","link":"/tags/apache-nifi/"},{"name":"Feature Engineering","slug":"feature-engineering","link":"/tags/feature-engineering/"},{"name":"Naive Bayes Classifier","slug":"naive-bayes-classifier","link":"/tags/naive-bayes-classifier/"},{"name":"Ensemble","slug":"ensemble","link":"/tags/ensemble/"},{"name":"Boosting","slug":"boosting","link":"/tags/boosting/"},{"name":"AbaBoost","slug":"ababoost","link":"/tags/ababoost/"}],"categories":[{"name":"Statistics","slug":"statistics","link":"/categories/statistics/"},{"name":"DevSecOps","slug":"devsecops","link":"/categories/devsecops/"},{"name":"Machine Learning","slug":"machine-learning","link":"/categories/machine-learning/"},{"name":"Data Engineering","slug":"data-engineering","link":"/categories/data-engineering/"},{"name":"System","slug":"devsecops/system","link":"/categories/devsecops/system/"},{"name":"Security","slug":"devsecops/security","link":"/categories/devsecops/security/"},{"name":"Crash Course","slug":"machine-learning/crash-course","link":"/categories/machine-learning/crash-course/"},{"name":"Classification","slug":"machine-learning/classification","link":"/categories/machine-learning/classification/"},{"name":"Ensemble","slug":"machine-learning/ensemble","link":"/categories/machine-learning/ensemble/"},{"name":"Apache Nifi","slug":"data-engineering/apache-nifi","link":"/categories/data-engineering/apache-nifi/"}]}