<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title></title>
    <link>https://mchoi07.github.io/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Thu, 02 Apr 2020 02:33:50 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>boosting</title>
      <link>https://mchoi07.github.io/2020/04/01/boosting/</link>
      <guid>https://mchoi07.github.io/2020/04/01/boosting/</guid>
      <pubDate>Thu, 02 Apr 2020 02:47:52 GMT</pubDate>
      <description>
      
        &lt;p&gt;In &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;machine learning&lt;/a&gt;, &lt;strong&gt;boosting&lt;/strong&gt; is an &lt;a href=&quot;https://en.wikipedia.org/wiki/Ensemble_learning&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;ensemble&lt;/a&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Meta-algorithm&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;meta-algorithm&lt;/a&gt; for primarily reducing &lt;a href=&quot;https://en.wikipedia.org/wiki/Supervised_learning#Bias-variance_tradeoff&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;bias&lt;/a&gt;, and also variance in &lt;a href=&quot;https://en.wikipedia.org/wiki/Supervised_learning&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;supervised learning&lt;/a&gt;, and a family of machine learning algorithms that convert weak learners to strong ones. &lt;a href=&quot;https://en.wikipedia.org/wiki/Boosting_(machine_learning)&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>In <a href="https://en.wikipedia.org/wiki/Machine_learning" target="_blank" rel="external nofollow noopener noreferrer">machine learning</a>, <strong>boosting</strong> is an <a href="https://en.wikipedia.org/wiki/Ensemble_learning" target="_blank" rel="external nofollow noopener noreferrer">ensemble</a> <a href="https://en.wikipedia.org/wiki/Meta-algorithm" target="_blank" rel="external nofollow noopener noreferrer">meta-algorithm</a> for primarily reducing <a href="https://en.wikipedia.org/wiki/Supervised_learning#Bias-variance_tradeoff" target="_blank" rel="external nofollow noopener noreferrer">bias</a>, and also variance in <a href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" rel="external nofollow noopener noreferrer">supervised learning</a>, and a family of machine learning algorithms that convert weak learners to strong ones. <a href="https://en.wikipedia.org/wiki/Boosting_(machine_learning)" target="_blank" rel="external nofollow noopener noreferrer">Wikipedia</a></p><a id="more"></a><h1 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h1><p>Today, I would like to introduce <code>Boosting</code> method in Machine Learning. Basically, <code>Boosting</code> is a set of algorithms (classifiers) which changes weak learner to strong learners. It is one of the ensemble methods for improving the model predictions of any given learning algorithm. However, unlike the regular ensemble method which we group several model and use them in parallel, <strong>Boosting</strong> is using a single model in sequential order with differnt weights. </p><p><strong>Boosting</strong> is also using ramdom sampling with a replacement. It will start training model from Sample 1 to Sample N. In each training, there would be well-classified data and wrong-classified data. Since it allows a replacement, some wrong classified data might be included in different samples or not. </p><p>![Boosting Algorithm](/2020/04/01/boosting/Boosting Algorithm.png)</p><p>Since each previous model affect a current model by assigning weights to data which it coudln’t classify correctly, this is not parallel - it’s sequential. At the end, it will use all trained models with different weights and generate the output. One of the disadvantages of Boosting is that it easily get corrupted by outliers because it will assign heavy weights to those outliers and it will mislead the models.</p><h3 id="AdaBoost-Adaptive-Boosting"><a href="#AdaBoost-Adaptive-Boosting" class="headerlink" title="AdaBoost (Adaptive Boosting)"></a>AdaBoost (Adaptive Boosting)</h3><p>![Adaboost flow chart](/2020/04/01/boosting/Adaboost flow chart.png)</p><p><strong>AdaBoost</strong> works in a way putting more weights on difficult to classify data and less on those already handled well. </p><p>In AdaBoost, we use something called <em>Decision Stumps</em>, the simplest model we could construct on data. It split the data into two subsets based on the feature. To find the best decision stump, we should lay out all features of data along with every possible threshold and look for one gives us best accuracy. </p><p>In this example, I will consturct AdaBoost from the scratch with some mathematical explanation. </p><h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><p>X1 = (−1,0,+), X2 = (−0.5,0.5,+)<br>X3 = (0,1,−), X4 = (0.5,1,−)<br>X5 = (1,0,+), X6 = (1,−1,+)<br>X7 = (0,−1,−),X8 = (0,0,−)</p><p><img src="/2020/04/01/boosting/figure1.png" alt="figure1"></p><p><strong>How to?</strong></p><p>Constructing $D_t$. It’s basically weight of each $i^{th}$ data.</p><p>$D_{t+1}(i)$ = $\frac{D_t(i)}{Z_t} * e^{-\alpha_t}$ if $y_i = h_t(x_i)$<br>$D_{t+1}(i)$ = $\frac{D_t(i)}{Z_t} * e^{\alpha_t}$ if $y_i \ne h_t(x_i)$,</p><p>where $Z_t$ = Normalization Constant = Sum of $D_t$</p><p>and $\alpha_t$ = $\frac{1}{2}ln(\frac{1-\epsilon_t}{\epsilon_t})$.</p><p>$h_t: \epsilon_t = \sum\limits_{i=1}^{m}D_t\vert(y_i\ne h_t(x_i))$ if $(y_i\ne h_t(x_i)$ then 1 otherwise 0​</p><p><strong>Step 1</strong> </p><p>Put an initial random decision stump aka classifier $h_1$</p><p><img src="/2020/04/01/boosting/figure2.png" alt="figure2"></p><p>h1 is a randomly assinged decision stump.<br>Let’s get $D_1$ = $\frac{1}{m}$, where $m = 8$</p><table><thead><tr><th>t</th><th>err</th><th>alpha</th><th>Z</th><th>d1</th><th>d2</th><th>d3</th><th>d4</th><th>d5</th><th>d6</th><th>d7</th><th>d8</th></tr></thead><tbody><tr><td>1</td><td></td><td></td><td></td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td></tr></tbody></table><p>Now, in a second row, I will put 1 if $h_1$ classify $y_i$ incorrectly, else 0. </p><table><thead><tr><th>t</th><th>err</th><th>alpha</th><th>Z</th><th>d1</th><th>d2</th><th>d3</th><th>d4</th><th>d5</th><th>d6</th><th>d7</th><th>d8</th></tr></thead><tbody><tr><td>1</td><td></td><td></td><td></td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr></tbody></table><p>$h_1$ has classified correctly $d_t(1),d_t(3),d_t(4),d_t(5),d_t(6)$</p><p>Once we have $D_1(i)$ and a result of classification by $h_1$ we could calculate $\epsilon_1$, where $h_t: \epsilon_t = \sum\limits_{i=1}^{m}D_t\vert(y_i\ne h_t(x_i))$ if $(y_i\ne h_t(x_i)$ then 1 otherwise 0</p><p>In a thrid row, I will get all $D_1\vert(y_i\ne h_1(x_i)$, and if take sumation of all values, it’s our $\epsilon_1$.</p><table><thead><tr><th>t</th><th>err</th><th>alpha</th><th>Z</th><th>d1</th><th>d2</th><th>d3</th><th>d4</th><th>d5</th><th>d6</th><th>d7</th><th>d8</th></tr></thead><tbody><tr><td>1</td><td>0.375</td><td></td><td></td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0.13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.13</td><td>0.13</td></tr></tbody></table><p>Let’s get $\alpha_1$ and $Z_1$. Remember $Z_t$ = Normalization Constant = Sum of $D_t$, and $\alpha_t$ = $\frac{1}{2}ln(\frac{1-\epsilon_t}{\epsilon_t})$.</p><table><thead><tr><th>t</th><th>err</th><th>alpha</th><th>Z</th><th>d1</th><th>d2</th><th>d3</th><th>d4</th><th>d5</th><th>d6</th><th>d7</th><th>d8</th></tr></thead><tbody><tr><td>1</td><td>0.375</td><td>0.255</td><td>1.000</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0.13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.13</td><td>0.13</td></tr></tbody></table><p>If you see the table above, you could notice that d2, d7, d8 has more weight, 0.13 becase $h_1$ failed to classify them correctly. Based on this information, we could move onto second iteration. In order to calculate $D_2(i)$ we need additional information other than table above.</p><p>$D_{t+1}(i)$ = $\frac{D_t(i)}{Z_t} * e^{-\alpha_t}$ if $y_i = h_t(x_i)$<br>$D_{t+1}(i)$ = $\frac{D_t(i)}{Z_t} * e^{\alpha_t}$ if $y_i \ne h_t(x_i)$</p><p>We need to what what’s $e^{-\alpha_1}$ and $e^{\alpha_1}$</p><p>$e^{-\alpha_1}=0.775$</p><p>$e^{\alpha_1}=1.291$</p><table><thead><tr><th>t</th><th>err</th><th>alpha</th><th>Z</th><th>d1</th><th>d2</th><th>d3</th><th>d4</th><th>d5</th><th>d6</th><th>d7</th><th>d8</th></tr></thead><tbody><tr><td>1</td><td>0.375</td><td>0.255</td><td>1.000</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0.13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.13</td><td>0.13</td></tr><tr><td>2</td><td>0.323</td><td>0.371</td><td>0.968</td><td>0.097</td><td>0.161</td><td>0.097</td><td>0.097</td><td>0.097</td><td>0.097</td><td>0.161</td><td>0.161</td></tr></tbody></table><p>We’ve got new weights for data. d1 was 0.125; it has become 0.097. d2 has become 0.161 from 0.125 because $h_1$ failed to classfy this data. Let’s get the $h_2$ - I was focusing on having all <code>+</code>in the same group. </p><p><img src="/2020/04/01/boosting/figure3.png" alt="figure3"></p><p>If you do same calculation we’ve done above, You will get this table:</p><table><thead><tr><th>t</th><th>err</th><th>alpha</th><th>Z</th><th>d1</th><th>d2</th><th>d3</th><th>d4</th><th>d5</th><th>d6</th><th>d7</th><th>d8</th></tr></thead><tbody><tr><td>1</td><td>0.375</td><td>0.255</td><td>1.000</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0.13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.13</td><td>0.13</td></tr><tr><td>2</td><td>0.323</td><td>0.371</td><td>0.968</td><td>0.097</td><td>0.161</td><td>0.097</td><td>0.097</td><td>0.097</td><td>0.097</td><td>0.161</td><td>0.161</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.16</td><td>0.16</td></tr></tbody></table><p>$e^{-\alpha_2}=0.490$</p><p>$e^{\alpha_2}=2.041$</p><p>Lastly, let’s do the final iteration.</p><table><thead><tr><th>t</th><th>err</th><th>alpha</th><th>Z</th><th>d1</th><th>d2</th><th>d3</th><th>d4</th><th>d5</th><th>d6</th><th>d7</th><th>d8</th></tr></thead><tbody><tr><td>1</td><td>0.375</td><td>0.255</td><td>1.000</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0.13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.13</td><td>0.13</td></tr><tr><td>2</td><td>0.194</td><td>0.713</td><td>0.968</td><td>0.097</td><td>0.161</td><td>0.097</td><td>0.097</td><td>0.097</td><td>0.097</td><td>0.161</td><td>0.161</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0</td><td>0.1</td><td>0.1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>3</td><td></td><td></td><td></td><td>0.049</td><td>0.082</td><td>0.204</td><td>0.204</td><td>0.049</td><td>0.049</td><td>0.082</td><td>0.082</td></tr></tbody></table><p><img src="/2020/04/01/boosting/figure4.png" alt="figure4"></p><p>A reasoning of $h_3$ is to have all <code>o</code> in a same group. Also, d5 and d6 have never been misclassified, so I want to have a information of d5 and d6 in my model. And the result will be:</p><table><thead><tr><th>t</th><th>err</th><th>alpha</th><th>Z</th><th>d1</th><th>d2</th><th>d3</th><th>d4</th><th>d5</th><th>d6</th><th>d7</th><th>d8</th></tr></thead><tbody><tr><td>1</td><td>0.375</td><td>0.255</td><td>1.000</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0.13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.13</td><td>0.13</td></tr><tr><td>2</td><td>0.323</td><td>0.371</td><td>0.968</td><td>0.097</td><td>0.161</td><td>0.097</td><td>0.097</td><td>0.097</td><td>0.097</td><td>0.161</td><td>0.161</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.16</td><td>0.16</td></tr><tr><td>3</td><td>0.138</td><td>0.916</td><td>0.943</td><td>0.069</td><td>0.115</td><td>0.069</td><td>0.069</td><td>0.069</td><td>0.069</td><td>0.241</td><td>0.241</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.07</td><td>0.07</td><td>0</td><td>0</td></tr></tbody></table><p>Graphically, it will look like:</p><p><img src="/2020/04/01/boosting/figure5.png" alt="figure5"></p><p>Lastly, the final classifier is:</p><p>$H_{final}(x)=sign(0.255<em>h_1+0.371</em>h_2+0.943*h_3)$</p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2020/04/01/boosting/#disqus_thread</comments>
    </item>
    
    <item>
      <title>(ENG) Naive Bayes from scratch</title>
      <link>https://mchoi07.github.io/2020/03/23/Naive%20Bayes%20from%20scratch/</link>
      <guid>https://mchoi07.github.io/2020/03/23/Naive%20Bayes%20from%20scratch/</guid>
      <pubDate>Mon, 23 Mar 2020 21:54:04 GMT</pubDate>
      <description>
      
        &lt;p&gt;Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predcitors) in a learning problem. &lt;code&gt;Maxumum-likelihood&lt;/code&gt; training can be done by evaluting a closed-form exporession, which takes linear time, rather tahn by expensive iterative approximation as used for many other typs of classifier.  &lt;a href=&quot;https://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predcitors) in a learning problem. <code>Maxumum-likelihood</code> training can be done by evaluting a closed-form exporession, which takes linear time, rather tahn by expensive iterative approximation as used for many other typs of classifier.  <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" target="_blank" rel="external nofollow noopener noreferrer">Wikipedia</a></p><a id="more"></a><h2 id="Usecase-Spam-filter"><a href="#Usecase-Spam-filter" class="headerlink" title="Usecase - Spam filter"></a>Usecase - Spam filter</h2><p>We will use the <code>Naive Bayes</code> algorithm to fit a spam filter. </p><p>Spam filters are used in alal email services to classify received emails as “Spam” or “Not Spam”. A simple apporach involves maintaining a vocabulary of words that commonly occur in “Spam” emails and classifying an email as “Spam” if the number of words from the dictionary that are present in the email is over a certain threshold. </p><p>Assume we are given the vocabulary consists of 15 words</p><p>$V$ = {secret, offer, low, price, valued, customer, today, dollar, million, sports, is, for, play, healthy, pizza} </p><p>We will use $V_i$ to represent the ith word in $V$. As our training dataset, we are also given 3 example spam messages: <br><br>• million dollar offer <br><br>• secret offer today <br><br>• secret is secret <br></p><p>and 4 example non-spam messages<br><br>• low price for valued customer <br><br>• play secret sports today<br><br>• sports is healthy<br><br>• low price pizza<br></p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"></span><br><span class="line">V = [<span class="hljs-string">'secret'</span>, <span class="hljs-string">'offer'</span>, <span class="hljs-string">'low'</span>, <span class="hljs-string">'price'</span>, <span class="hljs-string">'valued'</span>, <span class="hljs-string">'customer'</span>, <span class="hljs-string">'today'</span>, <span class="hljs-string">'dollar'</span>, <span class="hljs-string">'million'</span>, <span class="hljs-string">'sports'</span>, <span class="hljs-string">'is'</span>, <span class="hljs-string">'for'</span>, <span class="hljs-string">'play'</span>, <span class="hljs-string">'healthy'</span>, <span class="hljs-string">'pizza'</span>]</span><br><span class="line"></span><br><span class="line">message = [<span class="hljs-string">'million dollar offer'</span>,<span class="hljs-string">'secret offer today'</span>,<span class="hljs-string">'secret is secret'</span>,</span><br><span class="line">          <span class="hljs-string">'low price for valued customer'</span>, <span class="hljs-string">'play secret sports today'</span>, </span><br><span class="line">          <span class="hljs-string">'sports is healthy'</span>, <span class="hljs-string">'low price pizza'</span>]</span><br></pre></td></tr></table></figure><p><code>train</code> is our input vector x corresponding to each training message, and it has length n = 15 (length of V). Since we have 7 training example of message, We will have 7 by 15 training data - 7 data 15 features. </p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train = np.zeros((<span class="hljs-number">7</span>, <span class="hljs-number">15</span>))</span><br><span class="line">label = [<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">Converting 7 training message data set to x vector which has length n = 15</span></span><br><span class="line"><span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(train)):</span><br><span class="line">    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(len(V)):</span><br><span class="line">        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(len(message[i].split(<span class="hljs-string">" "</span>))):</span><br><span class="line">            <span class="hljs-keyword">if</span> V[j] == message[i].split(<span class="hljs-string">" "</span>)[k]:</span><br><span class="line">                train[i,j] += <span class="hljs-number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">This is the feture x matrix.</span></span><br><span class="line"><span class="hljs-string">row = n message</span></span><br><span class="line"><span class="hljs-string">col = i_th feature vector </span></span><br><span class="line"><span class="hljs-string">'''</span></span><br><span class="line">train</span><br></pre></td></tr></table></figure><pre><code>array([[0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],       [1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],       [2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],       [0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],       [1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.],       [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])</code></pre><p>Let’s list them separately</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(label)):</span><br><span class="line">    <span class="hljs-keyword">if</span> label[i] == <span class="hljs-number">0</span>:</span><br><span class="line">        print(<span class="hljs-string">"this is &#123;&#125;th message - It's SCAM"</span>.format(i))</span><br><span class="line">        print(train[i])</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        print(<span class="hljs-string">"this is &#123;&#125;th message - It's NOT SCAM"</span>.format(i))</span><br><span class="line">        print(train[i])</span><br></pre></td></tr></table></figure><pre><code>this is 0th message - It&apos;s SCAM[0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]this is 1th message - It&apos;s SCAM[1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]this is 2th message - It&apos;s SCAM[2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]this is 3th message - It&apos;s NOT SCAM[0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]this is 4th message - It&apos;s NOT SCAM[1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0.]this is 5th message - It&apos;s NOT SCAM[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.]this is 6th message - It&apos;s NOT SCAM[0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">spam and ham array will be count of x_i when each message is spam or not spam.</span></span><br><span class="line"><span class="hljs-string">spam = (Y=0|x_i)</span></span><br><span class="line"><span class="hljs-string">ham = (Y=1|x_i)</span></span><br><span class="line"><span class="hljs-string">'''</span></span><br><span class="line">spam = np.zeros((<span class="hljs-number">1</span>, <span class="hljs-number">15</span>))</span><br><span class="line">ham = np.zeros((<span class="hljs-number">1</span>, <span class="hljs-number">15</span>))</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> range(len(label)):</span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(train.shape[<span class="hljs-number">1</span>]):</span><br><span class="line">        <span class="hljs-keyword">if</span> label[c] == <span class="hljs-number">0</span>: <span class="hljs-comment"># Spam</span></span><br><span class="line">            spam[<span class="hljs-number">0</span>,i] += train[c,i]</span><br><span class="line">            </span><br><span class="line">        <span class="hljs-keyword">else</span>: <span class="hljs-comment"># Not Spam</span></span><br><span class="line">            ham[<span class="hljs-number">0</span>,i] += train[c,i]</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spam</span><br></pre></td></tr></table></figure><pre><code>array([[3., 2., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0.]])</code></pre><p>If you see a result above, <code>spam[0] = 3</code> ,<code>spam[1] = 2</code> , <code>spam [2] = 0</code>, it means total count of <code>x0 = secret</code> is three among training data classified as <em>spam</em>. We have two <code>offer</code> and zeor <code>low</code> in our training data.  </p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">In this section, we will calculate probabiliy of each word xi when a message is spam or not.</span></span><br><span class="line"><span class="hljs-string">prob_spam = P(x_i|y=0)</span></span><br><span class="line"><span class="hljs-string">prob_ham = P(x_i|y=1)</span></span><br><span class="line"><span class="hljs-string">'''</span></span><br><span class="line">spam_counter = <span class="hljs-number">0</span></span><br><span class="line">ham_counter = <span class="hljs-number">0</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> range(len(label)):</span><br><span class="line">    <span class="hljs-keyword">if</span> label[c] == <span class="hljs-number">0</span>:</span><br><span class="line">        spam_counter += <span class="hljs-number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        ham_counter += <span class="hljs-number">1</span></span><br><span class="line">        </span><br><span class="line">prob_spam_x_i = spam/spam_counter</span><br><span class="line">prob_ham_x_i = ham/ham_counter</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prob_spam_x_i</span><br></pre></td></tr></table></figure><pre><code>array([[1.        , 0.66666667, 0.        , 0.        , 0.        ,        0.        , 0.33333333, 0.33333333, 0.33333333, 0.        ,        0.33333333, 0.        , 0.        , 0.        , 0.        ]])</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prob_ham_x_i</span><br></pre></td></tr></table></figure><pre><code>array([[0.25, 0.  , 0.5 , 0.5 , 0.25, 0.25, 0.25, 0.  , 0.  , 0.5 , 0.25,        0.25, 0.25, 0.25, 0.25]])</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">Bayes Therorm</span></span><br><span class="line"><span class="hljs-string">Calculating P(y=0|x_i)</span></span><br><span class="line"><span class="hljs-string">prob_spam_per_word = P(y=0|x_i)</span></span><br><span class="line"><span class="hljs-string">'''</span></span><br><span class="line">prob_spam_per_word = np.zeros((<span class="hljs-number">1</span>, <span class="hljs-number">15</span>))</span><br><span class="line">prob_ham_per_word = np.zeros((<span class="hljs-number">1</span>, <span class="hljs-number">15</span>))</span><br><span class="line">prob_spam = <span class="hljs-number">3</span>/<span class="hljs-number">7</span></span><br><span class="line">prob_ham = <span class="hljs-number">4</span>/<span class="hljs-number">7</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(prob_spam_per_word.shape[<span class="hljs-number">1</span>]):</span><br><span class="line">    spam_a = (prob_spam_x_i[<span class="hljs-number">0</span>,i]*prob_spam) </span><br><span class="line">    spam_b = spam_a + (prob_ham_x_i[<span class="hljs-number">0</span>,i]*prob_ham)</span><br><span class="line">    spam_c = spam_a/spam_b</span><br><span class="line">    </span><br><span class="line">    ham_a = (prob_ham_x_i[<span class="hljs-number">0</span>,i]*prob_ham) </span><br><span class="line">    ham_b = ham_a + (prob_spam_x_i[<span class="hljs-number">0</span>,i]*prob_spam)</span><br><span class="line">    ham_c = ham_a/ham_b</span><br><span class="line">    </span><br><span class="line">    prob_spam_per_word[<span class="hljs-number">0</span>,i] = spam_c</span><br><span class="line">    prob_ham_per_word[<span class="hljs-number">0</span>,i] = ham_c</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prob_spam_per_word</span><br></pre></td></tr></table></figure><pre><code>array([[0.75, 1.  , 0.  , 0.  , 0.  , 0.  , 0.5 , 1.  , 1.  , 0.  , 0.5 ,        0.  , 0.  , 0.  , 0.  ]])</code></pre><p>The result above means that the probability of spam per i_th words. For example, the probability of spam when we have <code>secret</code> is 75% and a probability of spam of <code>offer</code> is 100%.</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prob_ham_per_word <span class="hljs-comment"># This is the probability of ham per words.</span></span><br></pre></td></tr></table></figure><pre><code>array([[0.25, 0.  , 1.  , 1.  , 1.  , 1.  , 0.5 , 0.  , 0.  , 1.  , 0.5 ,        1.  , 1.  , 1.  , 1.  ]])</code></pre><p><strong>Since we don’t have enough data, I will just use training dataset to evaluate the bayes classifier</strong></p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">threshold = <span class="hljs-number">0.50</span> <span class="hljs-comment"># In what percentage do you want to classfy an eamil as spam.</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(train.shape[<span class="hljs-number">0</span>]):</span><br><span class="line">    conditional_prob_spam = prob_spam</span><br><span class="line">    conditional_prob_ham = prob_ham</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(prob_spam_per_word.shape[<span class="hljs-number">1</span>]):</span><br><span class="line">        <span class="hljs-keyword">if</span> train[t][i] == <span class="hljs-number">1</span>:</span><br><span class="line">            conditional_prob_spam = conditional_prob_spam * prob_spam_per_word[<span class="hljs-number">0</span>,i]</span><br><span class="line">            conditional_prob_ham = conditional_prob_ham * prob_ham_per_word[<span class="hljs-number">0</span>,i]</span><br><span class="line">            </span><br><span class="line">    <span class="hljs-keyword">if</span> conditional_prob_spam != <span class="hljs-number">0</span>:</span><br><span class="line">        prob = conditional_prob_spam / (conditional_prob_spam + conditional_prob_ham) * <span class="hljs-number">100</span></span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        prob = <span class="hljs-number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> prob &gt; threshold*<span class="hljs-number">100</span>:</span><br><span class="line">        label = <span class="hljs-string">"SPAM"</span>  </span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        label = <span class="hljs-string">"NOT SPAM"</span></span><br><span class="line">        </span><br><span class="line">    print(<span class="hljs-string">"&#123;&#125; th email is &#123;&#125; with a probability of being spam &#123;&#125;%"</span>.format(t,label,prob))</span><br><span class="line">    <span class="hljs-comment">#print(prob)</span></span><br></pre></td></tr></table></figure><pre><code>0 th email is SPAM with a probability of being spam 100.0%1 th email is SPAM with a probability of being spam 100.0%2 th email is NOT SPAM with a probability of being spam 42.857142857142854%3 th email is NOT SPAM with a probability of being spam 0.0%4 th email is NOT SPAM with a probability of being spam 0.0%5 th email is NOT SPAM with a probability of being spam 0.0%6 th email is NOT SPAM with a probability of being spam 0.0%</code></pre><h4 id=""><a href="#" class="headerlink" title=""></a></h4><p>Given a new message <code>“today is secret”</code>, decide whether it is spam or not spam, based on the Naive Bayes classifier, learned from the above data.</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">"today is secret"</span></span><br><span class="line"><span class="hljs-string">Vectorizing the email.. </span></span><br><span class="line"><span class="hljs-string">'''</span></span><br><span class="line">target = np.array([<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>])</span><br><span class="line">target[<span class="hljs-number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">Please play around with different threshold. </span></span><br><span class="line"><span class="hljs-string">'''</span></span><br><span class="line">threshold = <span class="hljs-number">0.50</span> <span class="hljs-comment"># In what percentage do you want to classfy an eamil as spam.</span></span><br><span class="line"></span><br><span class="line">conditional_prob_spam = prob_spam</span><br><span class="line">conditional_prob_ham = prob_ham</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(prob_spam_per_word.shape[<span class="hljs-number">1</span>]):</span><br><span class="line">    <span class="hljs-keyword">if</span> target[i] == <span class="hljs-number">1</span>:</span><br><span class="line">        conditional_prob_spam = conditional_prob_spam * prob_spam_per_word[<span class="hljs-number">0</span>,i]</span><br><span class="line">        conditional_prob_ham = conditional_prob_ham * prob_ham_per_word[<span class="hljs-number">0</span>,i]</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">if</span> conditional_prob_spam != <span class="hljs-number">0</span>:</span><br><span class="line">    prob = conditional_prob_spam / (conditional_prob_spam + conditional_prob_ham) * <span class="hljs-number">100</span></span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">    prob = <span class="hljs-number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">if</span> prob &gt; threshold*<span class="hljs-number">100</span>:</span><br><span class="line">    label = <span class="hljs-string">"SPAM"</span>  </span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">    label = <span class="hljs-string">"NOT SPAM"</span></span><br><span class="line"></span><br><span class="line">print(<span class="hljs-string">"&#123;&#125; th email is &#123;&#125; with a probability of being spam &#123;&#125;%"</span>.format(t,label,prob))</span><br><span class="line"><span class="hljs-comment">#print(prob)</span></span><br></pre></td></tr></table></figure><pre><code>6 th email is SPAM with a probability of being spam 69.23076923076923%</code></pre>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2020/03/23/Naive%20Bayes%20from%20scratch/#disqus_thread</comments>
    </item>
    
    <item>
      <title>(ENG) Feature-Engineering-101</title>
      <link>https://mchoi07.github.io/2020/01/23/feature-engineering-101/</link>
      <guid>https://mchoi07.github.io/2020/01/23/feature-engineering-101/</guid>
      <pubDate>Fri, 24 Jan 2020 03:20:01 GMT</pubDate>
      <description>
      
        &lt;p&gt;If you ask yourself what’s the most important thing in machine learning, what’s your answer? All data scientist would have different answers. &lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>If you ask yourself what’s the most important thing in machine learning, what’s your answer? All data scientist would have different answers. </p><a id="more"></a><p>Among the other many answers, I believe feature engineering is one of the most important in machine learning. Sometimes, it’s a more critical step than a model selection and training a model because a model cannot improve a model itself even though we put a lot of effort on a hyper parameter turning. However, well selected/extracted features could be applied to many different models and improve a performance. </p><p>A feature engineering includes feature selection and feature extraction. A feature selection is trial-error process to select relevant features from existing features. Since all features are simply selected from original features it’s easy to interpret what those features means. However, it is difficult to consider a relationship in selected features. </p><p>On the other hands, a feature extraction is more like functional process to extract relevant features from existing features. It requires a form of function which enable an algorithm to create/extract a new set of features. A relationship between features will be considered and number of features could be significantly reduced. Yet, an interpretation of extracted features is not easy.</p><p>We should use different methods of feature engineering depending on a machine learning algorithm we want to use. </p><p>In supervised learning, we could select features form <code>Information gain</code>, <code>Stepwise regression</code>, <code>LASSO</code>, <code>Genetic algorithms</code>, etc. If we want to extract features, <code>Partial Least Squares (PLS)</code> is an option. </p><p>In unsupervised learning, we could do a feature selection with <code>PCA loading</code>; a feature extraction uses <code>Principal component analysis (PCA)</code>, <code>Wavelets transforms</code>, <code>Autoencoder</code>, etc. </p><h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><p><a href="https://pngimage.net/features-png-7/" target="_blank" rel="external nofollow noopener noreferrer">Thumnail</a></p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2020/01/23/feature-engineering-101/#disqus_thread</comments>
    </item>
    
    <item>
      <title>(KOR) 머신러닝 속성코스 02 - 기본용어</title>
      <link>https://mchoi07.github.io/2019/12/16/mlcrash02-keyterm/</link>
      <guid>https://mchoi07.github.io/2019/12/16/mlcrash02-keyterm/</guid>
      <pubDate>Tue, 17 Dec 2019 01:52:35 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;supervised&lt;/code&gt;  &lt;code&gt;unsupervised&lt;/code&gt; &lt;code&gt;feature&lt;/code&gt; &lt;code&gt;label&lt;/code&gt; … 머신러닝을 시작하게 되면 새로 배워야 하는 용어들이 많죠? 하지만 이러한 용어들을 자신의 개념으로 잘 정리하는 것이 참 중요합니다. 왜냐하면, 우리가 앞으로 배우게 될 머신러닝의 기초가 되기 때문이죠…&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p><code>supervised</code>  <code>unsupervised</code> <code>feature</code> <code>label</code> … 머신러닝을 시작하게 되면 새로 배워야 하는 용어들이 많죠? 하지만 이러한 용어들을 자신의 개념으로 잘 정리하는 것이 참 중요합니다. 왜냐하면, 우리가 앞으로 배우게 될 머신러닝의 기초가 되기 때문이죠…</p><a id="more"></a><hr><p>안녕하세요 AI Nomad 최민규입니다. 머신러닝 속성코스 두 번째 세션에서는 머신러닝에서 사용되는 기본적인 용어들을 정리해보려 합니다. </p><h4 id="1-Supervised-and-Unsupervised-Learning"><a href="#1-Supervised-and-Unsupervised-Learning" class="headerlink" title="1. Supervised and Unsupervised Learning"></a>1. Supervised and Unsupervised Learning</h4><p>머신러닝을 처음 시작하게 되면 가장 먼저 알게 되는 용어는 <code>Supervised Learning</code>  과 <code>Unsupervised Learning</code> 입니다.  </p><p>영어로 supervised라고 하면 감독이 돼다, 관리되다 정도로 해석됩니다. 따라서 supervised learning 을 직역하면 감독의 지시 아래 배워지다 정도로 해석됩니다. 우리가 무언가를 배울 때 좋은 감독이 있으면 배움이 편합니다. 왜냐하면, 그들이 우리가 무엇을 배워야 할지 감독해주고 지시해주기 때문입니다. </p><p>머신러닝에서 supervised learning 도 비슷한 의미가 있다고 할 수 있습니다. 컴퓨터가 사람의 감독 아래 지시되고 학습되게 되는 거죠. 감독들은 선수들을 지도하고 지시합니다. A와 B를하라고 왜냐하면 감독들은 A와 B를 했을때 C라는 결과가 나올 거를 경험을 통해 알기 때문이죠. 머신러닝에서 A 와 B 를 feature 이라고 부릅니다. 그리고 C 를 label 이라고 부르죠.  supervised learning 을 학습시키기 위해서는 feature  와 label 이 필요합니다. 감독이 있기 때문이죠.  </p><p>그렇다면 감독이 존재하지 않는 unsupervised learning 은 어떨까요? 감독은 없고 선수들만 있는 팀이 있다고 가정합시다. 선수들은 매주 경기에 나가야 하죠. 구단주는 선수들에게 리그 3위 안에 들지 않으면 팀을 해체하겠다고 합니다. 감독이 없다는 것은 label 이 없다는 것이죠. 선수들은 직관적으로 어떠한 훈련을 해야 할지는 알 수 있을 것입니다. 하지만 감독의 경험이 없는 것이죠. 그러므로 unsupervised learning 에는 feature 만이 존재합니다. 이 상황에서 선수들은 어떻게 감독 없이 팀을 빌딩 할 수 있을까요? 스스로 답을 찾아야죠. 훈련과 경기를 통한 <code>trial and error</code> 즉 시행착오를 거쳐서 선수들 스스로 배워가는 수밖에 없겠죠. 이것이 머신러닝에서 unsupervised learning 이라고 불리는 것입니다. 데이터에는 feature 들만 존재하고 label 이 없습니다. 따라서 여러 알고리즘들이 feature 을 가지고 시행착오를 거쳐 가장 이상적인 label 을 찾아가는 것을 우리는 unsupervised learning 이라고 합니다</p><h4 id="2-Labels-결과값"><a href="#2-Labels-결과값" class="headerlink" title="2. Labels (결과값)"></a>2. Labels (결과값)</h4><p>앞에서 나온 <code>label</code>은 어떠한 x들에 대한 y 즉 결과값이라고 정리 할 수 있겠습니다. 예를 들면, 미래의 주식가격, 사진 속에 등장한 동물의 종류, 어떤 소리에 의미 등이 있겠군요. </p><h4 id="3-Feature-원인"><a href="#3-Feature-원인" class="headerlink" title="3. Feature (원인)"></a>3. Feature (원인)</h4><p><code>Feature</code>은 간단하게 어떤 결과에 대한 원인입니다. y라는 결과에 대한 x라는 원인이죠. 대학교 성적을 결과라고 한다면 이 결과에 대한 feature 즉 원인으로는 하루 공부 시간, 연애 여부, 아르바이트 여부, 등등이 있겠네요. </p><h4 id="4-Model-모델"><a href="#4-Model-모델" class="headerlink" title="4. Model (모델)"></a>4. Model (모델)</h4><p><code>모델</code>은 feature와 feature 사이 또 feature와 label의 관계라고 보시면 됩니다. 모델은 A와 B라는 feature들의 관계를 설명하고 A,B 라는 feature들과 C라는 label의 관계를 설명합니다. 이들의 관계를 잘 설명하는 모델을 우리는 좋은 모델이라고 부르며 좋은 결과를 내게 됩니다. </p><h4 id="5-Train-학습"><a href="#5-Train-학습" class="headerlink" title="5. Train (학습)"></a>5. Train (학습)</h4><p>우리가 모델을 이야기할 때 <code>train</code> 시킨다 <code>학습</code> 시킨다는 용어를 많이 사용합니다. 모델을 학습시킨다 즉 train 한다는 말은 쉽게 말하면 모델을 만든다 모델을 학습시킨다고 이해하시면 되겠습니다. 그렇다면 모델을 만들고 학습시킨다는데 어떻게 무엇으로 학습시킨다는 거죠? 바로 데이터입니다. 데이터에 존재하는 많으면 많은 적으면 적은 feature들과 lable을 통해서 모델을 학습시키는 것입니다. </p><p>우리가 강아지에게 손을 달라고 했을 때 손을 주는 훈련을 한다고 가정을 해봅시다. 강아지가 이 훈련을 습득하는 방법은 보통 다음과 같겠죠</p><p>손 -&gt; 간식 (o)<br>발 -&gt; 간식 (x)<br>누움 -&gt; 간식 (x)<br>손 -&gt; 간식 (o)<br>손 -&gt; 간식 (o)<br>손 -&gt; 간식 (o)</p><p>강아지는 손을 줬을 때 간식을 먹었다는 데이터를 기반으로 학습했고 사람이 손이라고 이야기했을 때 손을 주는 행동을 하게 되죠. 머신러닝도 마찮가지로 데이터를 기반으로 어떤 모델을 학습하고, 우리가 모델에 input을 주었을 때 모델은 학습된 데이터를 바탕으로 어떤 output 즉 <code>inference</code> 를 만들어 내게 됩니다. </p><h4 id="6-Inference-암시"><a href="#6-Inference-암시" class="headerlink" title="6. Inference (암시)"></a>6. Inference (암시)</h4><p><code>Inference</code>는 unlabeled example을 학습된 모델에 적용할 때 사용합니다. 즉 모델을 학습시키고 새로운 input을 모델에 넣었을 때 모델이 만드는 output을 inference라고 부르죠. 결국 이 output도 예측값이기 때문에 결과를 암시한다 라는 의미로 inference라고 부릅니다. </p><h4 id="7-요약"><a href="#7-요약" class="headerlink" title="7. 요약"></a>7. 요약</h4><p>데이터를 빼놓고는 머신러닝을 이야기 할 수 없습니다. 데이터가 quantitative or qualitative 한지를 떠나서 머신러닝에서 데이터를 보는 관점은 딱 두 가지 입니다. <code>feature</code> 와 <code>label</code>. 이러한 feature와 label의 관계를 설명하는 것이 모델이고, 모델을 만들기 위해서는 데이터를 가지고 모델을 <code>train</code> (학습) 시키는 과정이 필요합니다. 모델을 통해 생성된 결과값 즉 prediction이 바로 <code>inference</code> 가 되게 되는 것이죠. </p><p>이렇게 이번 세션에서는 기본적인 머신러닝 용어들에 대해 배워봤습니다. 다음 세션은 본격적으로 모델에 사용되는 알고리즘과 알고리즘이 학습되는 로직에 대한 세션을 준비하겠습니다. </p><p><strong><a href="https://mchoi07.github.io/2019/12/13/mlcrash01-overview/">&gt;&gt; Previous: Overview</a></strong></p><h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><p><a href="https://www.kdnuggets.com/2016/05/machine-learning-key-terms-explained.html" target="_blank" rel="external nofollow noopener noreferrer">Thumnail</a><br><a href="https://developers.google.com/machine-learning/crash-course" target="_blank" rel="external nofollow noopener noreferrer">Google Machine Learning Crash Course</a></p><hr><p><em>Except as otherwise noted, the content of this page is licensed under the <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="external nofollow noopener noreferrer">Creative Commons Attribution 4.0 License</a>, and code samples are licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0" target="_blank" rel="external nofollow noopener noreferrer">Apache 2.0 License</a>. For details, see the <a href="https://developers.google.com/site-policies" target="_blank" rel="external nofollow noopener noreferrer">Google Developers Site Policies</a>. Java is a registered trademark of Oracle and/or its affiliates.</em></p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2019/12/16/mlcrash02-keyterm/#disqus_thread</comments>
    </item>
    
    <item>
      <title>(KOR) 머신러닝 속성코스 01 - Overview</title>
      <link>https://mchoi07.github.io/2019/12/13/mlcrash01-overview/</link>
      <guid>https://mchoi07.github.io/2019/12/13/mlcrash01-overview/</guid>
      <pubDate>Sat, 14 Dec 2019 01:15:30 GMT</pubDate>
      <description>
      
        &lt;p&gt;구글의 Tensorflow로 배워보는 머신러닝 속성학습 지금부터 시작합니다. &lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>구글의 Tensorflow로 배워보는 머신러닝 속성학습 지금부터 시작합니다. </p><a id="more"></a><p>안녕하세요 AI Nomad 최민규입니다. </p><p>앞으로 진행하게 될 AI에 관련된 다양한 프로젝트들과 콘텐츠에 앞서 어떤 프로젝트로 시작하면 좋을까 생각하다가 구글에서 있는 Machine Learning Crash 코스를 go over 하면서 기본적인 머신러닝에 대한 개념을 정리하면 어떨까 생각해 봤습니다.</p><p>구글에서는 이 코스를 이미 한국어로 번역해 두었지만. 번역이 조금 서툴더군요. 그래서 제가 구글의 자료를 쭉 정리하며 조금 더 자세한 설명을 붙여가면서 이 컨탠츠를 진행해 보도록 하겠습니다. </p><h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><p><a href="https://developers.google.com/machine-learning/crash-course" target="_blank" rel="external nofollow noopener noreferrer">Thumnail</a><br><a href="https://developers.google.com/machine-learning/crash-course" target="_blank" rel="external nofollow noopener noreferrer">Google Machine Learning Crash Course</a></p><hr><p><em>Except as otherwise noted, the content of this page is licensed under the <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="external nofollow noopener noreferrer">Creative Commons Attribution 4.0 License</a>, and code samples are licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0" target="_blank" rel="external nofollow noopener noreferrer">Apache 2.0 License</a>. For details, see the <a href="https://developers.google.com/site-policies" target="_blank" rel="external nofollow noopener noreferrer">Google Developers Site Policies</a>. Java is a registered trademark of Oracle and/or its affiliates.</em></p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2019/12/13/mlcrash01-overview/#disqus_thread</comments>
    </item>
    
    <item>
      <title>(KOR) 암호화복호화</title>
      <link>https://mchoi07.github.io/2019/12/07/%EC%95%94%ED%98%B8%ED%99%94%EB%B3%B5%ED%98%B8%ED%99%94/</link>
      <guid>https://mchoi07.github.io/2019/12/07/%EC%95%94%ED%98%B8%ED%99%94%EB%B3%B5%ED%98%B8%ED%99%94/</guid>
      <pubDate>Sat, 07 Dec 2019 21:22:36 GMT</pubDate>
      <description>
      
        &lt;p&gt;여러분들의 데이터는 안전합니까? &lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>여러분들의 데이터는 안전합니까? </p><a id="more"></a><p>오늘은 암호화 (Encryption) 복호화(Decryption)에 대한 이야기를 나눠보고자 합니다. </p><h2 id="암호와-복호와"><a href="#암호와-복호와" class="headerlink" title="암호와/복호와"></a>암호와/복호와</h2><p>암호화는 데이터를 암호화 하여서 누군가가 읽을 수 없도록 정보를 전달화는 과정입니다. 암호와에는 여러가지 알고리즘이 쓰입니다. </p><p>복호와는 암호화된 정보를 다시 읽을 수 있게하는 과정으로써 데이터가 누출되더라도 복호화를 하지못하면 암호화된 데이터를 읽을 수 없습니다. </p><h2 id="암호와-종류"><a href="#암호와-종류" class="headerlink" title="암호와 종류"></a>암호와 종류</h2><ol><li><p>단방향 암호: 암호화 후 복호화 할 수 없습니다. 예를 들면 <code>사용자 비밀번호</code> 사용자가 입력한 비밀번호를 암호화 하고 모든 접근자는 암호화 된 코드를 다시 평문으로 볼 수 없습니다. 해킹이 되어도 복호화가 굉장히 어렵습니다. 예외적인 경우로는 <code>RainbowTable</code> 이 있습니다. </p><p>더 자세한 정보: <a href="https://www.youtube.com/watch?v=TeIVhioUAXs" target="_blank" rel="external nofollow noopener noreferrer">참고영상</a></p></li><li><p>양방향 암호: 암호와와 복호화 모두 가능합니다. <code>사용자 주소, 이메일, 전자서명</code> 등과 같이 정보를 재사용해야 되는 경우에 사용합니다. </p><p>양방향 암호에는 크게 두 가지 종류가 있습니다. </p><ul><li><p>대칭형 암호 (비밀키 암호)</p><p>대칭형 암호는 암호화 할 때 사용하는 키와 복호화 할 때 사용하는 키가 동일한 암호화 기법입니다. 예를 들면 “APPLE”를 “ABCDE”로 암호화 했다면 복호화도 반드시 “ABCDE”로 해야됩니다. 예를 들면 <code>AES Algorithm</code></p><p>하지만 대칭형 암호에는 키 배송에 관한 문제가 발생됩니다. 송신 측에서는 데이터를 암호화한 후에 수신 측에 암호키를 전달해야되고 전달하는 과정에서 이 함호 키가 털리면 데이터가 유출됩니다. 그리고 키 관리가 어렵습니다.</p></li></ul></li></ol><ul><li><p>비대칭형 암호 (공개키 암호)</p><p>비대칭현 암호는 암호와 키와 복호화 키가 다릅니다. </p><p>클라이언트와 서버가 각각의 공개키와 비밀키를 갖고, 서로 공개키를 공개합니다. 클라이언트는 서버의 공개키로 데이터를 암호화한 후에 서버로 보내면 서버는 자신의 비밀키를 가지고 클라이언트가 보낸 데이터를 복호화 합니다. 예를 들면 <code>RSA, Diffe-Hellman, ECC, etc</code></p><p><code>공개키</code> 는 공유되지만 <code>암호키</code> 는 공개되지 않기에 공개키가 중간에 탈취되어도 데이터를 안전하게 지킬 수 있습니다. </p><p>하지만 문제는 비대칭형 암호는 대칭형 암호에 비해 느리고 많은 자료를 암호와 복호화 하는데 불편합니다 단점이 있습니다. </p></li></ul><h2 id="암호-알고리즘"><a href="#암호-알고리즘" class="headerlink" title="암호 알고리즘"></a>암호 알고리즘</h2><h3 id="단방향"><a href="#단방향" class="headerlink" title="단방향"></a>단방향</h3><ol><li><code>SHA</code> : 가장 대표적인 해시함수</li><li><code>PBKDF2</code> : 해시함수의 컨테이너인 PBKDF2는 솔트를 적용한 후 해시 함수의 반복 횟수를 임의로 선택할 수 있다. PBKDF2는 구현하기 쉬운 알고리즘이며 SHA와 같이 검증된 해시 함수만 사용합니다.</li><li><code>bcrypt</code> : 패스워드 저장을 목적으로 설계되었으며 가장 많이 쓰이는 알고리즘입니다. 입력값을 72 byte로 해야하기 때문에 조금 사용에 불편함이 있을 수 있습니다. </li><li><code>scrypt</code> : scrypt는 상대적으로 최신 알고리즘이며 위에 알고리즘들 보다 더 성능적으로 뛰어난다고 평가되지만 잘 알려져 있지 않습니다. scrypt는 다이제스트를 생성할 때 메모리 오버헤드를 갖도록 설계되어, 억지 기법 공격 (brute-force attack)을 시도할 때 병렬화 처리가 매우 어렵습니다. 따라서 PBKDF2보다 안전하고 bcrypt에 비해 더 경쟁력 있다고 여겨집니다. </li></ol><h3 id="양방향"><a href="#양방향" class="headerlink" title="양방향"></a>양방향</h3><ol><li><code>AES</code>:  현재 가장 보편적으로 쓰이는 암호와 방식이며 미국 표준 방식인 AES. 128 ~ 256 byte 키를 적용 할 수 있어서 보안성이 뛰어난 공개된 알고리즘입니다. </li><li><code>RSA</code> : 공개키 암호 시스템의 하나로 암호와 뿐만 아니라 전자서명까지 가증한 알고리즘입니다. </li></ol><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p><a href="https://sieunlim.tistory.com/16" target="_blank" rel="external nofollow noopener noreferrer">https://sieunlim.tistory.com/16</a></p><p><a href="https://record22.tistory.com/44" target="_blank" rel="external nofollow noopener noreferrer">https://record22.tistory.com/44</a></p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2019/12/07/%EC%95%94%ED%98%B8%ED%99%94%EB%B3%B5%ED%98%B8%ED%99%94/#disqus_thread</comments>
    </item>
    
    <item>
      <title>(ENG) Realtime Virus Scanning</title>
      <link>https://mchoi07.github.io/2019/11/20/nifi-virus-scanning/</link>
      <guid>https://mchoi07.github.io/2019/11/20/nifi-virus-scanning/</guid>
      <pubDate>Wed, 20 Nov 2019 20:14:06 GMT</pubDate>
      <description>
      
        &lt;p&gt;To protect our system and computer we should make sure that data which we download is clean. Everytime we bring data to our system or user upload data such as file attachments, we must make sure that data is free from viruses and trojans. &lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>To protect our system and computer we should make sure that data which we download is clean. Everytime we bring data to our system or user upload data such as file attachments, we must make sure that data is free from viruses and trojans. </p><a id="more"></a><p>If our system has sensitive data and critical for operation you have to be more cautious about bringing data to your system - cyber attack, nowadays, is being serious and cunning. </p><p>In a normal usecase, we set up Anti Virus (AV) scanner on a file system. AV scanner monitor our file system and RAM in real-time or batch. However, it cannot make sure that each file doesn’t have any malicious content in real-time. In this project, we will use two open source products to detect virus/trojan in realtime. We are going to use <code>Apache Nifi</code> and <code>ClamAV</code> </p><p><a href="https://nifi.apache.org/" target="_blank" rel="external nofollow noopener noreferrer">Apache Nifi</a> is a very powerful, easy to use and stable system to process and distribute data between disparate system. Apache Nifi is a real time data ingestion platform, which can transfer and manage data transfer between different sources and destination systems. </p><p><a href="https://www.clamav.net/" target="_blank" rel="external nofollow noopener noreferrer">ClamAV</a> is an open source antivirus engine for detecting trojans, viruses, malware &amp; other malicious threats. </p><h2 id="1-Usecase"><a href="#1-Usecase" class="headerlink" title="1. Usecase"></a>1. Usecase</h2><p>A usecase is that user need to transfer some files to the applicaion, and we have to make sure that the files don’t contain any malicious codes or contents. Since this is not bulk transformation, we want to transfer a file to endpoint in realtime after scanning. A diagram below is a high level work flow of this usecase. </p><p><img src="/2019/11/20/nifi-virus-scanning/workflow1.png" alt="workflow1"></p><h2 id="2-Setting-Nifi-Server"><a href="#2-Setting-Nifi-Server" class="headerlink" title="2. Setting Nifi Server"></a>2. Setting Nifi Server</h2><p>There are many different ways that you could set up Nifi server depending on the operating system. In this project, I am using <code>Ubuntu 16.04</code>. </p><h3 id="Updating-and-Upgrading-apt-get"><a href="#Updating-and-Upgrading-apt-get" class="headerlink" title="Updating and Upgrading apt-get"></a>Updating and Upgrading apt-get</h3><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">apt-get autoclean</span><br><span class="line">apt-get clean all</span><br><span class="line">apt-get -y update</span><br><span class="line">apt-get -y upgrade</span><br></pre></td></tr></table></figure><h3 id="Installing-Java-JRE"><a href="#Installing-Java-JRE" class="headerlink" title="Installing Java (JRE)"></a>Installing Java (JRE)</h3><p>Apache Nifi is built on Java. We have to have java installed in the system</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt install oracle-java8-installer -y</span><br></pre></td></tr></table></figure><h3 id="Installing-Nifi"><a href="#Installing-Nifi" class="headerlink" title="Installing Nifi"></a>Installing Nifi</h3><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget "https://www-us.apache.org/dist/nifi/1.10.0/nifi-1.10.0-bin.tar.gz"</span><br><span class="line"></span><br><span class="line">mkdir /opt/nifi</span><br><span class="line"></span><br><span class="line">tar -xvzf nifi-1.10.0-bin.tar.gz --directory /opt/nifi --strip-components 1</span><br></pre></td></tr></table></figure><h3 id="Set-JAVA-HOME"><a href="#Set-JAVA-HOME" class="headerlink" title="Set JAVA_HOME"></a>Set JAVA_HOME</h3><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/java-8-oracle</span><br><span class="line">source ~./bashrc</span><br></pre></td></tr></table></figure><h3 id="Start-Stop-Apache-Nifi"><a href="#Start-Stop-Apache-Nifi" class="headerlink" title="Start/Stop Apache Nifi"></a>Start/Stop Apache Nifi</h3><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/opt/nifi/bin/nifi.sh start</span><br><span class="line"><span class="hljs-meta">#</span><span class="hljs-bash">or</span></span><br><span class="line">/opt/nifi/bin/nifi.sh stop</span><br></pre></td></tr></table></figure><h3 id="Get-Started"><a href="#Get-Started" class="headerlink" title="Get Started"></a>Get Started</h3><p>You should open a browser to access NiFI GUI.</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-meta">#</span><span class="hljs-bash">default</span></span><br><span class="line"><span class="hljs-meta">#</span><span class="hljs-bash">http://localhost:8080/nifi</span></span><br><span class="line"><span class="hljs-meta">#</span><span class="hljs-bash">or</span></span><br><span class="line"><span class="hljs-meta">#</span><span class="hljs-bash">http://IP-Address:8080/nifi</span></span><br></pre></td></tr></table></figure><p>If you need to change port:</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vi /opt/nifi/conf/nifi.properties</span><br><span class="line"><span class="hljs-meta">#</span><span class="hljs-bash"> change the defalt port to what you desire</span></span><br></pre></td></tr></table></figure><p>If everything is good you should be able to see this screen.</p><p><img src="/2019/11/20/nifi-virus-scanning/nifi_main_page.png" alt="nifi_main_page"></p><h2 id="2-Setting-ClamAV-Server-at-rest"><a href="#2-Setting-ClamAV-Server-at-rest" class="headerlink" title="2. Setting ClamAV Server at rest"></a>2. Setting ClamAV Server at rest</h2><p>We are going to deply a virus scanner and make it usable in a server at REST. Even though we have multiple applications like one for email attachment, SFTP, etc., we just need to deploy a AV scanner for many applicaions.  </p><p>Simple Clam AV REST Proxy. This will be built on top of clamav-java. Pleas fine more detail <a href="https://github.com/solita/clamav-rest" target="_blank" rel="external nofollow noopener noreferrer">here</a>.</p><p>We need two containers. One is <a href="https://hub.docker.com/r/mkodockx/docker-clamav" target="_blank" rel="external nofollow noopener noreferrer">ClamAV daemon</a> as a Docker images. It <em>builds</em> with a current virus database and <em>runs</em> <code>freshclam</code> in the background constantly updating the virus signature database. <code>clamd</code> itself is listening on exposed port <code>3310</code>.</p><p>Another one is the server implementation. This is a precompiled and packaged docker container running the server. You also need the ClamAV virus scanner for the REST endpoint.</p><p>To run use something like this.</p><ol><li>Start ClamAV server, using <a href="https://hub.docker.com/r/mkodockx/docker-clamav/" target="_blank" rel="external nofollow noopener noreferrer">https://hub.docker.com/r/mkodockx/docker-clamav/</a> here <code>docker run -d --name clamav-server -p 3310:3310 mkodockx/docker-clamav</code></li><li>Test that it’s running ok: <code>curl localhost:3310</code> <code>UNKNOWN COMMAND</code></li><li>Start the REST API image, clamd-server docker container linked to this container. <code>docker run -d -e &#39;CLAMD_HOST=clamav-server&#39; -p 8080:8080 --link clamav-server:clamav-server -t -i lokori/clamav-rest</code></li><li>Test the REST api: <code>curl localhost:8080</code> <code>Clamd responding: true</code></li></ol><p><strong>Testing the REST service</strong></p><p>You can use <a href="http://curl.haxx.se/" target="_blank" rel="external nofollow noopener noreferrer">curl</a> as it’s REST. Here’s an example test session:</p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">curl localhost:8080</span><br><span class="line">Clamd responding: true</span><br><span class="line"></span><br><span class="line">curl -F &quot;name=blabla&quot; -F &quot;file=@./eicar.txt&quot; localhost:8080/scan</span><br><span class="line">Everything ok : false</span><br></pre></td></tr></table></figure><p>EICAR is a test file which is recognized as a virus by scanners even though it’s not really a virus. Read more <a href="http://www.eicar.org/86-0-Intended-use.html" target="_blank" rel="external nofollow noopener noreferrer">EICAR information here</a>.</p><h2 id="3-Design-Dataflow-in-Nifi"><a href="#3-Design-Dataflow-in-Nifi" class="headerlink" title="3. Design Dataflow in Nifi"></a>3. Design Dataflow in Nifi</h2><p>In our previous discussion, we’ve setup nifi server. </p><p>We’ll use three processor to make it working. <code>GetFile</code>, <code>ExecuteStreamCommand</code>, <code>RouteOnAttribute</code> and <code>PutFile</code>. <code>GetFile</code> and <code>PutFile</code> can be chnaged to any endpoint of your application. For example, we could get a file from SFTP and put file to HDFS. </p><p><img src="/2019/11/20/nifi-virus-scanning/Nifi-Data-Flow.png" alt="Nifi-Data-Flow"></p><p>I would like to focus on <code>ExecuteStreamCommand</code> becase rest of processors are straight forward. Please find more information about those processors from an offical <a href="https://nifi.apache.org/" target="_blank" rel="external nofollow noopener noreferrer">Apache Nifi Website</a>.</p><p><code>ExecuteStreamCommand</code> will executes an external command on the contents of a flow file, and creates a new flow file with the results of the command. We will use Python. Therefore, when files are come from GetFile Nifi will execute a python script to check the virus via API from ClamAV server. You should install <code>python</code> properly in your Nifi server. </p><p> ![ExecuteStreamCommand Configuration](/2019/11/20/nifi-virus-scanning/ExecuteStreamCommand Configuration.png)</p><p>This is a configuration of <code>Command Arguments</code></p><ol><li><code>Command Path</code> is where your python command located.</li><li><code>Working Directory</code> is where your python script is located.</li><li><code>Command Arguments</code> is your python script</li><li><code>OutPut Destination Attribute</code> Make sure that you define this value because we have to keep our content of file. By doing so we will write the result of scanning as an attribute. And then we will sort out files throught <code>RouteOnAttribute</code> processor. </li></ol><p>Let’s take a look our python script.</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> sys</span><br><span class="line"><span class="hljs-keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">if</span> __name__ ==<span class="hljs-string">'__main__'</span>:</span><br><span class="line">      url = <span class="hljs-string">'http://localhost:9090/scan'</span></span><br><span class="line">      payload = &#123;<span class="hljs-string">'name'</span>: <span class="hljs-string">'value1'</span>&#125;</span><br><span class="line">      systemin = sys.stdin</span><br><span class="line">      files = &#123;<span class="hljs-string">'file'</span>: systemin&#125;</span><br><span class="line">      r = requests.post(url, files=files, data=payload)</span><br><span class="line">      <span class="hljs-keyword">if</span> <span class="hljs-string">'false'</span> <span class="hljs-keyword">in</span> r.text:</span><br><span class="line">         sys.stdout.write(<span class="hljs-string">"False"</span>)</span><br><span class="line">          </span><br><span class="line">      sys.stdout.write(<span class="hljs-string">"True"</span>)</span><br></pre></td></tr></table></figure><p>If a file is clean, it will have an Attribute value <code>True</code>. If it’s not an attribute value will look like this <code>FalseTrue</code> . We will route our files based one this value. Let’s check the configuration of <code>RouteOnAttribute</code>.</p><p><img src="/2019/11/20/nifi-virus-scanning/RouteOnAttribute.png" alt="RouteOnAttribute Configuration"></p><p>It will let your nifi to send your files to next processor only if the files are clean. </p><h2 id="4-Conclusion"><a href="#4-Conclusion" class="headerlink" title="4. Conclusion"></a>4. Conclusion</h2><p>It might not be only way to do this process. However, both Nifi and Clam are open source, so we don’t need to purchase any other license like Mcafee. Also with these simple tools we could process the files in realtime, and it works fairly well! Hopefully you enjoy this article. If you have a question or comment, you are very welcome to email me at any time. </p><h5 id="References"><a href="#References" class="headerlink" title="References"></a>References</h5><p><a href="https://www.tutorialspoint.com/apache_nifi/apache_nifi_introduction.html" target="_blank" rel="external nofollow noopener noreferrer">link1</a><br><a href="https://dev.solita.fi/2015/06/02/rest-virusscan.html" target="_blank" rel="external nofollow noopener noreferrer">link2</a><br><a href="https://hub.docker.com/r/lokori/clamav-rest/" target="_blank" rel="external nofollow noopener noreferrer">link3</a><br><a href="https://hub.docker.com/r/mkodockx/docker-clamav" target="_blank" rel="external nofollow noopener noreferrer">link4</a></p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2019/11/20/nifi-virus-scanning/#disqus_thread</comments>
    </item>
    
    <item>
      <title>(ENG) Secure File Transfer Protocol</title>
      <link>https://mchoi07.github.io/2019/11/18/sftp/</link>
      <guid>https://mchoi07.github.io/2019/11/18/sftp/</guid>
      <pubDate>Tue, 19 Nov 2019 00:34:49 GMT</pubDate>
      <description>
      
        &lt;p&gt;SFTP (SSH File Transfer Protocol) is a secure file transfer protocol. It runs over the &lt;a href=&quot;https://www.ssh.com/ssh/protocol/&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;SSH protocol&lt;/a&gt;. It supports the full security and authentication functionality of SSH.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>SFTP (SSH File Transfer Protocol) is a secure file transfer protocol. It runs over the <a href="https://www.ssh.com/ssh/protocol/" target="_blank" rel="external nofollow noopener noreferrer">SSH protocol</a>. It supports the full security and authentication functionality of SSH.</p><a id="more"></a><p><strong>System Requirement</strong></p><blockquote><p>Ubuntu 16.04</p></blockquote><h2 id="Step-1-OpenSSH"><a href="#Step-1-OpenSSH" class="headerlink" title="Step 1 - OpenSSH"></a>Step 1 - OpenSSH</h2><p>First, we need to check the SSH connection. By default OpenSSH comes with the most of the Lunux system. Please confirm this with this command.</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -v localhost</span><br></pre></td></tr></table></figure><p>If everything is good, you should be able to see this.</p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; debug1: Connecting to localhost [127.0.0.1] port 22.</span><br><span class="line">&gt; debug1: Connection established.</span><br></pre></td></tr></table></figure><p>If you don’t have OpenSSH set up. You should install it on your system. </p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install openssh-server</span><br><span class="line"></span><br><span class="line">sudo systemctl stop ssh.service</span><br><span class="line">sudo systemctl start ssh.service</span><br><span class="line">sudo systemctl enable ssh.service</span><br></pre></td></tr></table></figure><h2 id="Step-2-Create-SFTP-GROUP-and-USER"><a href="#Step-2-Create-SFTP-GROUP-and-USER" class="headerlink" title="Step 2 - Create SFTP GROUP and USER"></a>Step 2 - Create SFTP GROUP and USER</h2><p><strong>Create a New User</strong></p><p>Switch to the root user:</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo -s</span><br></pre></td></tr></table></figure><p>Add a new user</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">adduser &lt;UbuntuUsername&gt;</span><br></pre></td></tr></table></figure><p>You will be prompted to add a password. Put a simple password and change it later.</p><p><strong>Create a Group</strong></p><p>We have to create the sftp_group first. You could name it whatever you want.</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo groupadd sftp_group</span><br></pre></td></tr></table></figure><p>Now, we could add user into this group</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -aG sftp_group &lt;UbuntuUsername&gt;</span><br></pre></td></tr></table></figure><h2 id="Step-3-Configure-SFTP-Chroot"><a href="#Step-3-Configure-SFTP-Chroot" class="headerlink" title="Step 3 - Configure SFTP / Chroot"></a>Step 3 - Configure SFTP / Chroot</h2><p>A chroot enable system to isolate application form the rest of your computer by limiting them. If you turn on chroot on user account, the account will be isolated and can only access its own directory and files.</p><p>There are two different ways which you could do access control. </p><p><strong>Locking down per user</strong></p><p>We might need to provide limited access to our user because if we give full access to our user it would be a huge security flaw. If you want to lock down user to only specific directory to add and remove files, please follow steps below.</p><ol><li>Create desired path and directory.</li></ol><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-meta">#</span><span class="hljs-bash">For example</span></span><br><span class="line">/home/sftp_root/sftp_home</span><br></pre></td></tr></table></figure><p><code>/home/sftp_root</code> is owned by root while <code>../sftp_home</code> can be ownd by our user or user group</p><ol start="2"><li>Change a permission</li></ol><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod 755 /home/sftp_root</span><br></pre></td></tr></table></figure><p>This changes our permissions to only allow writing by the user who owns the directory while read and execute to everyone else.</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-meta">#</span><span class="hljs-bash">it changes a directory to be owned by the user root and group root.</span></span><br><span class="line">chown root:root /home/sftp_root</span><br><span class="line"></span><br><span class="line"><span class="hljs-meta">#</span><span class="hljs-bash">it gives ownership to the user and usergroup only to sftp_home.</span></span><br><span class="line">chown &lt;User&gt;:&lt;Usergroup&gt; /home/sftp_root/sftp_home</span><br></pre></td></tr></table></figure><ol start="3"><li>Locking down user</li></ol><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc.ssh/sshd_config</span><br></pre></td></tr></table></figure><p>Find this and comment it out</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Subsystem sftp /var/lib/openssh/sftp-server</span><br><span class="line"><span class="hljs-meta">#</span><span class="hljs-bash">to</span></span><br><span class="line"><span class="hljs-meta">#</span><span class="hljs-bash">Subsystem sftp /var/lib/openssh/sftp-server</span></span><br></pre></td></tr></table></figure><p>And add this:</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Subsystem sftp internal-sftp</span><br><span class="line">   </span><br><span class="line">Match User [Your New Username] ChrootDirectory /home/sftp_root</span><br><span class="line">X11Forwarding no</span><br><span class="line">AllowTcpForwarding no</span><br><span class="line">AllowAgentForwarding no</span><br><span class="line">ForceCommand internal-sftp</span><br><span class="line">PasswordAuthentication yes</span><br></pre></td></tr></table></figure><p><strong>Match User</strong>: Tells the SSH server to only apply the following settings to the one user</p><p><strong>ChrootDirectory:</strong> This tells the server what directory our user is allowed to ONLY work within this directory</p><p><strong>X11Forwading, AllowTCPForwarding, AllowAgentForwarding:</strong> Prohibits the user from port forwarding, tunneling and X11 forwarding fot the user. These are all security things.</p><p><strong>ForceCommand internal-sftp:</strong> Forces the SSH server to the run the SFTP program upon access which disables shell access.</p><p><strong>PasswordAuthentication:</strong> Allows for the user to login with a typed password. You can remove this is you would rather use a security key which is by far safer.</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart ssh.service</span><br><span class="line"><span class="hljs-meta">#</span><span class="hljs-bash">or</span></span><br><span class="line">/etc/init.d/ssh restart</span><br></pre></td></tr></table></figure><h4 id="Locking-down-User-Group"><a href="#Locking-down-User-Group" class="headerlink" title="Locking down User Group"></a>Locking down User Group</h4><p>Only step 4 is different from locking down per user. Add this:</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Subsystem sftp internal-sftp</span><br><span class="line"></span><br><span class="line">Match Group sftp_group</span><br><span class="line">X11Forwarding no</span><br><span class="line">AllowTcpForwarding no</span><br><span class="line">ChrootDirectory /home/sftp_root</span><br><span class="line">ForceCommand internal-sftp</span><br></pre></td></tr></table></figure><h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><p><a href="https://monovm.com/blog/connect-to-sftp-on-centos-without-shell-access/" target="_blank" rel="external nofollow noopener noreferrer">Thumnail</a><br><a href="http://www.inanzzz.com/index.php/post/ef2z/setting-up-a-sftp-server-and-users-on-ubuntu-16-04" target="_blank" rel="external nofollow noopener noreferrer">link1</a><br><a href="https://websiteforstudents.com/setup-retrictive-sftp-with-chroot-on-ubuntu-16-04-17-10-and-18-04/" target="_blank" rel="external nofollow noopener noreferrer">link2</a></p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2019/11/18/sftp/#disqus_thread</comments>
    </item>
    
    <item>
      <title>(ENG) Simulation 04 - Output Analysis</title>
      <link>https://mchoi07.github.io/2019/07/30/output-analysis/</link>
      <guid>https://mchoi07.github.io/2019/07/30/output-analysis/</guid>
      <pubDate>Tue, 30 Jul 2019 20:40:25 GMT</pubDate>
      <description>
      
        &lt;p&gt;Ananyzing the ouput of a simulation model is important. How can we be sure that our output is proper and will not hurt an experiment result using those outputs. &lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Ananyzing the ouput of a simulation model is important. How can we be sure that our output is proper and will not hurt an experiment result using those outputs. </p><a id="more"></a><p>Keep this in mind - out is rearely i.i.d. Why do we worry about output? In put processes driving a simulation are random variables. It means our output from the simulation must be random. If we runs the simulation it only yields estimates of measure of system performace, and these estimators are themselves random variables, and are therfore subject to sampling error. Sampling error must be taken into account to make valid inferences concerning system performance. </p><p><strong>Measures of Interest</strong></p><ul><li>Means - what is the mean customer waiting time? </li><li>Variances - how much is the waiting time liable to vary?</li><li>Quantiles - what’s the 99% quantile of the line length in a certain queue?</li><li>Sucess probabilities - will my job be completed on time?</li><li>Would like point estimators and confidence intervals for the above. </li></ul><p>There are two general types of simulations with respect to output analysis. To facilitate the presentation, we identify two types of simulations with respect to output analysis: </p><ul><li><em>Finite-Horizon (Terminating) Simulations</em> - Interested in short-term performance<ul><li>The termincation of finite-horizon simulation takes place at a specific time or is caused by the occurrence of a specific event.</li><li>EX1 - Mass transit system during rush hour</li><li>EX2 - Distribution system over one month</li></ul></li><li><em>Steady-State simulations</em> - Interested in long-term performance<ul><li>The purpose of <em>steady-state simulation</em> is to study the long-run behavior of a system. A performance measure is a steady-state parameter if it is a characteristic of the equilibrium distribution of an output process. </li><li>EX1 - Continuously operating communication system where the objective is the computation of the mean delay of a packet in the long run</li><li>EX2 - Distribution system over a long period of time</li><li>EX3 - Markov chains</li></ul></li></ul><p><strong>Finite-Horizon Simulation</strong></p><p>First thing we have to do to conduct this simulation is getting expected values from replications. So basically, we need to decide a number of independetn replications (IR). IR estimates Var($\bar{Y}_m$) by conducting $r$ independent simulation runs (replications) of the system under study, where each replication consists of $m$ observations. It is easy to make the replications independent - just re-initialize each replication with a different pseudo-random number seed Sample means from replication</p><p>If each run is started under the same operating conditions (e.g., all queues empty and idle), then the replication sample means $Z_1, Z_2, . . . , Z_r$ are $i.i.d.$ random variables. </p><p><img src="/2019/07/30/output-analysis/02.png" alt=""></p><p><img src="/2019/07/30/output-analysis/04.png" alt=""></p><p>Suppose we want to estimate the expected average waiting time for the first m = 5000 customers at the bank. We make r = 5 independent replications of the system, each initialized empty and<br>idle and consisting of 5000 waiting times. The resulting replicate means are:</p><p><img src="/2019/07/30/output-analysis/01.png" alt=""></p><p><img src="/2019/07/30/output-analysis/05.png" alt=""></p><p><strong>Steady-state simulation</strong></p><p>How about we need to simulate the entire time line? We should consider to use a steady-state simulation. Estimate some parameter of interest, e.g., the mean customer waiting time or the expected profit produced by a certain factory configuration. In particular, suppose the mean of this output is the unknown quantity $\mu$. We’ll use the sample mean $\bar{Y}_n$ to estimate $\mu$</p><p>We must accompany the value of any point extimator with a measure of its variance. In stead of Var($\bar{Y}_n)$ we canestimate the <em>variance parameter</em>,</p><p><img src="/2019/07/30/output-analysis/06.png" alt=""></p><p>Thus, $\sigma^2$ is imply the sume of all covariances! $\sigma^2$ pops up all over the place: simulation output analysis, Brownian motions, fnancial engineering application, etc. </p><p><img src="/2019/07/30/output-analysis/07.png" alt=""></p><p>Many methods for estimating $\sigma^2$ and for conducting steady-state output analysis in general:</p><ol><li><p>Batch means</p><p>The method of <em>batch means</em> (BM) is often used to estimate $\sigma^2$ and to calculate confidence intervals for $\mu$</p><p>Idea: Divide one long simulation run into a number of contiguous batches, and then appeal to a central limit theorem to assume that the resulting batch sample means are approximately i.i.d. normal. </p><p>In particular, suppose that we partition $Y_1, Y_2, . . . , Y_n$ into $b$ nonoverlapping, contiguous batches, each consisting of $m$ observations (assume that $n = bm$) </p><p><img src="/2019/07/30/output-analysis/08.png" alt=""></p><p>The $i$th batch mean is the sample mean of the $m$ observations from batch $i = 1, 2, . . . , b$</p><p><img src="/2019/07/30/output-analysis/10.png" alt=""></p><p><img src="/2019/07/30/output-analysis/11.png" alt=""></p><p><img src="/2019/07/30/output-analysis/12.png" alt=""></p><p><img src="/2019/07/30/output-analysis/13.png" alt=""></p><p><img src="/2019/07/30/output-analysis/14.png" alt=""></p><p><img src="/2019/07/30/output-analysis/15.png" alt=""></p></li></ol><p>$E[H]$ decreases in b, though it smooths out around b = 30. A common recommendation is to take b =. 30 and concentrate on increasing the batch size m as much as possible. </p><p>The technique of BM is intuitively appealing and easy to understand. </p><p>But problems can come up if the Yj ’s are not stationary (e.g., if significant initialization bias is present), if the batch means are not normal, or if the batch means are not independent. </p><p>If any of these assumption violations exist, poor confidence interval coverage may result — unbeknownst to the analyst.</p><p>To ameliorate the initialization bias problem, the user can truncate some of the data or make a long run </p><p>In addition, the lack of independence or normality of the batch means can be countered by increasing the batch size m.</p><p><img src="/2019/07/30/output-analysis/16.png" alt=""></p><p><img src="/2019/07/30/output-analysis/17.png" alt=""></p><p><img src="/2019/07/30/output-analysis/18.png" alt=""></p><p><img src="/2019/07/30/output-analysis/19.png" alt=""></p><p><img src="/2019/07/30/output-analysis/20.png" alt=""></p><p><img src="/2019/07/30/output-analysis/21.png" alt=""></p><p><img src="/2019/07/30/output-analysis/22.png" alt=""></p><h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><p><a href="https://www.computerhope.com/jargon/s/stdin.htm" target="_blank" rel="external nofollow noopener noreferrer">Thumnail</a><br>Georgia Tech’s <code>ISYE6644</code> class content</p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2019/07/30/output-analysis/#disqus_thread</comments>
    </item>
    
    <item>
      <title>(ENG) Simulation 03 - Input Analysis</title>
      <link>https://mchoi07.github.io/2019/07/23/input-analysis/</link>
      <guid>https://mchoi07.github.io/2019/07/23/input-analysis/</guid>
      <pubDate>Tue, 23 Jul 2019 20:40:08 GMT</pubDate>
      <description>
      
        &lt;p&gt;How can we tell our random variables are well made? In simulation terminology, we have something called &lt;code&gt;input analysis&lt;/code&gt;.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>How can we tell our random variables are well made? In simulation terminology, we have something called <code>input analysis</code>.</p><a id="more"></a><p>In my previous two postings, <a href="">random number</a>, <a href="">random variable</a>, I’ve talked about how to generate a random number and how we could make a radom variable by using those random numbers. Then how can we tell our random variables are well made? In simulation terminology, we have something called <code>input analysis</code>. The random variables are our input, and we need to analyze those inputs to verify it’s relevance. If you use the wrong input random variables in the simulation model, it will result in wrong output. A proper input analysis can save you from <code>Garbage-in-garbage-out</code></p><p>How can we conduct a proper input analysis?</p><ol><li>We have to collect data<ul><li>Data Sampling - we could shuffle the data and take some samples from there</li></ul></li><li>We have to figure out a distribution of data<ul><li>Plot the data to histogram </li><li>Discrete vs contunuous</li><li>Univariate / Multivariate</li><li>If data is not enough — we can guess a good distribution</li></ul></li><li>We have to do a statistical test to verify the distribution</li></ol><h2 id="1-Point-Estimation"><a href="#1-Point-Estimation" class="headerlink" title="1. Point Estimation"></a>1. Point Estimation</h2><p>A <em>statistic</em> can not explicitly depend on any <code>unknown parameters</code> because statics are based on the actural observations. <em>Statistics</em> are random variable - we could expect to have two different values of a static when we take two different samples. But we need to find the <code>unknown parameters</code>. How can we do it? we could estimate <code>unknown parameters</code> from the existing probability of distribution.  </p><p>Let $X_1, . . . . , X_n$ be i.i.d. Random Variables and let $M(X) ≡ M(X_1, . . . . , X_n)$ be a statistic based on the $X_i$’s. <strong>Suppose we use $M(X)$ to estimate some unknown parameter $θ$ Then $M(X)$ is called a <em>point estimator</em> for $θ$ .</strong></p><ul><li>$\bar{x}$ is a point estimator for the mean $μ = E[Xi]$</li><li>$S^2$ is a point estimator for the variance  $σ2 = Var(Xi) $</li></ul><p>*<em>It would be nice if $M(X)$ had certain properties: *</em></p><ul><li>Its expected value should equal the parameter it’s trying to estimate</li><li>It should have low variance</li></ul><p>We all know that good estimator should be unbiased because if we use the biased estimator we won’t figure out whether our model is good or not. We will be fooled by a biased estimator. $T(X)$ is <em>unbiased</em> for $θ$ if $E[M(X)] = θ$</p><p><strong>EX1</strong> -  Suppose $X_1, . . . , X_n$ are i.i.d. anything with mean μ. Then</p><p><img src="/2019/07/19/random-variable/sample_mean.png" alt="sample_mean"></p><p>So $\bar{X}$ is alwys unbiased for $\mu$. That’s why ${\bar{X}}$ is called the <em>sample mean</em> </p><p><strong>EX2</strong> -  Suppose $X_1, . . . , X_n$ are i.i.d. anything with mean μ and variance $\sigma^2$. Then</p><p><img src="/2019/07/19/random-variable/sample_variance.png" alt="sample_variance"></p><p>Thus, $S^2$ is always unbiased for $\sigma^2$. This is why $S^2$ is called the sample variance. </p><h2 id="2-Mean-Square-Error"><a href="#2-Mean-Square-Error" class="headerlink" title="2. Mean Square Error"></a>2. Mean Square Error</h2><p>In a perfect sceanario, our estimator will be exactly same as $\theta$. Then we will see no error. However, it’s not the real case. Our goal is to reduce the error between an estimator and $\theta$. </p><p>The <em>mean squared error</em> of an estimator $M(X)$ of $\theta$ is </p><p>$$<br>\begin{aligned}<br> MSE(M(X)) ≡ E[(M(X)-\theta)^2]<br>\end{aligned}<br>$$</p><p>$$<br>\begin{aligned}<br> Baia(M(X)) ≡ E[T(X) - \theta]<br>\end{aligned}<br>$$</p><p>We could interpret MSE like this: </p><p><img src="/2019/07/19/random-variable/mse.png" alt="mse"></p><p>Lower MSE mean we are avoiding the bias and <strong>variance</strong>. Our goal should be finding a good estimator which can lower our error. If $M_1(X)$ and $M_2(X)$ are two estimators of $\theta$, we’d usually prefer the one with the lower MSE — even if it happens to have higer bias.</p><h2 id="3-Maximum-Linelihood-Estimators"><a href="#3-Maximum-Linelihood-Estimators" class="headerlink" title="3. Maximum Linelihood Estimators"></a>3. Maximum Linelihood Estimators</h2><p>What if we don’t have a set of data, but we have a pdf/pmf $f(x)$ of the distribution. How can we find the $\theta$?</p><p>Consider an i.i.d. random sample $X_1, . . . , X_n$, where each $X_i$ has pdf/pmf $f(x)$. Further, suppose that $\theta$ is some unknown parameter from $X_i$. The likelihood function is $L(\theta) ≡ \prod f(x_i)$</p><p>The maximum likelihood estimator (MLE) of $\theta$ is the value of $\theta$ that maximizes $L(\theta)$. The MLE is a function of the $X_i$’s and is a RV. </p><p><strong>EX1</strong> -  Suppose $X_1, . . . , X_n$ ~ Exp$(\gamma)$. Find the MLE for $\gamma$</p><p><img src="/2019/07/19/random-variable/mle-1.png" alt="mle-1"></p><p>Since the natural log function is one-to-one, it’s easy to see that the $\gamma$ that maximizes $L(\gamma)$ also maximize $ln(L(\gamma))$</p><p><img src="/2019/07/19/random-variable/mle-2.png" alt="mle-2"></p><p><img src="/2019/07/19/random-variable/mle-3.png" alt="mle-3"></p><p>This implies that the MLE is $\hat{\gamma} = 1 / \bar{X}$</p><p><strong>Invariance Property</strong> </p><p>If $\hat{\theta}$ is the MLE of some parameter $\theta$ and $h(.)$ is a one-to-one function, then $h(\hat{\theta})$ is the MLE of $h(\theta)$</p><p>For Bern(p) distribution the MLE of $p$ is $\hat{p}=\bar{X}$ (which also happens to be unbiased). If we consider the 1:1 function $h(\theta) = \theta^2$ for ($\theta &gt; 0)$, then the <em>Invariance property</em> says that the MLE of $p^2$ is $\bar{X}^2$</p><p>But such a property does not hold for unbiasedness</p><p>$$<br>\begin{aligned}<br>E[S^2] = σ^2<br>\end{aligned}<br>but<br>\begin{aligned}<br>E[\sqrt{S^2}]= σ<br>\end{aligned}<br>$$</p><p>Really that MLE for $\sigma^2$ is $\hat{\sigma^2} = 1/n\sum(X_i - \bar{X})^2$. The good news is that we can still get the MLE for $\sigma$. If we consider the 1:1 function $h(\theta) = +\sqrt{\theta}$, then the invariance property says that the MLE of $\sigma$ is </p><p>$$<br>\begin{aligned}<br>\hat{\sigma} = \sqrt{\hat{\sigma^2}} = \sqrt{\sum(X_i-\bar{X})^2/n}<br>\end{aligned}<br>$$</p><h2 id="4-The-Method-of-Moments"><a href="#4-The-Method-of-Moments" class="headerlink" title="4. The Method of Moments"></a>4. The Method of Moments</h2><p>Recall: the $k$th moment of a random variable X is </p><p><img src="/2019/07/19/random-variable/mom-1.png" alt="mom-1"></p><p>Suppose $X_1, . . . , X_n$ are i.i.d. from p.m.f. / p.d.f. $f(x)$. Then the method of moments(MOM) estimator for $E[X^k]$ is</p><p>$$<br>m_k ≡ 1/n\sum X^k_i<br>$$</p><p><img src="/2019/07/19/random-variable/mom-2.png" alt="mom-2"></p><h2 id="5-Goodness-of-Fit-Tests"><a href="#5-Goodness-of-Fit-Tests" class="headerlink" title="5. Goodness-of-Fit Tests"></a>5. Goodness-of-Fit Tests</h2><p>We finanlly guessed a reasonable distribution and then estimated the relevant parameters. Now let’s conduct a formal test to see just how sucessful our toils have been.</p><p>In particular, test</p><p>$$<br>H_0 : X_1, X_2, . . . , X_n - p.m.f / p.d.f. f(x)<br>$$</p><p>At level of significance</p><p>$$<br>\alpha ≡ P(Reject H_0 | H_0 true) =P(\text{Type I Error})<br>$$</p><h5 id="Chi-Square-Test"><a href="#Chi-Square-Test" class="headerlink" title="Chi-Square-Test"></a>Chi-Square-Test</h5><p>Goodness-of-fit test procedure: </p><ol><li>Divide the domain of $f(x)$ into $k$ sets, say, $A_1, A_2, A_3, . . . , A_k$ (distinct points if X is discret r intervals if X is continuous)</li><li>Tally the actual number of observations that fall in ach set, say, $O_i i = 1, 2, . . . , k$. If $p_i ≡ P(X ∈ Ai)$, then $O_i \text{~ Bin(}n,p_i)$</li><li>Determine the expected number of observations that would fall in each set if $H_0$ were true, say, $E_i = E[O-I] = np_i, i = 1,2, . . . , k$</li><li>Calculate a test statistic based on the differences between the $E_i$ and $O_i$.</li></ol><p><img src="/2019/07/19/random-variable/gof.png" alt="gof"></p><ol start="5"><li>A large value of $X^2_0$ indicate a bad fit. We reject $H_0 \text{ if } \chi^2_0 &gt; \chi^2_{\alpha,k-1-s}$, where $s$ is the number of nuknown paramets from $f(x) that have to be estimated. </li></ol><p>Usual recommendation: For the $\chi^2$ g-o-f test to work, pick $k,n$ such that $E_i &gt;= 5$ and $n$ at least 30</p><p><strong>Kolmogorov_Smirnov Goodness-of-Fit Test</strong></p><p>We’ll test $H_0 : X_1, X_2, . . . , X_n$ some distribution with $c.d.f. F (x)$.</p><p>Recall the difincation of the <em>empirical c.d.f. (also called the sample c.d.f)</em> of the data is</p><p>$$<br>\hat{F_n}(x) ≡ (\text{number of } X_i &lt;= x) / n<br>$$</p><p>The Glivenko-Cantelli Lemma says that $\hat{F_n}(x) -&gt; F(x)$ for all $x$ as $ n-&gt;\infinite$.  So if $H_0$ is ture then $\hat{F_n}(x)$ should be a good approxmination to the true c.d.f. $F(x)$, for large $n$ The main question: Does the empirical distribution actually support the assumption that H0 is true? </p><p> <img src="/2019/07/19/random-variable/ks.png" alt="ks"></p><h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><p><a href="https://www.computerhope.com/jargon/s/stdin.htm" target="_blank" rel="external nofollow noopener noreferrer">Thumnail</a><br>Georgia Tech’s <code>ISYE6644</code> class content</p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2019/07/23/input-analysis/#disqus_thread</comments>
    </item>
    
    <item>
      <title>(ENG) Simulation 02 - Random variable</title>
      <link>https://mchoi07.github.io/2019/07/19/random-variable/</link>
      <guid>https://mchoi07.github.io/2019/07/19/random-variable/</guid>
      <pubDate>Fri, 19 Jul 2019 04:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Random variable can be generated from a good random number generator. If real variables has moved the reality, we could design a future with a good random variables.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Random variable can be generated from a good random number generator. If real variables has moved the reality, we could design a future with a good random variables.</p><a id="more"></a><blockquote><p>Author: Minkyu Choi<br>Last updated: 07/19/2019</p></blockquote><p><strong>Inverse Transform Method</strong><br>Inverse transform sampling is a method for generating random numbers from any probability distribution by using its inverse cumulative distribution F−1(x)F−1(x). Recall that the cumulative distribution for a random variable XX is FX(x)=P(X≤x)FX(x)=P(X≤x). In what follows, we assume that our computer can, on demand, generate independent realizations of a random variable UU uniformly distributed on [0,1]</p><p><strong>Cutpoint Method</strong><br>This inverse-transform method has the advantage of having an optimal O(n) setup time. However, the average number of steps required to sample X is not optimal, and if several samples of X are needed, then the cutpoint method offers an average number of two comparison steps needed to sample an observation, yet still has an O(n) initial setup time</p><p>Without loss of generality, we can assume that X = [1, n]. Also, let qi = P(X ≤ i). Then the idea behind the cutpoint method is to choose m ≥ n, and define sets Q1, . . . , Qm for which</p><p><img src="/2019/07/19/random-variable/cutpoint.png" alt="Cutpoint Method"></p><p>for all i = 1, . . . , m. In words, the unit interval [0, 1] is partitioned into m equal sub-intervals of the form $[\frac{(i−1)} m,  \frac{i}m)$, i = 1, . . . , m. And when U falls into the i th sub-interval, then Qi contains all the possible qj values for which F −1 (U) = j. That way, instead of searching through all of the q values, we save time by only examining the qj values in Qi , since these are the only possible values for which $F^{-1} (U) = j$.</p><p><strong>Convolution Method</strong></p><ul><li>Sum of n variables: $x = y_1 + y_2 + … y_n$</li><li>Generate n random variate yi’s and sum </li><li>For sums of two variables, pdf of x = convolution of pdfs of y1 and y2. Hence the name </li><li>Although no convolution in generation </li><li>If pdf or CDF = Sum ⇒ Composition </li><li>Variable x = Sum ⇒ Convolution</li></ul><p><strong>Acceptance-Rejection Method</strong><br>Finding an explicit formula for F −1 (y) for the cdf of a rv X we wish to generate, F(x) = P(X ≤ x), is not always possible. Moreover, even if it is, there may be alternative methods for generating a rv distributed as F that is more efficient than the inverse transform method or other methods we have come across. Here we present a very clever method known as the acceptance-rejection method.</p><p><strong>Composition Method</strong><br>Can be used when m can be expressed as a convex combination of other distributions Fi , where we hope to be able to sample from $F_i$ more easily than from F directly.</p><p><img src="/2019/07/19/random-variable/Cm.png" alt="Composition Method"></p><p><strong>References</strong><br><a href="https://newonlinecourses.science.psu.edu/stat414/node/104/" target="_blank" rel="external nofollow noopener noreferrer">Link-1</a><br><a href="https://stephens999.github.io/fiveMinuteStats/inverse_transform_sampling.html" target="_blank" rel="external nofollow noopener noreferrer">Link-2</a><br><a href="http://web.csulb.edu/~tebert/teaching/lectures/552/variate/variate.pdf" target="_blank" rel="external nofollow noopener noreferrer">Link-3</a><br><a href="https://www.cse.wustl.edu/~jain/cse567-08/ftp/k_28rvg.pdf" target="_blank" rel="external nofollow noopener noreferrer">Link-4</a><br><a href="http://www.columbia.edu/~ks20/4703-Sigman/4703-07-Notes-ARM.pdf" target="_blank" rel="external nofollow noopener noreferrer">Link-5</a></p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2019/07/19/random-variable/#disqus_thread</comments>
    </item>
    
    <item>
      <title>(ENG) Simulation 01 - Random Number</title>
      <link>https://mchoi07.github.io/2019/07/18/random-number/</link>
      <guid>https://mchoi07.github.io/2019/07/18/random-number/</guid>
      <pubDate>Thu, 18 Jul 2019 13:27:50 GMT</pubDate>
      <description>
      
        &lt;p&gt;A simulation is not real but it can represent the real. It’s because a simulation is an imitation of real situation - it can’t be exact but it can be approximate.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>A simulation is not real but it can represent the real. It’s because a simulation is an imitation of real situation - it can’t be exact but it can be approximate.</p><a id="more"></a><p>Most of simulation models are strated from generating random number because randomness creates value on a simulation modeling. It is really important to give an algorithm that produces a sequence of pseudo-random number (PRNs) $R_1, R_2,…$ that “appear” to be iid Unif(0,1). There are many different Uniform(0,1) Generators.</p><p><strong>Output of random device</strong></p><ul><li>Nice randomness properties. However, Unif(0,1) sequence storage difficult, sot it’s tough to repeat experiment</li><li>Examples:<ul><li>flip a coin</li><li>particle count by Geiger coutner</li><li>least significant digits of atomic clock</li></ul></li></ul><p><strong>Table of random numbers</strong></p><ul><li>List of digits supplied in tables - A Million random Digits with 100,00 Normal Deviates.</li><li>Cumbersome, slow, table too small - not very useful </li></ul><p><strong>Mid-Square</strong></p><ul><li>Idea - Take the middle part of square of the previous random number. John von Neumann was a brilliant and fun-loving guy, but method is terrible</li><li>Example: Take $R_i = X_i/10000$, ∀i, where the Xi’s are positive<br>integers &lt; 10000.</li><li>Set seed $X_0 = 6642$; then $6632^2$ = 43<strong>9834</strong>24</li><li>so $X_1 = 9834$; then $9834^2$ - 96<strong>7075</strong>56</li><li>so $X_2$ = 7075, etc,…</li><li>Unfortunately, positive serial correlation in $R_i$’s. Also, occasionally degenerates; eg., consider $X_i$ = 0003</li></ul><p><strong>Fibonacci</strong></p><ul><li>These methods are also no good!! </li><li>Take $X_i = (X_{i-1} + X_{i-2})mod(m), i = 1,2,…,$ where $R_i = X_i/m$ ,$m$ is the modulus, $X_01,X_0$ are seeds, and $a = b mod m$ if $a$ is the remainer of $b/m$ </li><li>Problem: small numbers follow small numbers</li><li>Also, it’s not possible to get $X_{i-1} &lt; X_{i+1} &lt; X_i$ or </li><li>$X_i &lt; X_{i+1} &lt; X_{i-1} $ (which should occur w.p 1/3)</li><li>$X_{i+1}$ </li></ul><p><strong>Linear congruential (most commonly used in practice</strong></p><ul><li>LCGs are the most widely used generators. These are pretty good when implemented properly. </li><li>$X_i = (aX_{i-1} + c) mod(m)$, where $X_0$ is the seed.</li><li>$R_i = X_i/m, i = 1,2,…$ </li><li>Choose a,c,m carefully to get good stastistical quality and long period or cycle length, i.e., time until LCG starts to repeat itself. </li><li>If $c = 0$, LCG is called a multiplicative generator </li></ul><p><strong>Tausworthe (linear recursion mod 2)</strong></p><ul><li><p>Tausworthe Generator is a kind of multicative recursive generator.</p></li><li><p>$X_{i+1} = (aX_{i-1} + c) mod(2)$, where $X_0$ is the seed.</p></li></ul><p><strong>Reference</strong></p><p>Georgia Tech’s <code>ISYE6644</code> class content</p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2019/07/18/random-number/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
