<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title></title>
    <link>https://mchoi07.github.io/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Sat, 07 Dec 2019 21:27:16 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>암호화복호화</title>
      <link>https://mchoi07.github.io/2019/12/07/%EC%95%94%ED%98%B8%ED%99%94%EB%B3%B5%ED%98%B8%ED%99%94/</link>
      <guid>https://mchoi07.github.io/2019/12/07/%EC%95%94%ED%98%B8%ED%99%94%EB%B3%B5%ED%98%B8%ED%99%94/</guid>
      <pubDate>Sat, 07 Dec 2019 21:22:36 GMT</pubDate>
      <description>
      
        &lt;p&gt;여러분들의 데이터는 안전합니까? &lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>여러분들의 데이터는 안전합니까? </p><a id="more"></a><p>오늘은 암호화 (Encryption) 복호화(Decryption)에 대한 이야기를 나눠보고자 합니다. </p><h2 id="암호와-복호와"><a href="#암호와-복호와" class="headerlink" title="암호와/복호와"></a>암호와/복호와</h2><p>암호화는 데이터를 암호화 하여서 누군가가 읽을 수 없도록 정보를 전달화는 과정입니다. 암호와에는 여러가지 알고리즘이 쓰임니다. </p><p>복호와는 암호화된 정보를 다시 읽을 수 있게하는 과정으로써 데이터가 누출되더라도 복호화를 하지못하면 암호화된 데이터를 읽을 수 없습니다. </p><h2 id="암호와-종류"><a href="#암호와-종류" class="headerlink" title="암호와 종류"></a>암호와 종류</h2><ol><li><p>단방향 암호: 암호화 후 복호화 할 수 없습니다. 예를 들면 <code>사용자 비밀번호</code> 사용자가 입력한 비밀번호를 암호화 하고 모든 접근자는 암호화 된 코드를 다시 평문으로 볼 수 없습니다. 해킹이 되어도 복호화가 굉장히 어렵습니다. 예외적인 경우로는 <code>RainbowTable</code> 이 있습니다. </p><p>더 자세한 정보: <a href="https://www.youtube.com/watch?v=TeIVhioUAXs" target="_blank" rel="external nofollow noopener noreferrer">참고영상</a></p></li><li><p>양방향 암호: 암호와와 복호화 모두 가능합니다. <code>사용자 주소, 이메일, 전자서명</code> 등과 같이 정보를 재사용해야 되는 경우에 사용합니다. </p><p>양방향 암호에는 크게 두 가지 종류가 있습니다. </p><ul><li><p>대칭형 암호 (비밀키 암호)</p><p>대칭형 암호는 암호화 할 때 사용하는 키와 복호화 할 때 사용하는 키가 동일한 암호화 기법입니다. 예를 들면 “APPLE”를 “ABCDE”로 암호화 했다면 복호화도 반드시 “ABCDE”로 해야됩니다. 예를 들면 <code>AES Algorithm</code></p><p>하지만 대칭형 암호에는 키 배송에 관한 문제가 발생됩니다. 송신 측에서는 데이터를 암호화한 후에 수신 측에 암호키를 전달해야되고 전달하는 과정에서 이 함호 키가 털리면 데이터가 유출됩니다. 그리고 키 관리가 어렵습니다.</p></li></ul></li></ol><ul><li><p>비대칭형 암호 (공개키 암호)</p><p>비대칭현 암호는 암호와 키와 복호화 키가 다릅니다. </p><p>클라이언트와 서버가 각각의 공개키와 비밀키를 갖고, 서로 공개키를 공개합니다. 클라이언트는 서버의 공개키로 데이터를 암호화한 후에 서버로 보내면 서버는 자신의 비밀키를 가지고 클라이언트가 보낸 데이터를 복호화 합니다. 예를 들면 <code>RSA, Diffe-Hellman, ECC, etc</code></p><p><code>공개키</code> 는 공유되지만 <code>암호키</code> 는 공개되지 않기에 공개키가 중간에 탈취되어도 데이터를 안전하게 지킬 수 있습니다. </p><p>하지만 문제는 비대칭형 암호는 대칭형 암호에 비해 느리고 많은 자료를 암호와 복호화 하는데 불편합니다 단점이 있습니다. </p></li></ul><h2 id="암호-알고리즘"><a href="#암호-알고리즘" class="headerlink" title="암호 알고리즘"></a>암호 알고리즘</h2><h3 id="단방향"><a href="#단방향" class="headerlink" title="단방향"></a>단방향</h3><ol><li><code>SHA</code> : 가장 대표적인 해시함수</li><li><code>PBKDF2</code> : 해시함수의 컨테이너인 PBKDF2는 솔트를 적용한 후 해시 함수의 반복 횟수를 임의로 선택할 수 있다. PBKDF2는 구현하기 쉬운 알고리즘이며 SHA와 같이 검증된 해시 함수만 사용합니다.</li><li><code>bcrypt</code> : 패스워드 저장을 목적으로 설계되었으며 가장 많이 쓰이는 알고리즘입니다. 입력값을 72 byte로 해야하기 때문에 조금 사용에 불편함이 있을 수 있습니다. </li><li><code>scrypt</code> : scrypt는 상대적으로 최신 알고리즘이며 위에 알고리즘들 보다 더 성능적으로 뛰어난다고 평가되지만 잘 알려져 있지 않습니다. scrypt는 다이제스트를 생성할 때 메모리 오버헤드를 갖도록 설계되어, 억지 기법 공격 (brute-force attack)을 시도할 때 병렬화 처리가 매우 어렵습니다. 따라서 PBKDF2보다 안전하고 bcrypt에 비해 더 경쟁력 있다고 여겨집니다. </li></ol><h3 id="양방향"><a href="#양방향" class="headerlink" title="양방향"></a>양방향</h3><ol><li><code>AES</code>:  현재 가장 보편적으로 쓰이는 암호와 방식이며 미국 표준 방식인 AES. 128 ~ 256 byte 키를 적용 할 수 있어서 보안성이 뛰어난 공개된 알고리즘입니다. </li><li><code>RSA</code> : 공개키 암호 시스템의 하나로 암호와 뿐만 아니라 전자서명까지 가증한 알고리즘입니다. </li></ol><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p><a href="https://sieunlim.tistory.com/16" target="_blank" rel="external nofollow noopener noreferrer">https://sieunlim.tistory.com/16</a></p><p><a href="https://record22.tistory.com/44" target="_blank" rel="external nofollow noopener noreferrer">https://record22.tistory.com/44</a></p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2019/12/07/%EC%95%94%ED%98%B8%ED%99%94%EB%B3%B5%ED%98%B8%ED%99%94/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Realtime Virus Scanning</title>
      <link>https://mchoi07.github.io/2019/11/20/nifi-virus-scanning/</link>
      <guid>https://mchoi07.github.io/2019/11/20/nifi-virus-scanning/</guid>
      <pubDate>Wed, 20 Nov 2019 20:14:06 GMT</pubDate>
      <description>
      
        &lt;p&gt;To protect our system and computer we should make sure that data which we download is clean. Everytime we bring data to our system or user upload data such as file attachments, we must make sure that data is free from viruses and trojans. &lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>To protect our system and computer we should make sure that data which we download is clean. Everytime we bring data to our system or user upload data such as file attachments, we must make sure that data is free from viruses and trojans. </p><a id="more"></a><p>If our system has sensitive data and critical for operation you have to be more cautious about bringing data to your system - cyber attack, nowadays, is being serious and cunning. </p><p>In a normal usecase, we set up Anti Virus (AV) scanner on a file system. AV scanner monitor our file system and RAM in real-time or batch. However, it cannot make sure that each file doesn’t have any malicious content in real-time. In this project, we will use two open source products to detect virus/trojan in realtime. We are going to use <code>Apache Nifi</code> and <code>ClamAV</code> </p><p><a href="https://nifi.apache.org/" target="_blank" rel="external nofollow noopener noreferrer">Apache Nifi</a> is a very powerful, easy to use and stable system to process and distribute data between disparate system. Apache Nifi is a real time data ingestion platform, which can transfer and manage data transfer between different sources and destination systems. </p><p><a href="https://www.clamav.net/" target="_blank" rel="external nofollow noopener noreferrer">ClamAV</a> is an open source antivirus engine for detecting trojans, viruses, malware &amp; other malicious threats. </p><h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><p><a href="https://isutah.com/virus-protection-cleanup/" target="_blank" rel="external nofollow noopener noreferrer">Thumnail Image</a>s</p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2019/11/20/nifi-virus-scanning/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Simulation 04 - Output Analysis</title>
      <link>https://mchoi07.github.io/2019/07/30/output-analysis/</link>
      <guid>https://mchoi07.github.io/2019/07/30/output-analysis/</guid>
      <pubDate>Tue, 30 Jul 2019 20:40:25 GMT</pubDate>
      <description>
      
        &lt;p&gt;Ananyzing the ouput of a simulation model is important. How can we be sure that our output is proper and will not hurt an experiment result using those outputs. &lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Ananyzing the ouput of a simulation model is important. How can we be sure that our output is proper and will not hurt an experiment result using those outputs. </p><a id="more"></a><p>Keep this in mind - out is rearely i.i.d. Why do we worry about output? In put processes driving a simulation are random variables. It means our output from the simulation must be random. If we runs the simulation it only yields estimates of measure of system performace, and these estimators are themselves random variables, and are therfore subject to sampling error. Sampling error must be taken into account to make valid inferences concerning system performance. </p><p><strong>Measures of Interest</strong></p><ul><li>Means - what is the mean customer waiting time? </li><li>Variances - how much is the waiting time liable to vary?</li><li>Quantiles - what’s the 99% quantile of the line length in a certain queue?</li><li>Sucess probabilities - will my job be completed on time?</li><li>Would like point estimators and confidence intervals for the above. </li></ul><p>There are two general types of simulations with respect to output analysis. To facilitate the presentation, we identify two types of simulations with respect to output analysis: </p><ul><li><em>Finite-Horizon (Terminating) Simulations</em> - Interested in short-term performance<ul><li>The termincation of finite-horizon simulation takes place at a specific time or is caused by the occurrence of a specific event.</li><li>EX1 - Mass transit system during rush hour</li><li>EX2 - Distribution system over one month</li></ul></li><li><em>Steady-State simulations</em> - Interested in long-term performance<ul><li>The purpose of <em>steady-state simulation</em> is to study the long-run behavior of a system. A performance measure is a steady-state parameter if it is a characteristic of the equilibrium distribution of an output process. </li><li>EX1 - Continuously operating communication system where the objective is the computation of the mean delay of a packet in the long run</li><li>EX2 - Distribution system over a long period of time</li><li>EX3 - Markov chains</li></ul></li></ul><p><strong>Finite-Horizon Simulation</strong></p><p>First thing we have to do to conduct this simulation is getting expected values from replications. So basically, we need to decide a number of independetn replications (IR). IR estimates Var($\bar{Y}_m$) by conducting $r$ independent simulation runs (replications) of the system under study, where each replication consists of $m$ observations. It is easy to make the replications independent - just re-initialize each replication with a different pseudo-random number seed Sample means from replication</p><p>If each run is started under the same operating conditions (e.g., all queues empty and idle), then the replication sample means $Z_1, Z_2, . . . , Z_r$ are $i.i.d.$ random variables. </p><p><img src="/2019/07/30/output-analysis/02.png" alt=""></p><p><img src="/2019/07/30/output-analysis/04.png" alt=""></p><p>Suppose we want to estimate the expected average waiting time for the first m = 5000 customers at the bank. We make r = 5 independent replications of the system, each initialized empty and<br>idle and consisting of 5000 waiting times. The resulting replicate means are:</p><p><img src="/2019/07/30/output-analysis/01.png" alt=""></p><p><img src="/2019/07/30/output-analysis/05.png" alt=""></p><p><strong>Steady-state simulation</strong></p><p>How about we need to simulate the entire time line? We should consider to use a steady-state simulation. Estimate some parameter of interest, e.g., the mean customer waiting time or the expected profit produced by a certain factory configuration. In particular, suppose the mean of this output is the unknown quantity $\mu$. We’ll use the sample mean $\bar{Y}_n$ to estimate $\mu$</p><p>We must accompany the value of any point extimator with a measure of its variance. In stead of Var($\bar{Y}_n)$ we canestimate the <em>variance parameter</em>,</p><p><img src="/2019/07/30/output-analysis/06.png" alt=""></p><p>Thus, $\sigma^2$ is imply the sume of all covariances! $\sigma^2$ pops up all over the place: simulation output analysis, Brownian motions, fnancial engineering application, etc. </p><p><img src="/2019/07/30/output-analysis/07.png" alt=""></p><p>Many methods for estimating $\sigma^2$ and for conducting steady-state output analysis in general:</p><ol><li><p>Batch means</p><p>The method of <em>batch means</em> (BM) is often used to estimate $\sigma^2$ and to calculate confidence intervals for $\mu$</p><p>Idea: Divide one long simulation run into a number of contiguous batches, and then appeal to a central limit theorem to assume that the resulting batch sample means are approximately i.i.d. normal. </p><p>In particular, suppose that we partition $Y_1, Y_2, . . . , Y_n$ into $b$ nonoverlapping, contiguous batches, each consisting of $m$ observations (assume that $n = bm$) </p><p><img src="/2019/07/30/output-analysis/08.png" alt=""></p><p>The $i$th batch mean is the sample mean of the $m$ observations from batch $i = 1, 2, . . . , b$</p><p><img src="/2019/07/30/output-analysis/10.png" alt=""></p><p><img src="/2019/07/30/output-analysis/11.png" alt=""></p><p><img src="/2019/07/30/output-analysis/12.png" alt=""></p><p><img src="/2019/07/30/output-analysis/13.png" alt=""></p><p><img src="/2019/07/30/output-analysis/14.png" alt=""></p><p><img src="/2019/07/30/output-analysis/15.png" alt=""></p></li></ol><p>$E[H]$ decreases in b, though it smooths out around b = 30. A common recommendation is to take b =. 30 and concentrate on increasing the batch size m as much as possible. </p><p>The technique of BM is intuitively appealing and easy to understand. </p><p>But problems can come up if the Yj ’s are not stationary (e.g., if significant initialization bias is present), if the batch means are not normal, or if the batch means are not independent. </p><p>If any of these assumption violations exist, poor confidence interval coverage may result — unbeknownst to the analyst.</p><p>To ameliorate the initialization bias problem, the user can truncate some of the data or make a long run </p><p>In addition, the lack of independence or normality of the batch means can be countered by increasing the batch size m.</p><p><img src="/2019/07/30/output-analysis/16.png" alt=""></p><p><img src="/2019/07/30/output-analysis/17.png" alt=""></p><p><img src="/2019/07/30/output-analysis/18.png" alt=""></p><p><img src="/2019/07/30/output-analysis/19.png" alt=""></p><p><img src="/2019/07/30/output-analysis/20.png" alt=""></p><p><img src="/2019/07/30/output-analysis/21.png" alt=""></p><p><img src="/2019/07/30/output-analysis/22.png" alt=""></p><h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><p><a href="https://www.computerhope.com/jargon/s/stdin.htm" target="_blank" rel="external nofollow noopener noreferrer">Thumnail</a><br>Georgia Tech’s <code>ISYE6644</code> class content</p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2019/07/30/output-analysis/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Simulation 03 - Input Analysis</title>
      <link>https://mchoi07.github.io/2019/07/23/input-analysis/</link>
      <guid>https://mchoi07.github.io/2019/07/23/input-analysis/</guid>
      <pubDate>Tue, 23 Jul 2019 20:40:08 GMT</pubDate>
      <description>
      
        &lt;p&gt;How can we tell our random variables are well made? In simulation terminology, we have something called &lt;code&gt;input analysis&lt;/code&gt;.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>How can we tell our random variables are well made? In simulation terminology, we have something called <code>input analysis</code>.</p><a id="more"></a><p>In my previous two postings, <a href="">random number</a>, <a href="">random variable</a>, I’ve talked about how to generate a random number and how we could make a radom variable by using those random numbers. Then how can we tell our random variables are well made? In simulation terminology, we have something called <code>input analysis</code>. The random variables are our input, and we need to analyze those inputs to verify it’s relevance. If you use the wrong input random variables in the simulation model, it will result in wrong output. A proper input analysis can save you from <code>Garbage-in-garbage-out</code></p><p>How can we conduct a proper input analysis?</p><ol><li>We have to collect data<ul><li>Data Sampling - we could shuffle the data and take some samples from there</li></ul></li><li>We have to figure out a distribution of data<ul><li>Plot the data to histogram </li><li>Discrete vs contunuous</li><li>Univariate / Multivariate</li><li>If data is not enough — we can guess a good distribution</li></ul></li><li>We have to do a statistical test to verify the distribution</li></ol><h2 id="1-Point-Estimation"><a href="#1-Point-Estimation" class="headerlink" title="1. Point Estimation"></a>1. Point Estimation</h2><p>A <em>statistic</em> can not explicitly depend on any <code>unknown parameters</code> because statics are based on the actural observations. <em>Statistics</em> are random variable - we could expect to have two different values of a static when we take two different samples. But we need to find the <code>unknown parameters</code>. How can we do it? we could estimate <code>unknown parameters</code> from the existing probability of distribution.  </p><p>Let $X_1, . . . . , X_n$ be i.i.d. Random Variables and let $M(X) ≡ M(X_1, . . . . , X_n)$ be a statistic based on the $X_i$’s. <strong>Suppose we use $M(X)$ to estimate some unknown parameter $θ$ Then $M(X)$ is called a <em>point estimator</em> for $θ$ .</strong></p><ul><li>$\bar{x}$ is a point estimator for the mean $μ = E[Xi]$</li><li>$S^2$ is a point estimator for the variance  $σ2 = Var(Xi) $</li></ul><p>*<em>It would be nice if $M(X)$ had certain properties: *</em></p><ul><li>Its expected value should equal the parameter it’s trying to estimate</li><li>It should have low variance</li></ul><p>We all know that good estimator should be unbiased because if we use the biased estimator we won’t figure out whether our model is good or not. We will be fooled by a biased estimator. $T(X)$ is <em>unbiased</em> for $θ$ if $E[M(X)] = θ$</p><p><strong>EX1</strong> -  Suppose $X_1, . . . , X_n$ are i.i.d. anything with mean μ. Then</p><p><img src="/2019/07/19/random-variable/sample_mean.png" alt="sample_mean"></p><p>So $\bar{X}$ is alwys unbiased for $\mu$. That’s why ${\bar{X}}$ is called the <em>sample mean</em> </p><p><strong>EX2</strong> -  Suppose $X_1, . . . , X_n$ are i.i.d. anything with mean μ and variance $\sigma^2$. Then</p><p><img src="/2019/07/19/random-variable/sample_variance.png" alt="sample_variance"></p><p>Thus, $S^2$ is always unbiased for $\sigma^2$. This is why $S^2$ is called the sample variance. </p><h2 id="2-Mean-Square-Error"><a href="#2-Mean-Square-Error" class="headerlink" title="2. Mean Square Error"></a>2. Mean Square Error</h2><p>In a perfect sceanario, our estimator will be exactly same as $\theta$. Then we will see no error. However, it’s not the real case. Our goal is to reduce the error between an estimator and $\theta$. </p><p>The <em>mean squared error</em> of an estimator $M(X)$ of $\theta$ is </p><p>$$<br>\begin{aligned}<br> MSE(M(X)) ≡ E[(M(X)-\theta)^2]<br>\end{aligned}<br>$$</p><p>$$<br>\begin{aligned}<br> Baia(M(X)) ≡ E[T(X) - \theta]<br>\end{aligned}<br>$$</p><p>We could interpret MSE like this: </p><p><img src="/2019/07/19/random-variable/mse.png" alt="mse"></p><p>Lower MSE mean we are avoiding the bias and <strong>variance</strong>. Our goal should be finding a good estimator which can lower our error. If $M_1(X)$ and $M_2(X)$ are two estimators of $\theta$, we’d usually prefer the one with the lower MSE — even if it happens to have higer bias.</p><h2 id="3-Maximum-Linelihood-Estimators"><a href="#3-Maximum-Linelihood-Estimators" class="headerlink" title="3. Maximum Linelihood Estimators"></a>3. Maximum Linelihood Estimators</h2><p>What if we don’t have a set of data, but we have a pdf/pmf $f(x)$ of the distribution. How can we find the $\theta$?</p><p>Consider an i.i.d. random sample $X_1, . . . , X_n$, where each $X_i$ has pdf/pmf $f(x)$. Further, suppose that $\theta$ is some unknown parameter from $X_i$. The likelihood function is $L(\theta) ≡ \prod f(x_i)$</p><p>The maximum likelihood estimator (MLE) of $\theta$ is the value of $\theta$ that maximizes $L(\theta)$. The MLE is a function of the $X_i$’s and is a RV. </p><p><strong>EX1</strong> -  Suppose $X_1, . . . , X_n$ ~ Exp$(\gamma)$. Find the MLE for $\gamma$</p><p><img src="/2019/07/19/random-variable/mle-1.png" alt="mle-1"></p><p>Since the natural log function is one-to-one, it’s easy to see that the $\gamma$ that maximizes $L(\gamma)$ also maximize $ln(L(\gamma))$</p><p><img src="/2019/07/19/random-variable/mle-2.png" alt="mle-2"></p><p><img src="/2019/07/19/random-variable/mle-3.png" alt="mle-3"></p><p>This implies that the MLE is $\hat{\gamma} = 1 / \bar{X}$</p><p><strong>Invariance Property</strong> </p><p>If $\hat{\theta}$ is the MLE of some parameter $\theta$ and $h(.)$ is a one-to-one function, then $h(\hat{\theta})$ is the MLE of $h(\theta)$</p><p>For Bern(p) distribution the MLE of $p$ is $\hat{p}=\bar{X}$ (which also happens to be unbiased). If we consider the 1:1 function $h(\theta) = \theta^2$ for ($\theta &gt; 0)$, then the <em>Invariance property</em> says that the MLE of $p^2$ is $\bar{X}^2$</p><p>But such a property does not hold for unbiasedness</p><p>$$<br>\begin{aligned}<br>E[S^2] = σ^2<br>\end{aligned}<br>but<br>\begin{aligned}<br>E[\sqrt{S^2}]= σ<br>\end{aligned}<br>$$</p><p>Really that MLE for $\sigma^2$ is $\hat{\sigma^2} = 1/n\sum(X_i - \bar{X})^2$. The good news is that we can still get the MLE for $\sigma$. If we consider the 1:1 function $h(\theta) = +\sqrt{\theta}$, then the invariance property says that the MLE of $\sigma$ is </p><p>$$<br>\begin{aligned}<br>\hat{\sigma} = \sqrt{\hat{\sigma^2}} = \sqrt{\sum(X_i-\bar{X})^2/n}<br>\end{aligned}<br>$$</p><h2 id="4-The-Method-of-Moments"><a href="#4-The-Method-of-Moments" class="headerlink" title="4. The Method of Moments"></a>4. The Method of Moments</h2><p>Recall: the $k$th moment of a random variable X is </p><p><img src="/2019/07/19/random-variable/mom-1.png" alt="mom-1"></p><p>Suppose $X_1, . . . , X_n$ are i.i.d. from p.m.f. / p.d.f. $f(x)$. Then the method of moments(MOM) estimator for $E[X^k]$ is</p><p>$$<br>m_k ≡ 1/n\sum X^k_i<br>$$</p><p><img src="/2019/07/19/random-variable/mom-2.png" alt="mom-2"></p><h2 id="5-Goodness-of-Fit-Tests"><a href="#5-Goodness-of-Fit-Tests" class="headerlink" title="5. Goodness-of-Fit Tests"></a>5. Goodness-of-Fit Tests</h2><p>We finanlly guessed a reasonable distribution and then estimated the relevant parameters. Now let’s conduct a formal test to see just how sucessful our toils have been.</p><p>In particular, test</p><p>$$<br>H_0 : X_1, X_2, . . . , X_n - p.m.f / p.d.f. f(x)<br>$$</p><p>At level of significance</p><p>$$<br>\alpha ≡ P(Reject H_0 | H_0 true) =P(\text{Type I Error})<br>$$</p><h5 id="Chi-Square-Test"><a href="#Chi-Square-Test" class="headerlink" title="Chi-Square-Test"></a>Chi-Square-Test</h5><p>Goodness-of-fit test procedure: </p><ol><li>Divide the domain of $f(x)$ into $k$ sets, say, $A_1, A_2, A_3, . . . , A_k$ (distinct points if X is discret r intervals if X is continuous)</li><li>Tally the actual number of observations that fall in ach set, say, $O_i i = 1, 2, . . . , k$. If $p_i ≡ P(X ∈ Ai)$, then $O_i \text{~ Bin(}n,p_i)$</li><li>Determine the expected number of observations that would fall in each set if $H_0$ were true, say, $E_i = E[O-I] = np_i, i = 1,2, . . . , k$</li><li>Calculate a test statistic based on the differences between the $E_i$ and $O_i$.</li></ol><p><img src="/2019/07/19/random-variable/gof.png" alt="gof"></p><ol start="5"><li>A large value of $X^2_0$ indicate a bad fit. We reject $H_0 \text{ if } \chi^2_0 &gt; \chi^2_{\alpha,k-1-s}$, where $s$ is the number of nuknown paramets from $f(x) that have to be estimated. </li></ol><p>Usual recommendation: For the $\chi^2$ g-o-f test to work, pick $k,n$ such that $E_i &gt;= 5$ and $n$ at least 30</p><p><strong>Kolmogorov_Smirnov Goodness-of-Fit Test</strong></p><p>We’ll test $H_0 : X_1, X_2, . . . , X_n$ some distribution with $c.d.f. F (x)$.</p><p>Recall the difincation of the <em>empirical c.d.f. (also called the sample c.d.f)</em> of the data is</p><p>$$<br>\hat{F_n}(x) ≡ (\text{number of } X_i &lt;= x) / n<br>$$</p><p>The Glivenko-Cantelli Lemma says that $\hat{F_n}(x) -&gt; F(x)$ for all $x$ as $ n-&gt;\infinite$.  So if $H_0$ is ture then $\hat{F_n}(x)$ should be a good approxmination to the true c.d.f. $F(x)$, for large $n$ The main question: Does the empirical distribution actually support the assumption that H0 is true? </p><p> <img src="/2019/07/19/random-variable/ks.png" alt="ks"></p><h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><p><a href="https://www.computerhope.com/jargon/s/stdin.htm" target="_blank" rel="external nofollow noopener noreferrer">Thumnail</a><br>Georgia Tech’s <code>ISYE6644</code> class content</p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2019/07/23/input-analysis/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Simulation 02 - Random variable</title>
      <link>https://mchoi07.github.io/2019/07/19/random-variable/</link>
      <guid>https://mchoi07.github.io/2019/07/19/random-variable/</guid>
      <pubDate>Fri, 19 Jul 2019 04:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Random variable can be generated from a good random number generator. If real variables has moved the reality, we could design a future with a good random variables.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Random variable can be generated from a good random number generator. If real variables has moved the reality, we could design a future with a good random variables.</p><a id="more"></a><blockquote><p>Author: Minkyu Choi<br>Last updated: 07/19/2019</p></blockquote><p><strong>Inverse Transform Method</strong><br>Inverse transform sampling is a method for generating random numbers from any probability distribution by using its inverse cumulative distribution F−1(x)F−1(x). Recall that the cumulative distribution for a random variable XX is FX(x)=P(X≤x)FX(x)=P(X≤x). In what follows, we assume that our computer can, on demand, generate independent realizations of a random variable UU uniformly distributed on [0,1]</p><p><strong>Cutpoint Method</strong><br>This inverse-transform method has the advantage of having an optimal O(n) setup time. However, the average number of steps required to sample X is not optimal, and if several samples of X are needed, then the cutpoint method offers an average number of two comparison steps needed to sample an observation, yet still has an O(n) initial setup time</p><p>Without loss of generality, we can assume that X = [1, n]. Also, let qi = P(X ≤ i). Then the idea behind the cutpoint method is to choose m ≥ n, and define sets Q1, . . . , Qm for which</p><p><img src="/2019/07/19/random-variable/cutpoint.png" alt="Cutpoint Method"></p><p>for all i = 1, . . . , m. In words, the unit interval [0, 1] is partitioned into m equal sub-intervals of the form $[\frac{(i−1)} m,  \frac{i}m)$, i = 1, . . . , m. And when U falls into the i th sub-interval, then Qi contains all the possible qj values for which F −1 (U) = j. That way, instead of searching through all of the q values, we save time by only examining the qj values in Qi , since these are the only possible values for which $F^{-1} (U) = j$.</p><p><strong>Convolution Method</strong></p><ul><li>Sum of n variables: $x = y_1 + y_2 + … y_n$</li><li>Generate n random variate yi’s and sum </li><li>For sums of two variables, pdf of x = convolution of pdfs of y1 and y2. Hence the name </li><li>Although no convolution in generation </li><li>If pdf or CDF = Sum ⇒ Composition </li><li>Variable x = Sum ⇒ Convolution</li></ul><p><strong>Acceptance-Rejection Method</strong><br>Finding an explicit formula for F −1 (y) for the cdf of a rv X we wish to generate, F(x) = P(X ≤ x), is not always possible. Moreover, even if it is, there may be alternative methods for generating a rv distributed as F that is more efficient than the inverse transform method or other methods we have come across. Here we present a very clever method known as the acceptance-rejection method.</p><p><strong>Composition Method</strong><br>Can be used when m can be expressed as a convex combination of other distributions Fi , where we hope to be able to sample from $F_i$ more easily than from F directly.</p><p><img src="/2019/07/19/random-variable/Cm.png" alt="Composition Method"></p><p><strong>References</strong><br><a href="https://newonlinecourses.science.psu.edu/stat414/node/104/" target="_blank" rel="external nofollow noopener noreferrer">Link-1</a><br><a href="https://stephens999.github.io/fiveMinuteStats/inverse_transform_sampling.html" target="_blank" rel="external nofollow noopener noreferrer">Link-2</a><br><a href="http://web.csulb.edu/~tebert/teaching/lectures/552/variate/variate.pdf" target="_blank" rel="external nofollow noopener noreferrer">Link-3</a><br><a href="https://www.cse.wustl.edu/~jain/cse567-08/ftp/k_28rvg.pdf" target="_blank" rel="external nofollow noopener noreferrer">Link-4</a><br><a href="http://www.columbia.edu/~ks20/4703-Sigman/4703-07-Notes-ARM.pdf" target="_blank" rel="external nofollow noopener noreferrer">Link-5</a></p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2019/07/19/random-variable/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Simulation 01 - Random Number</title>
      <link>https://mchoi07.github.io/2019/07/18/random-number/</link>
      <guid>https://mchoi07.github.io/2019/07/18/random-number/</guid>
      <pubDate>Thu, 18 Jul 2019 13:27:50 GMT</pubDate>
      <description>
      
        &lt;p&gt;A simulation is not real but it can represent the real. It’s because a simulation is an imitation of real situation - it can’t be exact but it can be approximate.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>A simulation is not real but it can represent the real. It’s because a simulation is an imitation of real situation - it can’t be exact but it can be approximate.</p><a id="more"></a><p>Most of simulation models are strated from generating random number because randomness creates value on a simulation modeling. It is really important to give an algorithm that produces a sequence of pseudo-random number (PRNs) $R_1, R_2,…$ that “appear” to be iid Unif(0,1). There are many different Uniform(0,1) Generators.</p><p><strong>Output of random device</strong></p><ul><li>Nice randomness properties. However, Unif(0,1) sequence storage difficult, sot it’s tough to repeat experiment</li><li>Examples:<ul><li>flip a coin</li><li>particle count by Geiger coutner</li><li>least significant digits of atomic clock</li></ul></li></ul><p><strong>Table of random numbers</strong></p><ul><li>List of digits supplied in tables - A Million random Digits with 100,00 Normal Deviates.</li><li>Cumbersome, slow, table too small - not very useful </li></ul><p><strong>Mid-Square</strong></p><ul><li>Idea - Take the middle part of square of the previous random number. John von Neumann was a brilliant and fun-loving guy, but method is terrible</li><li>Example: Take $R_i = X_i/10000$, ∀i, where the Xi’s are positive<br>integers &lt; 10000.</li><li>Set seed $X_0 = 6642$; then $6632^2$ = 43<strong>9834</strong>24</li><li>so $X_1 = 9834$; then $9834^2$ - 96<strong>7075</strong>56</li><li>so $X_2$ = 7075, etc,…</li><li>Unfortunately, positive serial correlation in $R_i$’s. Also, occasionally degenerates; eg., consider $X_i$ = 0003</li></ul><p><strong>Fibonacci</strong></p><ul><li>These methods are also no good!! </li><li>Take $X_i = (X_{i-1} + X_{i-2})mod(m), i = 1,2,…,$ where $R_i = X_i/m$ ,$m$ is the modulus, $X_01,X_0$ are seeds, and $a = b mod m$ if $a$ is the remainer of $b/m$ </li><li>Problem: small numbers follow small numbers</li><li>Also, it’s not possible to get $X_{i-1} &lt; X_{i+1} &lt; X_i$ or </li><li>$X_i &lt; X_{i+1} &lt; X_{i-1} $ (which should occur w.p 1/3)</li><li>$X_{i+1}$ </li></ul><p><strong>Linear congruential (most commonly used in practice</strong></p><ul><li>LCGs are the most widely used generators. These are pretty good when implemented properly. </li><li>$X_i = (aX_{i-1} + c) mod(m)$, where $X_0$ is the seed.</li><li>$R_i = X_i/m, i = 1,2,…$ </li><li>Choose a,c,m carefully to get good stastistical quality and long period or cycle length, i.e., time until LCG starts to repeat itself. </li><li>If $c = 0$, LCG is called a multiplicative generator </li></ul><p><strong>Tausworthe (linear recursion mod 2)</strong></p><ul><li><p>Tausworthe Generator is a kind of multicative recursive generator.</p></li><li><p>$X_{i+1} = (aX_{i-1} + c) mod(2)$, where $X_0$ is the seed.</p></li></ul><p><strong>Reference</strong></p><p>Georgia Tech’s <code>ISYE6644</code> class content</p>]]></content:encoded>
      
      <comments>https://mchoi07.github.io/2019/07/18/random-number/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
